{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be3addf",
   "metadata": {},
   "source": [
    "# 🦠 COVID-19 Data Collection & Initial Processing\n",
    "\n",
    "## 📋 Notebook Overview\n",
    "This notebook handles the collection and initial processing of COVID-19 data from multiple reliable sources including:\n",
    "- **Our World in Data (OWID)**: Comprehensive global dataset\n",
    "- **Johns Hopkins University**: Time series data for cases, deaths, and recoveries\n",
    "- **Data validation and quality checks**\n",
    "\n",
    "**Objective**: Download, clean, and prepare COVID-19 data for analysis\n",
    "\n",
    "**Author**: Your Name  \n",
    "**Date**: September 2025  \n",
    "**Duration**: ~30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1faeac8",
   "metadata": {},
   "source": [
    "## 📚 Step 1: Import Required Libraries\n",
    "\n",
    "First, we'll import all the necessary Python libraries for data collection, processing, and basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the src directory to Python path so we can import our custom modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom COVID-19 data utilities\n",
    "try:\n",
    "    from data_fetcher import CovidDataFetcher\n",
    "    from data_processor import CovidDataProcessor\n",
    "    print(\"✅ Successfully imported custom COVID-19 utilities\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing custom utilities: {e}\")\n",
    "    print(\"Make sure the src/ directory contains data_fetcher.py and data_processor.py\")\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.__version__ if hasattr(plt, '__version__') else 'Available'}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bddc4b",
   "metadata": {},
   "source": [
    "## 🌐 Step 2: Initialize Data Fetcher\n",
    "\n",
    "Now we'll set up our COVID-19 data fetcher to download data from reliable sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72103268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the COVID-19 data fetcher\n",
    "fetcher = CovidDataFetcher(data_dir=\"../data/raw\")\n",
    "\n",
    "# Show information about available data sources\n",
    "print(\"📊 Available COVID-19 Data Sources:\")\n",
    "print(\"=\" * 60)\n",
    "fetcher.get_data_info()\n",
    "\n",
    "# Check if we have any existing data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📁 Checking for existing data files...\")\n",
    "\n",
    "raw_data_path = Path(\"../data/raw\")\n",
    "if raw_data_path.exists():\n",
    "    existing_files = list(raw_data_path.glob(\"*.csv\"))\n",
    "    if existing_files:\n",
    "        print(f\"Found {len(existing_files)} existing data files:\")\n",
    "        for file in existing_files:\n",
    "            file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "            modified_time = datetime.fromtimestamp(file.stat().st_mtime)\n",
    "            print(f\"  📄 {file.name} ({file_size:.1f} MB, modified: {modified_time.strftime('%Y-%m-%d %H:%M')})\")\n",
    "    else:\n",
    "        print(\"No existing data files found. We'll download fresh data.\")\n",
    "else:\n",
    "    print(\"Data directory doesn't exist yet. It will be created when we download data.\")\n",
    "\n",
    "print(\"\\n🚀 Data fetcher initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1708b",
   "metadata": {},
   "source": [
    "## 📥 Step 3: Download COVID-19 Dataset from Kaggle Source\n",
    "\n",
    "This comprehensive dataset includes multiple files with different views of COVID-19 data from Johns Hopkins and Worldometer sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e60d2",
   "metadata": {},
   "source": [
    "## 📥 Step 3: Download Our World in Data (OWID) Dataset\n",
    "\n",
    "We'll start by downloading the main COVID-19 dataset from Our World in Data, which provides comprehensive global data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the main clean complete COVID-19 dataset\n",
    "print(\"🔄 Downloading COVID-19 clean complete dataset...\")\n",
    "print(\"This dataset contains daily country-level data without province/state details\")\n",
    "print(\"This may take a few minutes depending on your internet connection...\")\n",
    "\n",
    "try:\n",
    "    # Fetch the clean complete dataset (country-level daily data)\n",
    "    covid_data = fetcher.fetch_clean_complete_data(save_local=True)\n",
    "    \n",
    "    print(f\"\\n✅ Successfully downloaded COVID-19 data!\")\n",
    "    print(f\"📊 Dataset shape: {covid_data.shape[0]:,} rows × {covid_data.shape[1]} columns\")\n",
    "    \n",
    "    # Handle different date column names\n",
    "    date_col = 'Date' if 'Date' in covid_data.columns else 'date'\n",
    "    if date_col in covid_data.columns:\n",
    "        print(f\"📅 Date range: {covid_data[date_col].min()} to {covid_data[date_col].max()}\")\n",
    "    \n",
    "    # Handle different country column names\n",
    "    country_col = 'Country/Region' if 'Country/Region' in covid_data.columns else 'Country'\n",
    "    if country_col in covid_data.columns:\n",
    "        print(f\"🌍 Number of locations: {covid_data[country_col].nunique()}\")\n",
    "    \n",
    "    # Show a quick preview of the data\n",
    "    print(f\"\\n👀 First 5 rows:\")\n",
    "    display(covid_data.head())\n",
    "    \n",
    "    # Show column names\n",
    "    print(f\"\\n📋 Available columns ({len(covid_data.columns)} total):\")\n",
    "    for i, col in enumerate(covid_data.columns):\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    \n",
    "    # Also download the latest country summary for comparison\n",
    "    print(f\"\\n📊 Downloading latest country-wise summary...\")\n",
    "    country_latest = fetcher.fetch_country_wise_latest(save_local=True)\n",
    "    print(f\"✅ Country summary: {country_latest.shape[0]} countries\")\n",
    "    \n",
    "    # Download global day-wise data\n",
    "    print(f\"\\n🌍 Downloading global day-wise data...\")\n",
    "    day_wise_data = fetcher.fetch_day_wise_data(save_local=True)\n",
    "    print(f\"✅ Global day-wise data: {day_wise_data.shape[0]} days\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading COVID-19 data: {str(e)}\")\n",
    "    print(\"Please check your internet connection and try again.\")\n",
    "    print(\"You may also need to check if the data source URLs are still active.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf0e39",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Initial Data Exploration\n",
    "\n",
    "Let's take a closer look at our data to understand its structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"📊 DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {covid_data.shape[0]:,} rows × {covid_data.shape[1]} columns\")\n",
    "print(f\"Memory usage: {covid_data.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Date range: {covid_data['date'].min()} to {covid_data['date'].max()}\")\n",
    "print(f\"Number of unique locations: {covid_data['location'].nunique()}\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\n📋 DATA TYPES\")\n",
    "print(\"=\" * 30)\n",
    "data_types = covid_data.dtypes.value_counts()\n",
    "for dtype, count in data_types.items():\n",
    "    print(f\"{dtype}: {count} columns\")\n",
    "\n",
    "# Show sample locations\n",
    "print(f\"\\n🌍 SAMPLE LOCATIONS (first 20)\")\n",
    "print(\"=\" * 40)\n",
    "locations = covid_data['location'].unique()[:20]\n",
    "for i, location in enumerate(locations, 1):\n",
    "    print(f\"{i:2d}. {location}\")\n",
    "\n",
    "if len(covid_data['location'].unique()) > 20:\n",
    "    print(f\"... and {len(covid_data['location'].unique()) - 20} more locations\")\n",
    "\n",
    "# Basic statistics for key numerical columns\n",
    "key_columns = ['new_cases', 'new_deaths', 'total_cases', 'total_deaths']\n",
    "available_key_columns = [col for col in key_columns if col in covid_data.columns]\n",
    "\n",
    "if available_key_columns:\n",
    "    print(f\"\\n📈 BASIC STATISTICS FOR KEY METRICS\")\n",
    "    print(\"=\" * 50)\n",
    "    display(covid_data[available_key_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374a0d9",
   "metadata": {},
   "source": [
    "## ❓ Step 5: Data Quality Assessment\n",
    "\n",
    "Let's check for missing values and data quality issues that we'll need to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ee223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"🔍 MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "missing_values = covid_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(covid_data)) * 100\n",
    "\n",
    "# Create a summary of missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Only show columns with missing values\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"Found missing values in {len(missing_summary)} columns:\")\n",
    "    display(missing_summary.head(15))  # Show top 15 columns with missing values\n",
    "    \n",
    "    if len(missing_summary) > 15:\n",
    "        print(f\"... and {len(missing_summary) - 15} more columns with missing values\")\n",
    "else:\n",
    "    print(\"✅ No missing values found in the dataset!\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_count = covid_data.duplicated().sum()\n",
    "print(f\"\\n🔁 DUPLICATE ROWS: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"⚠️  Found {duplicate_count} duplicate rows that may need to be removed\")\n",
    "else:\n",
    "    print(\"✅ No duplicate rows found\")\n",
    "\n",
    "# Check date consistency\n",
    "print(f\"\\n📅 DATE ANALYSIS\")\n",
    "print(\"=\" * 20)\n",
    "print(f\"Date column type: {covid_data['date'].dtype}\")\n",
    "print(f\"Earliest date: {covid_data['date'].min()}\")\n",
    "print(f\"Latest date: {covid_data['date'].max()}\")\n",
    "print(f\"Date range: {(covid_data['date'].max() - covid_data['date'].min()).days} days\")\n",
    "\n",
    "# Check for any invalid dates (nulls in date column)\n",
    "null_dates = covid_data['date'].isnull().sum()\n",
    "print(f\"Missing dates: {null_dates}\")\n",
    "\n",
    "# Quick sanity check on data values\n",
    "print(f\"\\n🔢 DATA SANITY CHECKS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Check for negative values in cumulative columns\n",
    "cumulative_columns = ['total_cases', 'total_deaths']\n",
    "for col in cumulative_columns:\n",
    "    if col in covid_data.columns:\n",
    "        negative_count = (covid_data[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"⚠️  {col}: {negative_count} negative values found\")\n",
    "        else:\n",
    "            print(f\"✅ {col}: No negative values\")\n",
    "\n",
    "print(f\"\\n📊 DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "total_cells = covid_data.shape[0] * covid_data.shape[1]\n",
    "missing_cells = covid_data.isnull().sum().sum()\n",
    "data_completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "\n",
    "print(f\"Overall data completeness: {data_completeness:.1f}%\")\n",
    "print(f\"Total cells: {total_cells:,}\")\n",
    "print(f\"Missing cells: {missing_cells:,}\")\n",
    "print(f\"Complete cells: {total_cells - missing_cells:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbf1b4",
   "metadata": {},
   "source": [
    "## 🧹 Step 6: Data Cleaning and Processing\n",
    "\n",
    "Now let's clean and process our data using our custom data processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data processor\n",
    "processor = CovidDataProcessor(raw_data_dir=\"../data/raw\", processed_data_dir=\"../data/processed\")\n",
    "\n",
    "print(\"🧹 Starting data cleaning and processing...\")\n",
    "\n",
    "try:\n",
    "    # Clean the COVID-19 data\n",
    "    cleaned_data = processor.clean_covid_data(covid_data)\n",
    "    \n",
    "    print(f\"\\n✅ Data cleaning completed!\")\n",
    "    print(f\"📊 Original shape: {covid_data.shape[0]:,} rows × {covid_data.shape[1]} columns\")\n",
    "    print(f\"📊 Cleaned shape: {cleaned_data.shape[0]:,} rows × {cleaned_data.shape[1]} columns\")\n",
    "    \n",
    "    # Show what changed\n",
    "    rows_removed = covid_data.shape[0] - cleaned_data.shape[0]\n",
    "    columns_added = cleaned_data.shape[1] - covid_data.shape[1]\n",
    "    \n",
    "    if rows_removed > 0:\n",
    "        print(f\"🗑️  Removed {rows_removed:,} rows (likely duplicates or data corrections)\")\n",
    "    if rows_removed < 0:\n",
    "        print(f\"➕ Added {abs(rows_removed):,} rows (likely from aggregation)\")\n",
    "    if columns_added > 0:\n",
    "        print(f\"➕ Added {columns_added} new derived columns\")\n",
    "    \n",
    "    # Show sample of cleaned data\n",
    "    print(f\"\\n👀 Sample of cleaned data:\")\n",
    "    display(cleaned_data.head())\n",
    "    \n",
    "    # Show the new/modified columns\n",
    "    new_columns = set(cleaned_data.columns) - set(covid_data.columns)\n",
    "    if new_columns:\n",
    "        print(f\"\\n✨ New columns added during processing:\")\n",
    "        for col in sorted(new_columns):\n",
    "            print(f\"  • {col}\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\n📋 Data types after cleaning:\")\n",
    "    for col, dtype in cleaned_data.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during data cleaning: {str(e)}\")\n",
    "    # If cleaning fails, we'll use the original data\n",
    "    cleaned_data = covid_data.copy()\n",
    "    print(\"Using original data for now...\")\n",
    "    \n",
    "    # Still try to standardize column names manually\n",
    "    if 'Country/Region' in cleaned_data.columns:\n",
    "        cleaned_data = cleaned_data.rename(columns={'Country/Region': 'location'})\n",
    "    if 'Date' in cleaned_data.columns:\n",
    "        cleaned_data['Date'] = pd.to_datetime(cleaned_data['Date'])\n",
    "        cleaned_data = cleaned_data.rename(columns={'Date': 'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e09e19",
   "metadata": {},
   "source": [
    "## 📈 Step 7: Create Processed Datasets\n",
    "\n",
    "Let's create different views of our data that will be useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different views of the data for analysis\n",
    "print(\"📊 Creating processed datasets for analysis...\")\n",
    "\n",
    "datasets_created = {}\n",
    "\n",
    "try:\n",
    "    # 1. Country summary (latest data for each country)\n",
    "    print(\"\\n1️⃣ Creating country summary...\")\n",
    "    country_summary = processor.create_country_summary(cleaned_data)\n",
    "    datasets_created['country_summary'] = country_summary\n",
    "    print(f\"   ✅ Country summary: {country_summary.shape[0]} countries\")\n",
    "    \n",
    "    # Show top 10 countries by total cases\n",
    "    print(\"\\n🏆 Top 10 countries by total cases:\")\n",
    "    display(country_summary.head(10))\n",
    "    \n",
    "    # 2. Time series for major countries\n",
    "    print(\"\\n2️⃣ Creating time series for major countries...\")\n",
    "    major_countries_ts = processor.create_time_series_data(cleaned_data)\n",
    "    datasets_created['major_countries_timeseries'] = major_countries_ts\n",
    "    print(f\"   ✅ Major countries time series: {major_countries_ts.shape[0]:,} rows\")\n",
    "    \n",
    "    # 3. Global aggregated data\n",
    "    print(\"\\n3️⃣ Creating global aggregated data...\")\n",
    "    global_data = processor.aggregate_global_data(cleaned_data)\n",
    "    datasets_created['global_timeseries'] = global_data\n",
    "    print(f\"   ✅ Global time series: {global_data.shape[0]} days\")\n",
    "    \n",
    "    # Show latest global stats\n",
    "    latest_global = global_data.iloc[-1]\n",
    "    print(f\"\\n🌍 Latest global statistics ({latest_global['date'].strftime('%Y-%m-%d')}):\")\n",
    "    print(f\"   Total cases: {latest_global['total_cases']:,.0f}\")\n",
    "    print(f\"   Total deaths: {latest_global['total_deaths']:,.0f}\")\n",
    "    print(f\"   Case fatality rate: {latest_global['case_fatality_rate']:.2f}%\")\n",
    "    print(f\"   New cases (7-day avg): {latest_global['new_cases_7day_avg']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Successfully created {len(datasets_created)} processed datasets!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating processed datasets: {str(e)}\")\n",
    "    # Create minimal datasets as fallback\n",
    "    datasets_created['cleaned_data'] = cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d7878",
   "metadata": {},
   "source": [
    "## 💾 Step 8: Save Processed Data\n",
    "\n",
    "Let's save our cleaned and processed datasets for use in subsequent analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all processed datasets\n",
    "print(\"💾 Saving processed datasets...\")\n",
    "\n",
    "# Add the full cleaned dataset to our collection\n",
    "datasets_created['covid_cleaned'] = cleaned_data\n",
    "\n",
    "try:\n",
    "    # Save using our processor\n",
    "    processor.save_processed_data(datasets_created)\n",
    "    \n",
    "    print(f\"\\n✅ Successfully saved {len(datasets_created)} datasets!\")\n",
    "    \n",
    "    # Show what we saved\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    if processed_dir.exists():\n",
    "        saved_files = list(processed_dir.glob(\"*.csv\"))\n",
    "        print(f\"\\n📁 Saved files in {processed_dir}:\")\n",
    "        \n",
    "        total_size = 0\n",
    "        for file in saved_files:\n",
    "            file_size = file.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "            total_size += file_size\n",
    "            print(f\"   📄 {file.name} ({file_size:.1f} MB)\")\n",
    "        \n",
    "        print(f\"\\n📊 Total size of processed data: {total_size:.1f} MB\")\n",
    "    \n",
    "    # Generate and display data quality report\n",
    "    print(f\"\\n📋 Final Data Quality Report:\")\n",
    "    print(\"=\" * 40)\n",
    "    quality_report = processor.get_data_quality_report(cleaned_data)\n",
    "    \n",
    "    print(f\"Total rows: {quality_report['total_rows']:,}\")\n",
    "    print(f\"Total columns: {quality_report['total_columns']}\")\n",
    "    print(f\"Date range: {quality_report['date_range']['start']} to {quality_report['date_range']['end']}\")\n",
    "    print(f\"Countries/locations: {quality_report['countries']}\")\n",
    "    print(f\"Duplicate rows: {quality_report['duplicate_rows']}\")\n",
    "    \n",
    "    # Show columns with significant missing data\n",
    "    missing_data = {k: v for k, v in quality_report['missing_values'].items() if v > 0}\n",
    "    if missing_data:\n",
    "        print(f\"\\nColumns with missing values:\")\n",
    "        for col, count in sorted(missing_data.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            percentage = (count / quality_report['total_rows']) * 100\n",
    "            print(f\"   {col}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving processed data: {str(e)}\")\n",
    "    print(\"You may need to create the processed data directory manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52db35e2",
   "metadata": {},
   "source": [
    "## 📊 Step 9: Quick Visualization Preview\n",
    "\n",
    "Let's create a quick preview visualization to make sure our data looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f15659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quick visualization to verify our data looks correct\n",
    "print(\"📊 Creating quick data validation plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('COVID-19 Data Collection - Quick Validation Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "try:\n",
    "    # 1. Global daily cases over time\n",
    "    if 'global_timeseries' in datasets_created:\n",
    "        global_data = datasets_created['global_timeseries']\n",
    "        axes[0, 0].plot(global_data['date'], global_data['new_cases_7day_avg'], \n",
    "                       color='blue', linewidth=2, alpha=0.8)\n",
    "        axes[0, 0].set_title('Global Daily Cases (7-day average)')\n",
    "        axes[0, 0].set_ylabel('New Cases')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Top 10 countries by total cases\n",
    "    if 'country_summary' in datasets_created:\n",
    "        top_10 = datasets_created['country_summary'].head(10)\n",
    "        axes[0, 1].barh(range(len(top_10)), top_10['Total_Cases'], color='lightcoral')\n",
    "        axes[0, 1].set_yticks(range(len(top_10)))\n",
    "        axes[0, 1].set_yticklabels(top_10['Country'])\n",
    "        axes[0, 1].set_title('Top 10 Countries - Total Cases')\n",
    "        axes[0, 1].set_xlabel('Total Cases')\n",
    "        # Invert y-axis so highest is at top\n",
    "        axes[0, 1].invert_yaxis()\n",
    "    \n",
    "    # 3. Global daily deaths over time\n",
    "    if 'global_timeseries' in datasets_created:\n",
    "        axes[1, 0].plot(global_data['date'], global_data['new_deaths_7day_avg'], \n",
    "                       color='red', linewidth=2, alpha=0.8)\n",
    "        axes[1, 0].set_title('Global Daily Deaths (7-day average)')\n",
    "        axes[1, 0].set_ylabel('New Deaths')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Case fatality rates for top 10 countries\n",
    "    if 'country_summary' in datasets_created:\n",
    "        axes[1, 1].bar(range(len(top_10)), top_10['Case_Fatality_Rate'], \n",
    "                      color='orange', alpha=0.7)\n",
    "        axes[1, 1].set_xticks(range(len(top_10)))\n",
    "        axes[1, 1].set_xticklabels(top_10['Country'], rotation=45, ha='right')\n",
    "        axes[1, 1].set_title('Case Fatality Rate - Top 10 Countries')\n",
    "        axes[1, 1].set_ylabel('CFR (%)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Quick validation plots created successfully!\")\n",
    "    print(\"📊 The data appears to be loaded and processed correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not create validation plots: {str(e)}\")\n",
    "    print(\"This doesn't affect the data collection, but you may want to check the data manually.\")\n",
    "\n",
    "# Show some quick statistics\n",
    "print(f\"\\n📈 QUICK STATS SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if 'global_timeseries' in datasets_created:\n",
    "    latest_global = datasets_created['global_timeseries'].iloc[-1]\n",
    "    print(f\"Latest global data ({latest_global['date'].strftime('%Y-%m-%d')}):\")\n",
    "    print(f\"  • Total cases: {latest_global['total_cases']:,.0f}\")\n",
    "    print(f\"  • Total deaths: {latest_global['total_deaths']:,.0f}\")\n",
    "    print(f\"  • New cases (7-day avg): {latest_global['new_cases_7day_avg']:,.0f}\")\n",
    "\n",
    "if 'country_summary' in datasets_created:\n",
    "    country_summary = datasets_created['country_summary']\n",
    "    print(f\"\\nCountry analysis:\")\n",
    "    print(f\"  • Countries analyzed: {len(country_summary)}\")\n",
    "    print(f\"  • Most affected country: {country_summary.iloc[0]['Country']}\")\n",
    "    print(f\"  • Highest CFR: {country_summary['Case_Fatality_Rate'].max():.2f}%\")\n",
    "\n",
    "print(f\"\\n🎉 Data collection and processing completed successfully!\")\n",
    "print(f\"Ready for detailed analysis in the next notebooks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042ca59",
   "metadata": {},
   "source": [
    "## 🎯 Summary & Next Steps\n",
    "\n",
    "### ✅ What We Accomplished\n",
    "\n",
    "1. **📥 Data Collection**: Successfully downloaded COVID-19 data from Our World in Data\n",
    "2. **🔍 Data Exploration**: Analyzed the structure and quality of our dataset\n",
    "3. **🧹 Data Cleaning**: Processed and cleaned the data for analysis\n",
    "4. **📊 Dataset Creation**: Created multiple processed datasets for different types of analysis\n",
    "5. **💾 Data Storage**: Saved all processed data for use in subsequent notebooks\n",
    "6. **✅ Validation**: Verified data quality with quick visualization checks\n",
    "\n",
    "### 📊 Datasets Created\n",
    "\n",
    "- **`covid_cleaned.csv`**: Full cleaned dataset with all countries and metrics\n",
    "- **`country_summary.csv`**: Latest statistics for each country\n",
    "- **`major_countries_timeseries.csv`**: Time series data for major countries\n",
    "- **`global_timeseries.csv`**: Global aggregated data by date\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Exploratory Analysis** (`02_exploratory_analysis.ipynb`): Deep dive into patterns and trends\n",
    "2. **Time Series Analysis** (`03_trend_analysis.ipynb`): Analyze temporal patterns and seasonality\n",
    "3. **Geographic Analysis** (`04_geographic_analysis.ipynb`): Compare countries and regions\n",
    "\n",
    "### 📁 Files Ready for Analysis\n",
    "\n",
    "All processed data files are saved in `../data/processed/` and ready for the next phase of analysis!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
