{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72dab83f",
   "metadata": {},
   "source": [
    "# 📄 Text Classification Deep Dive\n",
    "\n",
    "This notebook explores advanced text classification techniques beyond sentiment analysis:\n",
    "- **Multi-class text classification**\n",
    "- **Topic classification**\n",
    "- **Spam detection**\n",
    "- **Advanced evaluation metrics**\n",
    "- **Handling imbalanced datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44542ef2",
   "metadata": {},
   "source": [
    "## 📚 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_auc_score, \n",
    "    multilabel_confusion_matrix\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7875d",
   "metadata": {},
   "source": [
    "## 📊 Multi-Class Dataset Creation\n",
    "\n",
    "Let's create a comprehensive multi-class text classification dataset with various categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674998e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-class dataset - News Categories\n",
    "news_data = {\n",
    "    'Technology': [\n",
    "        \"New smartphone features artificial intelligence capabilities\",\n",
    "        \"Software company releases innovative machine learning platform\",\n",
    "        \"Tech startup develops breakthrough quantum computing solution\",\n",
    "        \"Major update brings enhanced security features to popular app\",\n",
    "        \"Cloud computing services expand to new global regions\",\n",
    "        \"Artificial intelligence helps improve medical diagnosis accuracy\",\n",
    "        \"New programming language simplifies data science workflows\",\n",
    "        \"Cryptocurrency adoption increases among financial institutions\"\n",
    "    ],\n",
    "    'Sports': [\n",
    "        \"Football team wins championship after incredible season performance\",\n",
    "        \"Basketball player breaks scoring record in playoff game\",\n",
    "        \"Tennis tournament features exciting matches and upsets\",\n",
    "        \"Olympic athletes prepare for upcoming international competition\",\n",
    "        \"Soccer team signs new player for record transfer fee\",\n",
    "        \"Baseball season starts with promising rookie performances\",\n",
    "        \"Swimming world records broken at international championship\",\n",
    "        \"Golf tournament concludes with thrilling final round\"\n",
    "    ],\n",
    "    'Health': [\n",
    "        \"Medical researchers discover new treatment for rare disease\",\n",
    "        \"Health experts recommend regular exercise for mental wellness\",\n",
    "        \"New vaccine shows promising results in clinical trials\",\n",
    "        \"Diet and nutrition study reveals benefits of Mediterranean eating\",\n",
    "        \"Mental health awareness campaign launches in schools nationwide\",\n",
    "        \"Medical device innovation improves patient care outcomes\",\n",
    "        \"Health insurance coverage expands to include preventive care\",\n",
    "        \"Fitness tracker data helps doctors monitor patient health\"\n",
    "    ],\n",
    "    'Business': [\n",
    "        \"Stock market reaches new highs amid economic recovery\",\n",
    "        \"Company reports strong quarterly earnings and revenue growth\",\n",
    "        \"Merger between two major corporations creates industry leader\",\n",
    "        \"Small business loans program helps entrepreneurs during pandemic\",\n",
    "        \"E-commerce sales continue rapid growth in retail sector\",\n",
    "        \"Investment firm announces new fund for sustainable projects\",\n",
    "        \"Manufacturing sector shows signs of economic improvement\",\n",
    "        \"Banking industry adapts to digital transformation trends\"\n",
    "    ],\n",
    "    'Entertainment': [\n",
    "        \"New movie breaks box office records on opening weekend\",\n",
    "        \"Popular television series announces final season premiere date\",\n",
    "        \"Music festival features lineup of international artists\",\n",
    "        \"Award ceremony celebrates achievements in film and television\",\n",
    "        \"Streaming service launches original content production studio\",\n",
    "        \"Celebrity couple announces engagement at red carpet event\",\n",
    "        \"Video game releases generate excitement among gaming community\",\n",
    "        \"Concert tour sells out venues across multiple countries\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "texts = []\n",
    "categories = []\n",
    "\n",
    "for category, articles in news_data.items():\n",
    "    texts.extend(articles)\n",
    "    categories.extend([category] * len(articles))\n",
    "\n",
    "df_multi = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "print(f\"Multi-class dataset created with {len(df_multi)} samples\")\n",
    "print(f\"Categories: {df_multi['category'].unique().tolist()}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df_multi['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0607ae",
   "metadata": {},
   "source": [
    "## 📊 Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Category distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "df_multi['category'].value_counts().plot(kind='bar', color='skyblue', alpha=0.8)\n",
    "plt.title('Category Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Text length analysis\n",
    "df_multi['text_length'] = df_multi['text'].str.len()\n",
    "df_multi['word_count'] = df_multi['text'].str.split().str.len()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(data=df_multi, x='category', y='text_length')\n",
    "plt.title('Text Length by Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=df_multi, x='category', y='word_count')\n",
    "plt.title('Word Count by Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "for category in df_multi['category'].unique():\n",
    "    category_data = df_multi[df_multi['category'] == category]['text_length']\n",
    "    plt.hist(category_data, alpha=0.6, label=category, bins=10)\n",
    "plt.title('Text Length Distribution by Category')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b88f5",
   "metadata": {},
   "source": [
    "## 🧹 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef479e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Advanced text preprocessing for multi-class classification\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits (keep some for context)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize and remove short words\n",
    "    words = [word for word in text.split() if len(word) > 2]\n",
    "    \n",
    "    # Remove stopwords (but keep some domain-specific ones)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove some words that might be important for classification\n",
    "    domain_words = {'new', 'major', 'first', 'last', 'best', 'good', 'great'}\n",
    "    stop_words = stop_words - domain_words\n",
    "    \n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_multi['processed_text'] = df_multi['text'].apply(advanced_preprocessing)\n",
    "\n",
    "print(\"Preprocessing Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nCategory: {df_multi.iloc[i]['category']}\")\n",
    "    print(f\"Original: {df_multi.iloc[i]['text']}\")\n",
    "    print(f\"Processed: {df_multi.iloc[i]['processed_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8d66e",
   "metadata": {},
   "source": [
    "## 🎯 Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b28d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "X = df_multi['processed_text']\n",
    "y = df_multi['category']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Classes: {label_encoder.classes_.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    max_df=0.8\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d40b8e",
   "metadata": {},
   "source": [
    "## 🤖 Multi-Class Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define algorithms for multi-class classification\n",
    "algorithms = {\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM (OvR)': OneVsRestClassifier(SVC(random_state=42, probability=True)),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Train and evaluate each algorithm\n",
    "results = {}\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    algorithm.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = algorithm.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Algorithm': list(results.keys()),\n",
    "    'Accuracy': [results[name]['accuracy'] for name in results.keys()],\n",
    "    'Precision': [results[name]['precision'] for name in results.keys()],\n",
    "    'Recall': [results[name]['recall'] for name in results.keys()],\n",
    "    'F1-Score': [results[name]['f1_score'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"\\nMulti-Class Classification Results:\")\n",
    "print(results_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize algorithm comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(results_df['Algorithm'], results_df['Accuracy'], color='skyblue', alpha=0.8)\n",
    "ax1.set_title('Accuracy Comparison')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Precision comparison\n",
    "ax2.bar(results_df['Algorithm'], results_df['Precision'], color='lightgreen', alpha=0.8)\n",
    "ax2.set_title('Precision Comparison')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Recall comparison\n",
    "ax3.bar(results_df['Algorithm'], results_df['Recall'], color='lightcoral', alpha=0.8)\n",
    "ax3.set_title('Recall Comparison')\n",
    "ax3.set_ylabel('Recall')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# F1-Score comparison\n",
    "ax4.bar(results_df['Algorithm'], results_df['F1-Score'], color='gold', alpha=0.8)\n",
    "ax4.set_title('F1-Score Comparison')\n",
    "ax4.set_ylabel('F1-Score')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee8229a",
   "metadata": {},
   "source": [
    "## 📊 Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e65435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Algorithm']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, best_predictions, \n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe12544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.ylabel('Actual Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'Category': label_encoder.classes_,\n",
    "    'Accuracy': class_accuracy\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(accuracy_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bc24a",
   "metadata": {},
   "source": [
    "## 🔍 Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36536e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most important features for each class (using Logistic Regression)\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    lr_model = algorithms['Logistic Regression']\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get coefficients for each class\n",
    "    coefficients = lr_model.coef_\n",
    "    \n",
    "    # Plot top features for each class\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        if i < len(axes):\n",
    "            # Get top features for this class\n",
    "            class_coef = coefficients[i]\n",
    "            top_indices = np.argsort(class_coef)[-10:][::-1]\n",
    "            top_features = [feature_names[idx] for idx in top_indices]\n",
    "            top_values = [class_coef[idx] for idx in top_indices]\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].barh(range(len(top_features)), top_values, color=f'C{i}', alpha=0.7)\n",
    "            axes[i].set_yticks(range(len(top_features)))\n",
    "            axes[i].set_yticklabels(top_features)\n",
    "            axes[i].set_title(f'Top Features - {class_name}')\n",
    "            axes[i].set_xlabel('Coefficient Value')\n",
    "    \n",
    "    # Hide empty subplot\n",
    "    if len(label_encoder.classes_) < len(axes):\n",
    "        axes[-1].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46c5c7",
   "metadata": {},
   "source": [
    "## 🧪 Testing with New Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text, model, vectorizer, label_encoder, preprocessor):\n",
    "    \"\"\"\n",
    "    Predict category for new text\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    processed_text = preprocessor(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vector = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(text_vector)[0]\n",
    "    probabilities = model.predict_proba(text_vector)[0]\n",
    "    \n",
    "    # Get category name\n",
    "    category = label_encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Get top predictions\n",
    "    top_indices = np.argsort(probabilities)[::-1]\n",
    "    top_predictions = [(label_encoder.classes_[idx], probabilities[idx]) for idx in top_indices]\n",
    "    \n",
    "    return category, top_predictions\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"Apple releases new iPhone with improved camera and battery life\",\n",
    "    \"Tennis player wins Wimbledon championship in straight sets\",\n",
    "    \"Scientists develop new cancer treatment using immunotherapy\",\n",
    "    \"Stock market surges after positive earnings reports\",\n",
    "    \"Netflix announces new original series starring popular actors\",\n",
    "    \"Machine learning algorithm helps doctors diagnose diseases faster\",\n",
    "    \"Football team trades star quarterback to division rival\",\n",
    "    \"Cryptocurrency prices fluctuate amid regulatory uncertainty\"\n",
    "]\n",
    "\n",
    "best_model = algorithms[best_model_name]\n",
    "\n",
    "print(\"Category Predictions for New Texts:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text in test_texts:\n",
    "    predicted_category, top_predictions = predict_category(\n",
    "        text, best_model, tfidf_vectorizer, label_encoder, advanced_preprocessing\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted Category: {predicted_category}\")\n",
    "    print(f\"Confidence: {top_predictions[0][1]:.4f}\")\n",
    "    print(\"Top 3 Predictions:\")\n",
    "    for category, prob in top_predictions[:3]:\n",
    "        print(f\"  {category}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbe4d7",
   "metadata": {},
   "source": [
    "## 📧 Spam Detection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ee2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spam detection dataset\n",
    "spam_emails = [\n",
    "    \"Congratulations! You have won $1000000! Click here to claim now!\",\n",
    "    \"Buy cheap medications online! Best prices guaranteed!\",\n",
    "    \"Make money fast! Work from home opportunity!\",\n",
    "    \"FREE! Get rich quick scheme! Limited time offer!\",\n",
    "    \"Urgent! Your account will be closed! Verify now!\",\n",
    "    \"Hot singles in your area want to meet you!\",\n",
    "    \"Lose weight fast with this amazing pill!\",\n",
    "    \"Get your loan approved instantly! No credit check!\"\n",
    "]\n",
    "\n",
    "legitimate_emails = [\n",
    "    \"Meeting scheduled for tomorrow at 2 PM in conference room\",\n",
    "    \"Please find attached the quarterly financial report\",\n",
    "    \"Thank you for your order. It will be shipped within 2 days\",\n",
    "    \"Your subscription has been renewed successfully\",\n",
    "    \"Welcome to our newsletter! Here are this week's updates\",\n",
    "    \"Your appointment has been confirmed for next Monday\",\n",
    "    \"Project deadline has been extended by one week\",\n",
    "    \"Happy birthday! Hope you have a wonderful day\"\n",
    "]\n",
    "\n",
    "# Create spam dataset\n",
    "spam_texts = spam_emails + legitimate_emails\n",
    "spam_labels = ['spam'] * len(spam_emails) + ['legitimate'] * len(legitimate_emails)\n",
    "\n",
    "spam_df = pd.DataFrame({\n",
    "    'text': spam_texts,\n",
    "    'label': spam_labels\n",
    "})\n",
    "\n",
    "print(\"Spam Detection Dataset:\")\n",
    "print(spam_df['label'].value_counts())\n",
    "\n",
    "# Preprocess spam data\n",
    "spam_df['processed_text'] = spam_df['text'].apply(advanced_preprocessing)\n",
    "\n",
    "# Train spam classifier\n",
    "X_spam = spam_df['processed_text']\n",
    "y_spam = spam_df['label']\n",
    "\n",
    "X_train_spam, X_test_spam, y_train_spam, y_test_spam = train_test_split(\n",
    "    X_spam, y_spam, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize spam data\n",
    "spam_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_train_spam_vec = spam_vectorizer.fit_transform(X_train_spam)\n",
    "X_test_spam_vec = spam_vectorizer.transform(X_test_spam)\n",
    "\n",
    "# Train spam classifier\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(X_train_spam_vec, y_train_spam)\n",
    "\n",
    "# Evaluate spam classifier\n",
    "spam_predictions = spam_classifier.predict(X_test_spam_vec)\n",
    "spam_accuracy = accuracy_score(y_test_spam, spam_predictions)\n",
    "\n",
    "print(f\"\\nSpam Classification Accuracy: {spam_accuracy:.4f}\")\n",
    "print(\"\\nSpam Classification Report:\")\n",
    "print(classification_report(y_test_spam, spam_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c81f5d",
   "metadata": {},
   "source": [
    "## 📈 Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified k-fold cross-validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = {}\n",
    "scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "\n",
    "# Use stratified k-fold for multi-class\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    print(f\"Cross-validating {name}...\")\n",
    "    \n",
    "    cv_scores = cross_validate(\n",
    "        algorithm, X_train_tfidf, y_train, cv=cv, scoring=scoring\n",
    "    )\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'accuracy': cv_scores['test_accuracy'],\n",
    "        'precision': cv_scores['test_precision_weighted'],\n",
    "        'recall': cv_scores['test_recall_weighted'],\n",
    "        'f1': cv_scores['test_f1_weighted']\n",
    "    }\n",
    "\n",
    "# Create CV results DataFrame\n",
    "cv_summary = []\n",
    "for name, scores in cv_results.items():\n",
    "    cv_summary.append({\n",
    "        'Algorithm': name,\n",
    "        'Accuracy_Mean': np.mean(scores['accuracy']),\n",
    "        'Accuracy_Std': np.std(scores['accuracy']),\n",
    "        'F1_Mean': np.mean(scores['f1']),\n",
    "        'F1_Std': np.std(scores['f1'])\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_summary)\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cv_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a6b58",
   "metadata": {},
   "source": [
    "## 📋 Key Takeaways and Best Practices\n",
    "\n",
    "### **Multi-Class Classification Insights:**\n",
    "- **Class Balance**: Ensure balanced representation across all classes\n",
    "- **Feature Selection**: TF-IDF with n-grams works well for most text classification tasks\n",
    "- **Algorithm Choice**: Naive Bayes and Logistic Regression are excellent starting points\n",
    "- **Evaluation Metrics**: Use F1-score for imbalanced datasets, accuracy for balanced ones\n",
    "\n",
    "### **Performance Optimization:**\n",
    "- **Preprocessing**: Proper text cleaning significantly improves performance\n",
    "- **Feature Engineering**: Experiment with different n-gram ranges and max_features\n",
    "- **Cross-Validation**: Always use stratified CV for multi-class problems\n",
    "- **Hyperparameter Tuning**: Grid search can improve model performance\n",
    "\n",
    "### **Real-World Applications:**\n",
    "- **Email Classification**: Spam detection, priority classification\n",
    "- **News Categorization**: Automatic content organization\n",
    "- **Customer Support**: Ticket routing and prioritization\n",
    "- **Content Moderation**: Automatic content filtering\n",
    "\n",
    "### **Common Challenges:**\n",
    "- **Class Imbalance**: Use SMOTE, class weights, or stratified sampling\n",
    "- **New Categories**: Consider online learning or model retraining\n",
    "- **Multilingual Text**: Language detection and separate models\n",
    "- **Short Text**: Consider context expansion or embeddings\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **Advanced Techniques**: Word embeddings, BERT, transformers\n",
    "2. **Ensemble Methods**: Combine multiple models for better performance\n",
    "3. **Active Learning**: Improve models with minimal labeling effort\n",
    "4. **Production Deployment**: API development and model serving"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
