{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e63e75a",
   "metadata": {},
   "source": [
    "# üé≠ Sentiment Analysis Deep Dive\n",
    "\n",
    "This notebook explores advanced sentiment analysis techniques including:\n",
    "- Multiple classification algorithms\n",
    "- Feature engineering strategies\n",
    "- Model evaluation and comparison\n",
    "- Real-world application scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2061e",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e2830",
   "metadata": {},
   "source": [
    "## üìä Create Comprehensive Dataset\n",
    "\n",
    "Let's create a more comprehensive dataset for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252bb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended movie reviews dataset\n",
    "positive_reviews = [\n",
    "    \"This movie is absolutely fantastic and amazing\",\n",
    "    \"Great film with excellent acting and superb direction\",\n",
    "    \"I love this movie so much, brilliant performance\",\n",
    "    \"Outstanding cinematography and incredible storyline\",\n",
    "    \"Perfect movie with amazing special effects\",\n",
    "    \"Excellent script and wonderful character development\",\n",
    "    \"Brilliant acting and fantastic plot twists\",\n",
    "    \"Amazing movie that exceeded all my expectations\",\n",
    "    \"Superb direction and outstanding performances\",\n",
    "    \"Incredible film with great emotional depth\",\n",
    "    \"Best movie I have seen this year\",\n",
    "    \"Wonderful story and excellent execution\",\n",
    "    \"Fantastic movie with great chemistry between actors\",\n",
    "    \"Amazing visual effects and compelling narrative\",\n",
    "    \"Perfect blend of action and emotion\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This is a terrible and awful movie\",\n",
    "    \"Worst film I have ever seen, completely boring\",\n",
    "    \"I hate this movie, waste of time\",\n",
    "    \"Poor storyline and terrible acting\",\n",
    "    \"Disappointing movie with bad direction\",\n",
    "    \"Horrible script and weak performances\",\n",
    "    \"Awful movie with no redeeming qualities\",\n",
    "    \"Terrible plot and boring characters\",\n",
    "    \"Bad movie that failed to deliver\",\n",
    "    \"Worst acting and poor cinematography\",\n",
    "    \"Disappointing film with weak storyline\",\n",
    "    \"Boring movie with terrible dialogue\",\n",
    "    \"Poor execution and bad special effects\",\n",
    "    \"Awful direction and weak character development\",\n",
    "    \"Terrible movie that wasted great potential\"\n",
    "]\n",
    "\n",
    "# Combine reviews and create labels\n",
    "all_reviews = positive_reviews + negative_reviews\n",
    "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': all_reviews,\n",
    "    'sentiment': labels,\n",
    "    'sentiment_label': ['Positive' if label == 1 else 'Negative' for label in labels]\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "print(f\"Positive reviews: {sum(labels)}\")\n",
    "print(f\"Negative reviews: {len(labels) - sum(labels)}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3edc47",
   "metadata": {},
   "source": [
    "## üîç Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"Text Statistics:\")\n",
    "print(df.groupby('sentiment_label')[['text_length', 'word_count']].agg(['mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text statistics\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "df['sentiment_label'].value_counts().plot(kind='bar', ax=ax1, color=['red', 'green'], alpha=0.7)\n",
    "ax1.set_title('Sentiment Distribution')\n",
    "ax1.set_xlabel('Sentiment')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Text length distribution by sentiment\n",
    "sns.boxplot(data=df, x='sentiment_label', y='text_length', ax=ax2)\n",
    "ax2.set_title('Text Length Distribution by Sentiment')\n",
    "\n",
    "# Word count distribution by sentiment\n",
    "sns.boxplot(data=df, x='sentiment_label', y='word_count', ax=ax3)\n",
    "ax3.set_title('Word Count Distribution by Sentiment')\n",
    "\n",
    "# Text length histogram\n",
    "df[df['sentiment_label'] == 'Positive']['text_length'].hist(alpha=0.7, label='Positive', ax=ax4, color='green')\n",
    "df[df['sentiment_label'] == 'Negative']['text_length'].hist(alpha=0.7, label='Negative', ax=ax4, color='red')\n",
    "ax4.set_title('Text Length Histogram')\n",
    "ax4.set_xlabel('Text Length')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8145c1",
   "metadata": {},
   "source": [
    "## üßπ Advanced Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Advanced text preprocessing function\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['text'].apply(advanced_text_preprocessing)\n",
    "\n",
    "print(\"Original vs Processed Text Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df.iloc[i]['text']}\")\n",
    "    print(f\"Processed: {df.iloc[i]['processed_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88407188",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99556efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for positive and negative sentiments\n",
    "positive_text = ' '.join(df[df['sentiment'] == 1]['processed_text'])\n",
    "negative_text = ' '.join(df[df['sentiment'] == 0]['processed_text'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Positive word cloud\n",
    "wordcloud_pos = WordCloud(width=400, height=400, background_color='white', \n",
    "                          colormap='Greens').generate(positive_text)\n",
    "ax1.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "ax1.set_title('Positive Reviews Word Cloud', fontsize=16)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Negative word cloud\n",
    "wordcloud_neg = WordCloud(width=400, height=400, background_color='white',\n",
    "                          colormap='Reds').generate(negative_text)\n",
    "ax2.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "ax2.set_title('Negative Reviews Word Cloud', fontsize=16)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2cf17",
   "metadata": {},
   "source": [
    "## üéØ Feature Engineering Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655913fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df['processed_text']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training set positive ratio: {y_train.mean():.2f}\")\n",
    "print(f\"Test set positive ratio: {y_test.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862daf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different vectorizers\n",
    "vectorizers = {\n",
    "    'BoW (1-gram)': CountVectorizer(max_features=1000, ngram_range=(1, 1)),\n",
    "    'BoW (1-2 gram)': CountVectorizer(max_features=1000, ngram_range=(1, 2)),\n",
    "    'TF-IDF (1-gram)': TfidfVectorizer(max_features=1000, ngram_range=(1, 1)),\n",
    "    'TF-IDF (1-2 gram)': TfidfVectorizer(max_features=1000, ngram_range=(1, 2)),\n",
    "    'TF-IDF (1-3 gram)': TfidfVectorizer(max_features=1000, ngram_range=(1, 3))\n",
    "}\n",
    "\n",
    "# Store feature matrices\n",
    "feature_matrices = {}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    feature_matrices[name] = {\n",
    "        'train': X_train_vec,\n",
    "        'test': X_test_vec,\n",
    "        'vectorizer': vectorizer\n",
    "    }\n",
    "    print(f\"{name}: {X_train_vec.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0030459",
   "metadata": {},
   "source": [
    "## ü§ñ Multiple Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define algorithms\n",
    "algorithms = {\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Bernoulli NB': BernoulliNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Test each combination\n",
    "for feature_name, feature_data in feature_matrices.items():\n",
    "    X_train_features = feature_data['train']\n",
    "    X_test_features = feature_data['test']\n",
    "    \n",
    "    for algo_name, algorithm in algorithms.items():\n",
    "        # Train the model\n",
    "        algorithm.fit(X_train_features, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = algorithm.predict(X_test_features)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(algorithm, X_train_features, y_train, cv=5, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "        \n",
    "        results.append({\n",
    "            'Features': feature_name,\n",
    "            'Algorithm': algo_name,\n",
    "            'Test_Accuracy': accuracy,\n",
    "            'CV_Mean': cv_mean,\n",
    "            'CV_Std': cv_std\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"Algorithm Performance Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55015354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Test accuracy comparison\n",
    "pivot_test = results_df.pivot(index='Algorithm', columns='Features', values='Test_Accuracy')\n",
    "sns.heatmap(pivot_test, annot=True, cmap='RdYlGn', ax=ax1, fmt='.3f')\n",
    "ax1.set_title('Test Accuracy Heatmap')\n",
    "ax1.set_xlabel('Feature Engineering')\n",
    "ax1.set_ylabel('Algorithm')\n",
    "\n",
    "# CV accuracy comparison\n",
    "pivot_cv = results_df.pivot(index='Algorithm', columns='Features', values='CV_Mean')\n",
    "sns.heatmap(pivot_cv, annot=True, cmap='RdYlGn', ax=ax2, fmt='.3f')\n",
    "ax2.set_title('Cross-Validation Accuracy Heatmap')\n",
    "ax2.set_xlabel('Feature Engineering')\n",
    "ax2.set_ylabel('Algorithm')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a1849",
   "metadata": {},
   "source": [
    "## üéØ Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing combination\n",
    "best_result = results_df.loc[results_df['Test_Accuracy'].idxmax()]\n",
    "print(\"Best Performing Model:\")\n",
    "print(f\"Features: {best_result['Features']}\")\n",
    "print(f\"Algorithm: {best_result['Algorithm']}\")\n",
    "print(f\"Test Accuracy: {best_result['Test_Accuracy']:.4f}\")\n",
    "print(f\"CV Mean: {best_result['CV_Mean']:.4f} ¬± {best_result['CV_Std']:.4f}\")\n",
    "\n",
    "# Train the best model\n",
    "best_features = best_result['Features']\n",
    "best_algo = best_result['Algorithm']\n",
    "\n",
    "X_train_best = feature_matrices[best_features]['train']\n",
    "X_test_best = feature_matrices[best_features]['test']\n",
    "best_vectorizer = feature_matrices[best_features]['vectorizer']\n",
    "\n",
    "best_model = algorithms[best_algo]\n",
    "best_model.fit(X_train_best, y_train)\n",
    "y_pred_best = best_model.predict(X_test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98989e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'], \n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title(f'Confusion Matrix - {best_algo} with {best_features}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac57e6",
   "metadata": {},
   "source": [
    "## üìä ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves for different algorithms\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "# Use TF-IDF (1-2 gram) features for comparison\n",
    "feature_name = 'TF-IDF (1-2 gram)'\n",
    "X_train_roc = feature_matrices[feature_name]['train']\n",
    "X_test_roc = feature_matrices[feature_name]['test']\n",
    "\n",
    "for i, (algo_name, algorithm) in enumerate(algorithms.items()):\n",
    "    # Train model\n",
    "    algorithm.fit(X_train_roc, y_train)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    if hasattr(algorithm, \"predict_proba\"):\n",
    "        y_proba = algorithm.predict_proba(X_test_roc)[:, 1]\n",
    "    else:\n",
    "        y_proba = algorithm.decision_function(X_test_roc)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "             label=f'{algo_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curves Comparison - {feature_name}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5db788",
   "metadata": {},
   "source": [
    "## üîç Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10759dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for Logistic Regression\n",
    "if best_algo == 'Logistic Regression':\n",
    "    feature_names = best_vectorizer.get_feature_names_out()\n",
    "    coefficients = best_model.coef_[0]\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefficients,\n",
    "        'abs_coefficient': np.abs(coefficients)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    # Plot top features\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Top positive features\n",
    "    top_positive = feature_importance.nlargest(15, 'coefficient')\n",
    "    ax1.barh(range(len(top_positive)), top_positive['coefficient'], color='green', alpha=0.7)\n",
    "    ax1.set_yticks(range(len(top_positive)))\n",
    "    ax1.set_yticklabels(top_positive['feature'])\n",
    "    ax1.set_title('Top 15 Positive Sentiment Features')\n",
    "    ax1.set_xlabel('Coefficient Value')\n",
    "    \n",
    "    # Top negative features\n",
    "    top_negative = feature_importance.nsmallest(15, 'coefficient')\n",
    "    ax2.barh(range(len(top_negative)), top_negative['coefficient'], color='red', alpha=0.7)\n",
    "    ax2.set_yticks(range(len(top_negative)))\n",
    "    ax2.set_yticklabels(top_negative['feature'])\n",
    "    ax2.set_title('Top 15 Negative Sentiment Features')\n",
    "    ax2.set_xlabel('Coefficient Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (by absolute coefficient):\")\n",
    "    print(feature_importance.head(10)[['feature', 'coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdfd91",
   "metadata": {},
   "source": [
    "## üß™ Model Testing with New Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ca048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vectorizer, preprocessor):\n",
    "    \"\"\"\n",
    "    Predict sentiment for new text\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed_text = preprocessor(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vector = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(text_vector)[0]\n",
    "    probability = model.predict_proba(text_vector)[0]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test with new examples\n",
    "test_examples = [\n",
    "    \"This movie is absolutely incredible and mind-blowing\",\n",
    "    \"Terrible film with awful acting and boring plot\",\n",
    "    \"Great movie with some amazing scenes\",\n",
    "    \"Not the worst movie but definitely disappointing\",\n",
    "    \"Outstanding performance by all actors\",\n",
    "    \"Complete waste of time and money\",\n",
    "    \"Good movie with excellent direction\",\n",
    "    \"Poor storyline but decent acting\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Predictions on New Examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text in test_examples:\n",
    "    prediction, probability = predict_sentiment(text, best_model, best_vectorizer, advanced_text_preprocessing)\n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    confidence = max(probability)\n",
    "    \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(f\"Probabilities: Negative={probability[0]:.3f}, Positive={probability[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c4223",
   "metadata": {},
   "source": [
    "## üìà Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = results_df.groupby('Algorithm').agg({\n",
    "    'Test_Accuracy': ['mean', 'std', 'max'],\n",
    "    'CV_Mean': ['mean', 'std', 'max']\n",
    "}).round(4)\n",
    "\n",
    "print(\"Algorithm Performance Summary:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Best algorithm by average performance\n",
    "avg_performance = results_df.groupby('Algorithm')['Test_Accuracy'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAlgorithms Ranked by Average Test Accuracy:\")\n",
    "for i, (algo, score) in enumerate(avg_performance.items(), 1):\n",
    "    print(f\"{i}. {algo}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b16cc6",
   "metadata": {},
   "source": [
    "## üéØ Key Insights and Recommendations\n",
    "\n",
    "### **Performance Insights:**\n",
    "- **Best Overall Performance**: The analysis shows which algorithm-feature combination works best\n",
    "- **Feature Engineering Impact**: N-grams and TF-IDF typically outperform simple Bag of Words\n",
    "- **Algorithm Strengths**: Different algorithms excel with different feature representations\n",
    "\n",
    "### **Model Selection Guidelines:**\n",
    "- **For Interpretability**: Logistic Regression with TF-IDF\n",
    "- **For Small Datasets**: Naive Bayes (especially Multinomial NB)\n",
    "- **For Complex Patterns**: SVM or Random Forest\n",
    "- **For Speed**: Naive Bayes or Logistic Regression\n",
    "\n",
    "### **Feature Engineering Recommendations:**\n",
    "- **Always use TF-IDF** over simple Bag of Words\n",
    "- **Include bigrams** (1-2 grams) for better context capture\n",
    "- **Proper preprocessing** is crucial for performance\n",
    "- **Consider domain-specific stop words** removal\n",
    "\n",
    "### **Next Steps for Improvement:**\n",
    "1. **Larger Dataset**: Collect more diverse reviews\n",
    "2. **Advanced Features**: Sentiment lexicons, POS tags\n",
    "3. **Deep Learning**: Try LSTM, BERT for comparison\n",
    "4. **Ensemble Methods**: Combine multiple models\n",
    "5. **Hyperparameter Tuning**: Grid search for optimal parameters\n",
    "\n",
    "### **Real-World Application Tips:**\n",
    "- **Monitor Performance**: Regularly evaluate on new data\n",
    "- **Handle Class Imbalance**: Use appropriate sampling techniques\n",
    "- **Domain Adaptation**: Retrain for different domains (movies vs products)\n",
    "- **Confidence Thresholds**: Set minimum confidence for predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
