{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d1b9d1",
   "metadata": {},
   "source": [
    "# 📝 Bag of Words and TF-IDF\n",
    "\n",
    "In this notebook, we'll explore two fundamental text representation techniques:\n",
    "- **Bag of Words (BoW)**: Simple word frequency counting\n",
    "- **TF-IDF**: Term Frequency-Inverse Document Frequency weighting\n",
    "\n",
    "These techniques convert text into numerical features that machine learning algorithms can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e94e07",
   "metadata": {},
   "source": [
    "## 📚 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f4f7c",
   "metadata": {},
   "source": [
    "## 📊 Sample Dataset Creation\n",
    "\n",
    "Let's create a sample dataset to understand BoW and TF-IDF concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews dataset\n",
    "documents = [\n",
    "    \"This movie is fantastic and amazing\",\n",
    "    \"Great movie with excellent acting\",\n",
    "    \"I love this movie so much\",\n",
    "    \"This is a terrible movie\",\n",
    "    \"Worst movie I have ever seen\",\n",
    "    \"I hate this boring movie\",\n",
    "    \"The acting is superb in this film\",\n",
    "    \"Amazing cinematography and great story\",\n",
    "    \"Poor storyline and bad acting\",\n",
    "    \"Excellent direction and fantastic cast\"\n",
    "]\n",
    "\n",
    "# Labels: 1 = positive, 0 = negative\n",
    "labels = [1, 1, 1, 0, 0, 0, 1, 1, 0, 1]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': documents,\n",
    "    'sentiment': labels\n",
    "})\n",
    "\n",
    "print(\"Sample Dataset:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2fe43",
   "metadata": {},
   "source": [
    "## 🎒 Bag of Words (BoW)\n",
    "\n",
    "Bag of Words creates a vocabulary of all unique words and counts their occurrences in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of Words vectorizer\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    lowercase=True,           # Convert to lowercase\n",
    "    stop_words='english',     # Remove English stop words\n",
    "    max_features=100          # Limit vocabulary size\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"Vocabulary: {list(feature_names)}\")\n",
    "print(f\"\\nBoW matrix shape: {bow_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dense matrix for visualization\n",
    "bow_dense = bow_matrix.toarray()\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_dense, columns=feature_names)\n",
    "bow_df['document'] = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
    "bow_df['text'] = documents\n",
    "\n",
    "print(\"Bag of Words Matrix:\")\n",
    "print(bow_df[['document'] + list(feature_names[:10])].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb11c2",
   "metadata": {},
   "source": [
    "## 📊 Visualize Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d37a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word frequencies across all documents\n",
    "word_freq = np.sum(bow_dense, axis=0)\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'frequency': word_freq\n",
    "}).sort_values('frequency', ascending=False)\n",
    "\n",
    "# Plot word frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(data=word_freq_df.head(10), x='frequency', y='word')\n",
    "plt.title('Top 10 Most Frequent Words (BoW)')\n",
    "plt.xlabel('Frequency')\n",
    "\n",
    "# Show word distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(word_freq, bins=10, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Word Frequencies')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of Words')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb17939",
   "metadata": {},
   "source": [
    "## 🔢 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF weights words based on:\n",
    "- **Term Frequency (TF)**: How often a word appears in a document\n",
    "- **Inverse Document Frequency (IDF)**: How rare/common a word is across all documents\n",
    "\n",
    "**Formula**: TF-IDF = TF × IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e02ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=100,\n",
    "    ngram_range=(1, 2)        # Include unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_features)}\")\n",
    "print(f\"Sample features: {list(tfidf_features[:15])}\")\n",
    "print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dense matrix\n",
    "tfidf_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_dense, columns=tfidf_features)\n",
    "tfidf_df['document'] = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
    "\n",
    "print(\"TF-IDF Matrix (first 5 features):\")\n",
    "print(tfidf_df[['document'] + list(tfidf_features[:5])].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432aa91",
   "metadata": {},
   "source": [
    "## 📊 Compare BoW vs TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01699bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scores for a specific word across documents\n",
    "word_to_compare = 'movie'\n",
    "\n",
    "if word_to_compare in feature_names:\n",
    "    bow_idx = list(feature_names).index(word_to_compare)\n",
    "    bow_scores = bow_dense[:, bow_idx]\n",
    "    \n",
    "    if word_to_compare in tfidf_features:\n",
    "        tfidf_idx = list(tfidf_features).index(word_to_compare)\n",
    "        tfidf_scores = tfidf_dense[:, tfidf_idx]\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Document': [f\"Doc {i+1}\" for i in range(len(documents))],\n",
    "            'Text': [doc[:50] + '...' if len(doc) > 50 else doc for doc in documents],\n",
    "            'BoW Score': bow_scores,\n",
    "            'TF-IDF Score': tfidf_scores.round(3)\n",
    "        })\n",
    "        \n",
    "        print(f\"Comparison for word '{word_to_compare}':\")\n",
    "        print(comparison_df)\n",
    "    else:\n",
    "        print(f\"Word '{word_to_compare}' not found in TF-IDF features\")\n",
    "else:\n",
    "    print(f\"Word '{word_to_compare}' not found in BoW features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e296148",
   "metadata": {},
   "source": [
    "## 🤖 Machine Learning with BoW and TF-IDF\n",
    "\n",
    "Let's compare how BoW and TF-IDF perform in a classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d542b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    documents, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e273b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with BoW features\n",
    "bow_vectorizer_train = CountVectorizer(lowercase=True, stop_words='english')\n",
    "X_train_bow = bow_vectorizer_train.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer_train.transform(X_test)\n",
    "\n",
    "# Naive Bayes with BoW\n",
    "nb_bow = MultinomialNB()\n",
    "nb_bow.fit(X_train_bow, y_train)\n",
    "y_pred_nb_bow = nb_bow.predict(X_test_bow)\n",
    "\n",
    "# Logistic Regression with BoW\n",
    "lr_bow = LogisticRegression(random_state=42)\n",
    "lr_bow.fit(X_train_bow, y_train)\n",
    "y_pred_lr_bow = lr_bow.predict(X_test_bow)\n",
    "\n",
    "print(\"BoW Results:\")\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb_bow):.3f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr_bow):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58448259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with TF-IDF features\n",
    "tfidf_vectorizer_train = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer_train.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer_train.transform(X_test)\n",
    "\n",
    "# Naive Bayes with TF-IDF\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Logistic Regression with TF-IDF\n",
    "lr_tfidf = LogisticRegression(random_state=42)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"TF-IDF Results:\")\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb_tfidf):.3f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr_tfidf):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74779290",
   "metadata": {},
   "source": [
    "## 📊 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Logistic Regression (TF-IDF)\n",
    "feature_names_tfidf = tfidf_vectorizer_train.get_feature_names_out()\n",
    "coefficients = lr_tfidf.coef_[0]\n",
    "\n",
    "# Create DataFrame with feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names_tfidf,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# Plot top positive and negative features\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top positive features (positive sentiment indicators)\n",
    "top_positive = feature_importance.nlargest(10, 'coefficient')\n",
    "ax1.barh(range(len(top_positive)), top_positive['coefficient'], color='green', alpha=0.7)\n",
    "ax1.set_yticks(range(len(top_positive)))\n",
    "ax1.set_yticklabels(top_positive['feature'])\n",
    "ax1.set_title('Top Positive Sentiment Features')\n",
    "ax1.set_xlabel('Coefficient Value')\n",
    "\n",
    "# Top negative features (negative sentiment indicators)\n",
    "top_negative = feature_importance.nsmallest(10, 'coefficient')\n",
    "ax2.barh(range(len(top_negative)), top_negative['coefficient'], color='red', alpha=0.7)\n",
    "ax2.set_yticks(range(len(top_negative)))\n",
    "ax2.set_yticklabels(top_negative['feature'])\n",
    "ax2.set_title('Top Negative Sentiment Features')\n",
    "ax2.set_xlabel('Coefficient Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53af6ed",
   "metadata": {},
   "source": [
    "## 🔍 Testing with New Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ab8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new examples\n",
    "new_reviews = [\n",
    "    \"This is an amazing and fantastic film\",\n",
    "    \"Terrible movie with poor acting\",\n",
    "    \"Great storyline and excellent direction\",\n",
    "    \"Worst film I have ever watched\"\n",
    "]\n",
    "\n",
    "# Transform new reviews\n",
    "new_reviews_tfidf = tfidf_vectorizer_train.transform(new_reviews)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_tfidf.predict(new_reviews_tfidf)\n",
    "probabilities = lr_tfidf.predict_proba(new_reviews_tfidf)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Review': new_reviews,\n",
    "    'Predicted_Sentiment': ['Positive' if p == 1 else 'Negative' for p in predictions],\n",
    "    'Confidence': [max(prob) for prob in probabilities]\n",
    "})\n",
    "\n",
    "print(\"Predictions on New Reviews:\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"\\nReview: {row['Review']}\")\n",
    "    print(f\"Sentiment: {row['Predicted_Sentiment']}\")\n",
    "    print(f\"Confidence: {row['Confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76117b9",
   "metadata": {},
   "source": [
    "## 📋 Key Takeaways\n",
    "\n",
    "### **Bag of Words (BoW):**\n",
    "- ✅ Simple and intuitive\n",
    "- ✅ Fast to compute\n",
    "- ✅ Good baseline for text classification\n",
    "- ❌ Ignores word importance\n",
    "- ❌ High-dimensional and sparse\n",
    "\n",
    "### **TF-IDF:**\n",
    "- ✅ Weights words by importance\n",
    "- ✅ Better performance than BoW\n",
    "- ✅ Good for information retrieval\n",
    "- ❌ More complex than BoW\n",
    "- ❌ Still loses word order information\n",
    "\n",
    "### **When to Use:**\n",
    "- **BoW**: Simple classification tasks, quick prototypes\n",
    "- **TF-IDF**: Better classification performance, information retrieval\n",
    "- **Both**: Good starting points before trying deep learning approaches\n",
    "\n",
    "### **Next Steps:**\n",
    "- Experiment with n-grams (bigrams, trigrams)\n",
    "- Try different preprocessing techniques\n",
    "- Explore word embeddings (Word2Vec, GloVe)\n",
    "- Compare with deep learning approaches"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
