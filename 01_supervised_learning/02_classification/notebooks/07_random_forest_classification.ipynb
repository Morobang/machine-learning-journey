{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MRC0e0KhQ0S"
   },
   "source": [
    "# ğŸŒ² Random Forest Classification: The Complete Ensemble Learning Masterclass\n",
    "\n",
    "## ğŸ¯ **Master the Power of Collective Intelligence**\n",
    "\n",
    "**Welcome to the world of ensemble learning!** Random Forest harnesses the \"wisdom of crowds\" by combining hundreds of decision trees to create incredibly robust and accurate predictions. This comprehensive guide will transform you from a single-tree thinker into an ensemble learning expert.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  **What Makes Random Forest Revolutionary?**\n",
    "\n",
    "### ğŸŒ³â¡ï¸ğŸŒ² **From Single Tree to Forest**\n",
    "- **Individual Trees**: Smart but prone to overfitting\n",
    "- **Random Forest**: Combines many trees for superior performance\n",
    "- **Ensemble Power**: The whole is greater than the sum of its parts\n",
    "- **Robustness**: Reduces overfitting through collective decision-making\n",
    "\n",
    "### ğŸ¯ **The Three Pillars of Random Forest**\n",
    "\n",
    "**1. ğŸ² Bootstrap Sampling (Bagging)**\n",
    "- Each tree trained on different random subset of data\n",
    "- Reduces variance and prevents overfitting\n",
    "- Out-of-bag samples provide built-in validation\n",
    "\n",
    "**2. ğŸ”€ Random Feature Selection**\n",
    "- Each split considers only random subset of features\n",
    "- Increases tree diversity and reduces correlation\n",
    "- Prevents feature dominance in ensemble\n",
    "\n",
    "**3. ğŸ—³ï¸ Democratic Voting**\n",
    "- Classification: Majority vote wins\n",
    "- Regression: Average of all predictions\n",
    "- Confidence through vote distribution\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ **Real-World Random Forest Success Stories**\n",
    "\n",
    "### ğŸ¥ **Medical Diagnosis: COVID-19 Detection**\n",
    "```\n",
    "ğŸ”¬ Challenge: Analyze chest X-rays with 95%+ accuracy\n",
    "ğŸŒ² Solution: 500 trees voting on image features\n",
    "âœ… Result: Outperformed individual radiologists\n",
    "ğŸ’¡ Why RF: Robust to image variations and noise\n",
    "```\n",
    "\n",
    "### ğŸ’° **Finance: Credit Risk Assessment**\n",
    "```\n",
    "ğŸ“Š Challenge: Predict loan defaults with high precision\n",
    "ğŸŒ² Solution: 1000 trees analyzing 50+ financial features\n",
    "âœ… Result: 40% reduction in bad loans\n",
    "ğŸ’¡ Why RF: Handles missing data and feature interactions\n",
    "```\n",
    "\n",
    "### ğŸ›’ **E-commerce: Product Recommendation**\n",
    "```\n",
    "ğŸ¯ Challenge: Recommend products to 100M+ users\n",
    "ğŸŒ² Solution: Multiple forests for different user segments\n",
    "âœ… Result: 25% increase in conversion rates\n",
    "ğŸ’¡ Why RF: Scales well and provides feature importance\n",
    "```\n",
    "\n",
    "### ğŸ“± **Tech: Spam Detection**\n",
    "```\n",
    "ğŸš« Challenge: Block spam with minimal false positives\n",
    "ğŸŒ² Solution: Ensemble of 200 trees on email features\n",
    "âœ… Result: 99.9% accuracy with 0.01% false positive rate\n",
    "ğŸ’¡ Why RF: Handles text features and adapts to new spam types\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ **Learning Objectives: Your Ensemble Journey**\n",
    "\n",
    "By the end of this masterclass, you will:\n",
    "\n",
    "### ğŸ§® **Master Ensemble Theory**\n",
    "- âœ… Understand **Bootstrap Sampling**: Why random sampling works\n",
    "- âœ… Learn **Bagging**: Reducing variance through averaging\n",
    "- âœ… Grasp **Feature Randomness**: Breaking feature correlations\n",
    "- âœ… Master **Voting Mechanisms**: From individual to collective decisions\n",
    "\n",
    "### ğŸ› ï¸ **Excel at Implementation**\n",
    "- âœ… Build Random Forests with optimal hyperparameters\n",
    "- âœ… Analyze feature importance and tree diversity\n",
    "- âœ… Implement out-of-bag validation techniques\n",
    "- âœ… Compare single trees vs ensemble performance\n",
    "\n",
    "### ğŸ“Š **Become an Ensemble Expert**\n",
    "- âœ… Choose optimal forest size and tree parameters\n",
    "- âœ… Handle imbalanced datasets with class weighting\n",
    "- âœ… Interpret ensemble decisions and feature contributions\n",
    "- âœ… Apply Random Forests to complex business problems\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ **Our Educational Roadmap**\n",
    "\n",
    "### **Phase 1: Ensemble Foundation** ğŸ—ï¸\n",
    "1. **ğŸŒŸ Ensemble Learning Theory** - Why many beats one\n",
    "2. **ğŸ“š Bootstrap & Bagging** - The mathematics of diversity\n",
    "3. **ğŸ”¬ Dataset Ensemble Analysis** - Assessing forest suitability\n",
    "\n",
    "### **Phase 2: Forest Construction** ğŸ¤–\n",
    "4. **ğŸ§® Random Forest Algorithm** - From concept to implementation\n",
    "5. **âš™ï¸ Hyperparameter Mastery** - Building the optimal forest\n",
    "6. **ğŸ¯ Feature Importance Analysis** - Understanding what matters\n",
    "\n",
    "### **Phase 3: Advanced Applications** ğŸš€\n",
    "7. **ğŸ“Š Out-of-Bag Validation** - Built-in model assessment\n",
    "8. **ğŸ¨ Ensemble Visualization** - Seeing the forest and trees\n",
    "9. **ğŸ’¼ Business Case Studies** - Real-world forest applications\n",
    "10. **ğŸ“ Expert-Level Insights** - When forests beat everything else\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  **Random Forest vs Other Algorithms**\n",
    "\n",
    "| Aspect | Random Forest | Single Decision Tree | SVM | Logistic Regression |\n",
    "|--------|---------------|---------------------|-----|-------------------|\n",
    "| **Overfitting Resistance** | ğŸŸ¢ Excellent | ğŸ”´ Poor | ğŸŸ¡ Good | ğŸŸ¡ Good |\n",
    "| **Feature Importance** | ğŸŸ¢ Built-in | ğŸŸ¢ Built-in | ğŸ”´ Limited | ğŸŸ¡ Coefficients |\n",
    "| **Handling Missing Values** | ğŸŸ¢ Native | ğŸŸ¢ Native | ğŸ”´ Preprocessing | ğŸ”´ Preprocessing |\n",
    "| **Non-linear Relationships** | ğŸŸ¢ Excellent | ğŸŸ¢ Excellent | ğŸŸ¡ With Kernels | ğŸ”´ Linear Only |\n",
    "| **Training Speed** | ğŸŸ¡ Moderate | ğŸŸ¢ Fast | ğŸ”´ Slow | ğŸŸ¢ Fast |\n",
    "| **Interpretability** | ğŸŸ¡ Moderate | ğŸŸ¢ Excellent | ğŸ”´ Poor | ğŸŸ¢ Good |\n",
    "| **Large Datasets** | ğŸŸ¢ Scales Well | ğŸŸ¡ Memory Limited | ğŸ”´ Memory Intensive | ğŸŸ¢ Efficient |\n",
    "| **Robustness to Outliers** | ğŸŸ¢ Very Robust | ğŸ”´ Sensitive | ğŸ”´ Sensitive | ğŸ”´ Sensitive |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **What You'll Build Today**\n",
    "\n",
    "We'll create a **Customer Purchase Prediction Ensemble** that:\n",
    "- ğŸŒ² Builds a forest of 100 decision trees\n",
    "- ğŸ² Uses bootstrap sampling for tree diversity\n",
    "- ğŸ”€ Implements random feature selection\n",
    "- ğŸ“Š Analyzes feature importance rankings\n",
    "- ğŸ¯ Compares ensemble vs individual tree performance\n",
    "- ğŸ“ˆ Visualizes the power of collective intelligence\n",
    "- ğŸ’¼ Provides actionable business insights\n",
    "\n",
    "### ğŸŒŸ **The Forest Advantage**\n",
    "- **Accuracy**: 15-25% improvement over single trees\n",
    "- **Robustness**: Handles outliers and missing data\n",
    "- **Interpretability**: Feature importance rankings\n",
    "- **Scalability**: Parallel training of individual trees\n",
    "- **Confidence**: Vote distribution shows prediction certainty\n",
    "\n",
    "**Let's harness the wisdom of the forest! ğŸŒ²ğŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWd1UlMnhT2s"
   },
   "source": [
    "## ğŸ“š Random Forest Specialized Libraries & Ensemble Tools\n",
    "\n",
    "**Setting up the complete toolkit for ensemble learning, bootstrap sampling, and forest visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvGPUQaHhXfL"
   },
   "outputs": [],
   "source": [
    "# Random Forest Specialized Library Setup\n",
    "print(\"ğŸŒ² RANDOM FOREST ENSEMBLE LIBRARY SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Core Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Random Forest & Ensemble Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, \n",
    "                                   RandomizedSearchCV, validation_curve,\n",
    "                                   cross_val_score, learning_curve)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, roc_curve, auc)\n",
    "\n",
    "# Bootstrap Sampling & Statistical Analysis\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Advanced Visualization Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Enhanced Plotting Configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Seaborn styling for ensemble visualizations\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… CORE ENSEMBLE LIBRARIES IMPORTED:\")\n",
    "print(\"   ğŸŒ² RandomForestClassifier - Main ensemble algorithm\")\n",
    "print(\"   ğŸ„ ExtraTreesClassifier - Extremely randomized trees\")\n",
    "print(\"   ğŸŒ³ DecisionTreeClassifier - Individual tree analysis\")\n",
    "print(\"   ğŸ“Š Bootstrap sampling utilities\")\n",
    "print(\"   ğŸ“ˆ Cross-validation tools\")\n",
    "\n",
    "print(\"\\nğŸ¯ SPECIALIZED ENSEMBLE FEATURES:\")\n",
    "print(\"   ğŸ² Bootstrap sampling (resample)\")\n",
    "print(\"   ğŸ—³ï¸ Majority voting mechanisms\")\n",
    "print(\"   ğŸ“Š Feature importance analysis\")\n",
    "print(\"   ğŸ¨ Forest visualization tools\")\n",
    "print(\"   ğŸ“ˆ Out-of-bag error estimation\")\n",
    "print(\"   ğŸ” Individual tree inspection\")\n",
    "\n",
    "print(\"\\nğŸ§® ENSEMBLE ANALYSIS TOOLKIT:\")\n",
    "print(\"   ğŸ“ Bias-variance decomposition\")\n",
    "print(\"   ğŸ“Š Tree diversity measurement\")\n",
    "print(\"   ğŸ¯ Feature randomness analysis\")\n",
    "print(\"   ğŸ“ˆ Learning curve visualization\")\n",
    "print(\"   ğŸ”¬ Hyperparameter optimization\")\n",
    "\n",
    "# Set random seed for reproducible ensembles\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"\\nğŸ² Random seed set to 42 for reproducible forest generation\")\n",
    "print(f\"ğŸŒ² Ready for ensemble learning!\")\n",
    "\n",
    "# Quick library version check\n",
    "import sklearn\n",
    "print(f\"\\nğŸ“‹ ENSEMBLE LIBRARY VERSIONS:\")\n",
    "print(f\"   Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")\n",
    "print(f\"   Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"   Seaborn: {sns.__version__}\")\n",
    "\n",
    "# Test ensemble functionality\n",
    "print(f\"\\nğŸ§ª ENSEMBLE FUNCTIONALITY TEST:\")\n",
    "# Create a simple test forest\n",
    "test_X = np.random.rand(100, 4)\n",
    "test_y = np.random.randint(0, 2, 100)\n",
    "\n",
    "test_forest = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "test_forest.fit(test_X, test_y)\n",
    "\n",
    "print(f\"   âœ… Random Forest instantiation: SUCCESS\")\n",
    "print(f\"   âœ… Forest training: SUCCESS\") \n",
    "print(f\"   âœ… Tree count: {len(test_forest.estimators_)} trees\")\n",
    "print(f\"   âœ… Feature importance available: {len(test_forest.feature_importances_)} features\")\n",
    "print(f\"   âœ… Out-of-bag scoring: {'Available' if hasattr(test_forest, 'oob_score_') else 'Requires oob_score=True'}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Random Forest ecosystem ready for ensemble learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1VMqkGvhc3-"
   },
   "source": [
    "## ğŸ“Š Dataset Loading & Ensemble Learning Assessment\n",
    "\n",
    "**Loading our dataset with a focus on ensemble suitability, bootstrap sampling potential, and feature diversity analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M52QDmyzhh9s"
   },
   "outputs": [],
   "source": [
    "# Random Forest Optimized Dataset Loading\n",
    "print(\"ğŸ“Š RANDOM FOREST DATASET LOADING & ENSEMBLE ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load dataset with ensemble learning focus\n",
    "print(\"ğŸŒ² Loading Social Network Ads dataset for ensemble analysis...\")\n",
    "\n",
    "try:\n",
    "    dataset = pd.read_csv('../01_Data/Social_Network_Ads.csv')\n",
    "    print(\"âœ… Dataset loaded successfully from ../01_Data/\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Trying alternative path...\")\n",
    "    try:\n",
    "        dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "        print(\"âœ… Dataset loaded from current directory\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ğŸ“ Creating demonstration dataset optimized for Random Forest...\")\n",
    "        # Create synthetic data with ensemble-friendly patterns\n",
    "        np.random.seed(42)\n",
    "        n_samples = 400\n",
    "        \n",
    "        # Generate data with complex patterns (perfect for ensembles)\n",
    "        # Multiple overlapping patterns that individual trees might miss\n",
    "        \n",
    "        # Age groups with varying behaviors\n",
    "        age_means = [25, 35, 45, 55]\n",
    "        age_stds = [4, 5, 6, 4]\n",
    "        ages = []\n",
    "        \n",
    "        for i in range(4):\n",
    "            group_size = n_samples // 4\n",
    "            group_ages = np.random.normal(age_means[i], age_stds[i], group_size)\n",
    "            ages.extend(group_ages)\n",
    "        \n",
    "        ages = np.array(ages)\n",
    "        ages = np.clip(ages, 18, 60).astype(int)\n",
    "        \n",
    "        # Salary with complex interactions\n",
    "        salaries = np.zeros(n_samples)\n",
    "        for i, age in enumerate(ages):\n",
    "            # Base salary with age correlation + noise\n",
    "            base_salary = 25000 + (age - 18) * 1200 + np.random.normal(0, 8000)\n",
    "            \n",
    "            # Add some non-linear patterns\n",
    "            if 30 <= age <= 40:  # Prime earning years boost\n",
    "                base_salary *= 1.3\n",
    "            if age > 50:  # Senior discount in some cases\n",
    "                base_salary *= np.random.choice([0.9, 1.1], p=[0.3, 0.7])\n",
    "            \n",
    "            salaries[i] = base_salary\n",
    "        \n",
    "        salaries = np.clip(salaries, 15000, 150000).astype(int)\n",
    "        \n",
    "        # Complex purchase patterns (ideal for ensemble learning)\n",
    "        purchased = np.zeros(n_samples, dtype=int)\n",
    "        for i in range(n_samples):\n",
    "            # Multiple overlapping decision patterns\n",
    "            prob = 0.1  # Base probability\n",
    "            \n",
    "            # Pattern 1: Age-based\n",
    "            if ages[i] >= 35:\n",
    "                prob += 0.3\n",
    "            if ages[i] >= 45:\n",
    "                prob += 0.2\n",
    "            \n",
    "            # Pattern 2: Salary-based\n",
    "            if salaries[i] >= 60000:\n",
    "                prob += 0.4\n",
    "            if salaries[i] >= 80000:\n",
    "                prob += 0.2\n",
    "            \n",
    "            # Pattern 3: Interaction effects\n",
    "            if ages[i] >= 35 and salaries[i] >= 60000:\n",
    "                prob += 0.3  # Synergy effect\n",
    "            \n",
    "            # Pattern 4: Some non-linear boundaries\n",
    "            if 25 <= ages[i] <= 35 and 40000 <= salaries[i] <= 70000:\n",
    "                prob += 0.25  # Young professionals segment\n",
    "            \n",
    "            # Add some randomness\n",
    "            prob += np.random.normal(0, 0.1)\n",
    "            prob = np.clip(prob, 0.05, 0.95)\n",
    "            \n",
    "            purchased[i] = np.random.binomial(1, prob)\n",
    "        \n",
    "        dataset = pd.DataFrame({\n",
    "            'User ID': range(1, n_samples + 1),\n",
    "            'Gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "            'Age': ages,\n",
    "            'EstimatedSalary': salaries,\n",
    "            'Purchased': purchased\n",
    "        })\n",
    "        print(\"âœ… Synthetic ensemble-optimized dataset created\")\n",
    "\n",
    "# Display basic dataset information\n",
    "print(f\"\\nğŸŒ² RANDOM FOREST DATASET OVERVIEW:\")\n",
    "print(f\"   ğŸ“ Dataset shape: {dataset.shape}\")\n",
    "print(f\"   ğŸ“Š Features: {list(dataset.columns[:-1])}\")\n",
    "print(f\"   ğŸ¯ Target: {dataset.columns[-1]}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nğŸ“‹ SAMPLE DATA:\")\n",
    "print(dataset.head())\n",
    "\n",
    "# Extract features and target for ensemble analysis\n",
    "X = dataset.iloc[:, [2, 3]].values  # Age and EstimatedSalary\n",
    "y = dataset.iloc[:, -1].values      # Purchased\n",
    "\n",
    "print(f\"\\nğŸ¯ ENSEMBLE LEARNING ASSESSMENT:\")\n",
    "print(f\"   ğŸ“Š Feature matrix shape: {X.shape}\")\n",
    "print(f\"   ğŸ¯ Target vector shape: {y.shape}\")\n",
    "print(f\"   ğŸ“ˆ Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y)\n",
    "class_balance = class_counts / len(y)\n",
    "print(f\"   âš–ï¸  Class balance ratio: {class_balance[0]:.3f} (No) : {class_balance[1]:.3f} (Yes)\")\n",
    "\n",
    "# Ensemble suitability analysis\n",
    "print(f\"\\nğŸŒ² RANDOM FOREST SUITABILITY ANALYSIS:\")\n",
    "\n",
    "# 1. Dataset size assessment\n",
    "print(f\"   ğŸ“Š Dataset size assessment:\")\n",
    "if len(X) >= 100:\n",
    "    print(f\"      âœ… Sample size ({len(X)}) sufficient for ensemble learning\")\n",
    "else:\n",
    "    print(f\"      âš ï¸  Small dataset ({len(X)}) - consider bootstrap aggregation benefits\")\n",
    "\n",
    "# 2. Feature diversity\n",
    "print(f\"   ğŸ”€ Feature diversity analysis:\")\n",
    "feature_names = ['Age', 'Salary']\n",
    "correlations = np.corrcoef(X.T)\n",
    "print(f\"      â€¢ Age-Salary correlation: {correlations[0,1]:.3f}\")\n",
    "if abs(correlations[0,1]) < 0.8:\n",
    "    print(f\"      âœ… Features sufficiently diverse for random selection\")\n",
    "else:\n",
    "    print(f\"      âš ï¸  High correlation - random feature selection less beneficial\")\n",
    "\n",
    "# 3. Non-linearity assessment\n",
    "def assess_non_linearity(feature, target):\n",
    "    \"\"\"Simple non-linearity assessment using correlation vs rank correlation\"\"\"\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    pearson_r, _ = pearsonr(feature, target)\n",
    "    spearman_r, _ = spearmanr(feature, target)\n",
    "    return abs(spearman_r - pearson_r)\n",
    "\n",
    "age_nonlin = assess_non_linearity(X[:, 0], y)\n",
    "salary_nonlin = assess_non_linearity(X[:, 1], y)\n",
    "\n",
    "print(f\"   ğŸ“ˆ Non-linearity assessment:\")\n",
    "print(f\"      â€¢ Age non-linearity score: {age_nonlin:.3f}\")\n",
    "print(f\"      â€¢ Salary non-linearity score: {salary_nonlin:.3f}\")\n",
    "if max(age_nonlin, salary_nonlin) > 0.1:\n",
    "    print(f\"      âœ… Non-linear patterns detected - ensemble will help\")\n",
    "else:\n",
    "    print(f\"      ğŸ“Š Mostly linear patterns - ensemble still beneficial for robustness\")\n",
    "\n",
    "# 4. Outlier assessment\n",
    "def detect_outliers_iqr(data):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    return outliers.sum()\n",
    "\n",
    "age_outliers = detect_outliers_iqr(X[:, 0])\n",
    "salary_outliers = detect_outliers_iqr(X[:, 1])\n",
    "\n",
    "print(f\"   ğŸ¯ Outlier analysis:\")\n",
    "print(f\"      â€¢ Age outliers: {age_outliers} ({age_outliers/len(X)*100:.1f}%)\")\n",
    "print(f\"      â€¢ Salary outliers: {salary_outliers} ({salary_outliers/len(X)*100:.1f}%)\")\n",
    "if age_outliers + salary_outliers > 0:\n",
    "    print(f\"      âœ… Outliers present - Random Forest robustness will help\")\n",
    "else:\n",
    "    print(f\"      ğŸ“Š Clean data - ensemble will still improve generalization\")\n",
    "\n",
    "# 5. Bootstrap sampling readiness\n",
    "print(f\"   ğŸ² Bootstrap sampling assessment:\")\n",
    "unique_samples = len(np.unique(X, axis=0))\n",
    "print(f\"      â€¢ Unique samples: {unique_samples}/{len(X)} ({unique_samples/len(X)*100:.1f}%)\")\n",
    "\n",
    "if unique_samples / len(X) > 0.7:\n",
    "    print(f\"      âœ… High sample diversity - excellent for bootstrap sampling\")\n",
    "elif unique_samples / len(X) > 0.5:\n",
    "    print(f\"      âœ… Good sample diversity - bootstrap sampling will be effective\")\n",
    "else:\n",
    "    print(f\"      âš ï¸  Limited sample diversity - bootstrap still beneficial but less varied\")\n",
    "\n",
    "# 6. Feature importance potential\n",
    "print(f\"   ğŸ† Feature importance potential:\")\n",
    "# Simple importance estimation using class separation\n",
    "age_separation = abs(X[y==1, 0].mean() - X[y==0, 0].mean()) / X[:, 0].std()\n",
    "salary_separation = abs(X[y==1, 1].mean() - X[y==0, 1].mean()) / X[:, 1].std()\n",
    "\n",
    "print(f\"      â€¢ Age discriminative power: {age_separation:.3f}\")\n",
    "print(f\"      â€¢ Salary discriminative power: {salary_separation:.3f}\")\n",
    "\n",
    "if max(age_separation, salary_separation) > 0.3:\n",
    "    print(f\"      âœ… Strong feature importance signals expected\")\n",
    "else:\n",
    "    print(f\"      ğŸ“Š Moderate importance - ensemble voting will help\")\n",
    "\n",
    "print(f\"\\nâœ… ENSEMBLE READINESS SUMMARY:\")\n",
    "strengths = []\n",
    "considerations = []\n",
    "\n",
    "if len(X) >= 100:\n",
    "    strengths.append(\"Sufficient sample size\")\n",
    "if abs(correlations[0,1]) < 0.8:\n",
    "    strengths.append(\"Feature diversity\")\n",
    "if max(age_nonlin, salary_nonlin) > 0.1:\n",
    "    strengths.append(\"Non-linear patterns\")\n",
    "if age_outliers + salary_outliers > 0:\n",
    "    strengths.append(\"Outlier robustness needed\")\n",
    "if unique_samples / len(X) > 0.5:\n",
    "    strengths.append(\"Bootstrap diversity\")\n",
    "\n",
    "print(f\"   ğŸŒŸ Strengths: {', '.join(strengths) if strengths else 'Standard benefits'}\")\n",
    "print(f\"   ğŸŒ² Random Forest Expected Benefits:\")\n",
    "print(f\"      â€¢ Reduced overfitting through ensemble averaging\")\n",
    "print(f\"      â€¢ Improved generalization via bootstrap sampling\")\n",
    "print(f\"      â€¢ Feature importance rankings\")\n",
    "print(f\"      â€¢ Built-in cross-validation (out-of-bag)\")\n",
    "print(f\"      â€¢ Robustness to outliers and noise\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Ready for ensemble learning theory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvxIPVyMhmKp"
   },
   "source": [
    "## ğŸ” Ensemble-Optimized Exploratory Data Analysis\n",
    "\n",
    "**Analyzing our data through the lens of ensemble learning, bootstrap sampling potential, and feature diversity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVzJWAXIhxoC"
   },
   "outputs": [],
   "source": [
    "# Ensemble-Optimized Exploratory Data Analysis\n",
    "print(\"ğŸ” ENSEMBLE-OPTIMIZED EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Analyze data specifically for Random Forest insights\n",
    "print(\"ğŸŒ² RANDOM FOREST PERSPECTIVE ANALYSIS\")\n",
    "print(\"-\"*42)\n",
    "\n",
    "# Class distribution analysis for ensemble learning\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "print(f\"ğŸ“Š CLASS DISTRIBUTION FOR ENSEMBLE:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"   Class {cls}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Assess class imbalance impact on ensemble\n",
    "class_ratio = min(class_counts) / max(class_counts)\n",
    "print(f\"   âš–ï¸  Class balance ratio: {class_ratio:.3f}\")\n",
    "if class_ratio > 0.3:\n",
    "    print(\"   âœ… Well balanced - standard Random Forest will work well\")\n",
    "elif class_ratio > 0.1:\n",
    "    print(\"   âš ï¸  Moderate imbalance - consider class_weight='balanced'\")\n",
    "else:\n",
    "    print(\"   ğŸ”´ Severe imbalance - use balanced_subsample or SMOTE\")\n",
    "\n",
    "# Feature statistics by class for ensemble understanding\n",
    "print(f\"\\nğŸ“ˆ FEATURE STATISTICS FOR BOOTSTRAP SAMPLING:\")\n",
    "for class_idx in unique_classes:\n",
    "    mask = y == class_idx\n",
    "    class_label = \"No Purchase\" if class_idx == 0 else \"Purchased\"\n",
    "    class_size = mask.sum()\n",
    "    \n",
    "    print(f\"   {class_label} (Class {class_idx}) - {class_size} samples:\")\n",
    "    print(f\"      Age: Î¼={X[mask, 0].mean():.1f}, Ïƒ={X[mask, 0].std():.1f}, range=[{X[mask, 0].min():.0f}, {X[mask, 0].max():.0f}]\")\n",
    "    print(f\"      Salary: Î¼=${X[mask, 1].mean():.0f}, Ïƒ=${X[mask, 1].std():.0f}, range=[${X[mask, 1].min():.0f}, ${X[mask, 1].max():.0f}]\")\n",
    "\n",
    "# Create comprehensive visualizations for ensemble analysis\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Feature distributions with bootstrap sampling implications\n",
    "plt.subplot(4, 4, 1)\n",
    "ages_no = X[y==0, 0]\n",
    "ages_yes = X[y==1, 0]\n",
    "plt.hist(ages_no, bins=20, alpha=0.7, label='No Purchase', color='red', density=True)\n",
    "plt.hist(ages_yes, bins=20, alpha=0.7, label='Purchased', color='green', density=True)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Age Distribution by Class\\n(Bootstrap Sampling Base)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show bootstrap variation potential\n",
    "age_mean_diff = abs(ages_yes.mean() - ages_no.mean())\n",
    "age_std_pooled = np.sqrt((ages_no.var() + ages_yes.var()) / 2)\n",
    "plt.text(0.05, 0.95, f'Separation: {age_mean_diff/age_std_pooled:.2f}Ïƒ', \n",
    "         transform=plt.gca().transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "\n",
    "plt.subplot(4, 4, 2)\n",
    "salaries_no = X[y==0, 1]\n",
    "salaries_yes = X[y==1, 1]\n",
    "plt.hist(salaries_no, bins=20, alpha=0.7, label='No Purchase', color='red', density=True)\n",
    "plt.hist(salaries_yes, bins=20, alpha=0.7, label='Purchased', color='green', density=True)\n",
    "plt.xlabel('Estimated Salary')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Salary Distribution by Class\\n(Bootstrap Sampling Base)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "salary_mean_diff = abs(salaries_yes.mean() - salaries_no.mean())\n",
    "salary_std_pooled = np.sqrt((salaries_no.var() + salaries_yes.var()) / 2)\n",
    "plt.text(0.05, 0.95, f'Separation: {salary_mean_diff/salary_std_pooled:.2f}Ïƒ', \n",
    "         transform=plt.gca().transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "\n",
    "# 2. Bootstrap sampling demonstration\n",
    "plt.subplot(4, 4, 3)\n",
    "# Simulate multiple bootstrap samples\n",
    "n_bootstrap = 5\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, n_bootstrap))\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Create bootstrap sample\n",
    "    bootstrap_indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    bootstrap_X = X[bootstrap_indices]\n",
    "    bootstrap_y = y[bootstrap_indices]\n",
    "    \n",
    "    # Plot age distribution for this bootstrap sample\n",
    "    ages_boot = bootstrap_X[bootstrap_y==1, 0]\n",
    "    if len(ages_boot) > 0:\n",
    "        plt.hist(ages_boot, bins=15, alpha=0.3, color=colors[i], \n",
    "                label=f'Bootstrap {i+1}', density=True)\n",
    "\n",
    "plt.hist(ages_yes, bins=15, alpha=0.8, color='black', linestyle='--', \n",
    "         histtype='step', linewidth=2, label='Original', density=True)\n",
    "plt.xlabel('Age (Purchased Class)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Bootstrap Sample Variation\\n(Age Distribution)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature correlation for random feature selection\n",
    "plt.subplot(4, 4, 4)\n",
    "correlation_matrix = np.corrcoef(X.T)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            xticklabels=['Age', 'Salary'], yticklabels=['Age', 'Salary'],\n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Matrix\\n(Random Selection Impact)')\n",
    "\n",
    "# Add correlation interpretation\n",
    "corr_value = correlation_matrix[0, 1]\n",
    "if abs(corr_value) < 0.3:\n",
    "    corr_text = \"Low correlation\\nGood for random selection\"\n",
    "elif abs(corr_value) < 0.7:\n",
    "    corr_text = \"Moderate correlation\\nStill beneficial\"\n",
    "else:\n",
    "    corr_text = \"High correlation\\nLimited benefit\"\n",
    "\n",
    "plt.text(1.05, 0.5, corr_text, transform=plt.gca().transAxes, \n",
    "         bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "\n",
    "# 4. 2D Feature space with ensemble decision regions\n",
    "plt.subplot(4, 4, 5)\n",
    "scatter_no = plt.scatter(X[y==0, 0], X[y==0, 1], c='red', alpha=0.6, s=40, label='No Purchase')\n",
    "scatter_yes = plt.scatter(X[y==1, 0], X[y==1, 1], c='green', alpha=0.6, s=40, label='Purchased')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.title('2D Feature Space\\n(Ensemble Decision Regions)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show potential for complex decision boundaries\n",
    "plt.text(0.05, 0.95, 'Complex boundaries\\nexpected from ensemble', \n",
    "         transform=plt.gca().transAxes, bbox=dict(boxstyle=\"round\", facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# 5. Sample diversity analysis\n",
    "plt.subplot(4, 4, 6)\n",
    "# Calculate distance matrix for sample diversity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Normalize features for distance calculation\n",
    "X_norm = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "distance_matrix = squareform(pdist(X_norm))\n",
    "\n",
    "# Plot distribution of pairwise distances\n",
    "distances = distance_matrix[np.triu_indices(len(X_norm), k=1)]\n",
    "plt.hist(distances, bins=30, alpha=0.7, color='skyblue', density=True)\n",
    "plt.xlabel('Normalized Euclidean Distance')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Sample Diversity Distribution\\n(Bootstrap Potential)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add diversity metrics\n",
    "mean_distance = distances.mean()\n",
    "std_distance = distances.std()\n",
    "plt.axvline(mean_distance, color='red', linestyle='--', \n",
    "           label=f'Mean: {mean_distance:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "# 6. Feature importance preview using simple methods\n",
    "plt.subplot(4, 4, 7)\n",
    "\n",
    "# Calculate feature importance using different methods\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Mutual information\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "# Quick Extra Trees for comparison\n",
    "et_quick = ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
    "et_quick.fit(X, y)\n",
    "et_importance = et_quick.feature_importances_\n",
    "\n",
    "features = ['Age', 'Salary']\n",
    "x_pos = np.arange(len(features))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = plt.bar(x_pos - width/2, mi_scores, width, label='Mutual Info', \n",
    "               color='lightblue', alpha=0.7)\n",
    "bars2 = plt.bar(x_pos + width/2, et_importance, width, label='Extra Trees', \n",
    "               color='lightcoral', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Feature Importance Preview\\n(Different Methods)')\n",
    "plt.xticks(x_pos, features)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 7. Out-of-bag error simulation\n",
    "plt.subplot(4, 4, 8)\n",
    "\n",
    "# Simulate how out-of-bag samples vary with different bootstrap samples\n",
    "oob_sizes = []\n",
    "n_simulations = 100\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    bootstrap_indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    unique_indices = np.unique(bootstrap_indices)\n",
    "    oob_size = len(X) - len(unique_indices)\n",
    "    oob_sizes.append(oob_size / len(X))  # As fraction\n",
    "\n",
    "plt.hist(oob_sizes, bins=20, alpha=0.7, color='orange', density=True)\n",
    "plt.xlabel('Out-of-Bag Fraction')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Out-of-Bag Sample Size\\n(Validation Data)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add theoretical expectation (â‰ˆ 36.8%)\n",
    "theoretical_oob = 1 - (1 - 1/len(X))**len(X)\n",
    "plt.axvline(theoretical_oob, color='red', linestyle='--', \n",
    "           label=f'Theoretical: {theoretical_oob:.1%}')\n",
    "plt.axvline(np.mean(oob_sizes), color='blue', linestyle='--', \n",
    "           label=f'Observed: {np.mean(oob_sizes):.1%}')\n",
    "plt.legend()\n",
    "\n",
    "# 8. Tree diversity potential\n",
    "plt.subplot(4, 4, 9)\n",
    "\n",
    "# Simulate different tree predictions to show diversity\n",
    "n_trees = 5\n",
    "tree_predictions = []\n",
    "\n",
    "for i in range(n_trees):\n",
    "    # Train tree on bootstrap sample with random features\n",
    "    bootstrap_indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    \n",
    "    # Simple tree simulation (not actual DecisionTreeClassifier for speed)\n",
    "    # Just use different thresholds to simulate tree diversity\n",
    "    age_threshold = np.random.uniform(25, 45)\n",
    "    salary_threshold = np.random.uniform(40000, 80000)\n",
    "    \n",
    "    predictions = ((X[:, 0] > age_threshold) & (X[:, 1] > salary_threshold)).astype(int)\n",
    "    tree_predictions.append(predictions)\n",
    "\n",
    "# Calculate pairwise agreement between trees\n",
    "agreements = []\n",
    "for i in range(n_trees):\n",
    "    for j in range(i+1, n_trees):\n",
    "        agreement = (tree_predictions[i] == tree_predictions[j]).mean()\n",
    "        agreements.append(agreement)\n",
    "\n",
    "plt.hist(agreements, bins=10, alpha=0.7, color='purple', density=True)\n",
    "plt.xlabel('Pairwise Agreement')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Tree Diversity Simulation\\n(Agreement Between Trees)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add interpretation\n",
    "mean_agreement = np.mean(agreements)\n",
    "plt.axvline(mean_agreement, color='red', linestyle='--', \n",
    "           label=f'Mean: {mean_agreement:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "if mean_agreement < 0.7:\n",
    "    diversity_text = \"Good diversity\"\n",
    "elif mean_agreement < 0.9:\n",
    "    diversity_text = \"Moderate diversity\"\n",
    "else:\n",
    "    diversity_text = \"Low diversity\"\n",
    "\n",
    "plt.text(0.05, 0.95, diversity_text, transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle=\"round\", facecolor='white'))\n",
    "\n",
    "# 9. Bias-Variance tradeoff visualization\n",
    "plt.subplot(4, 4, 10)\n",
    "\n",
    "# Simulate bias-variance tradeoff for different ensemble sizes\n",
    "ensemble_sizes = [1, 5, 10, 25, 50, 100]\n",
    "bias_values = [0.15, 0.14, 0.13, 0.12, 0.12, 0.12]  # Decreases slightly\n",
    "variance_values = [0.25, 0.15, 0.10, 0.06, 0.04, 0.03]  # Decreases significantly\n",
    "total_error = [b + v for b, v in zip(bias_values, variance_values)]\n",
    "\n",
    "plt.plot(ensemble_sizes, bias_values, 'r-o', label='BiasÂ²', linewidth=2)\n",
    "plt.plot(ensemble_sizes, variance_values, 'b-s', label='Variance', linewidth=2)\n",
    "plt.plot(ensemble_sizes, total_error, 'g-^', label='Total Error', linewidth=2)\n",
    "\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Error Component')\n",
    "plt.title('Bias-Variance Tradeoff\\n(Ensemble Size Impact)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "# 10. Learning curve preview\n",
    "plt.subplot(4, 4, 11)\n",
    "\n",
    "# Simulate learning curves for single tree vs ensemble\n",
    "training_sizes = np.array([50, 100, 150, 200, 250, 300, 350])\n",
    "single_tree_scores = 1 - (0.3 + 0.2 * np.exp(-training_sizes/100))  # Asymptotic\n",
    "ensemble_scores = 1 - (0.15 + 0.1 * np.exp(-training_sizes/80))     # Better asymptote\n",
    "\n",
    "plt.plot(training_sizes, single_tree_scores, 'r-o', label='Single Tree', linewidth=2)\n",
    "plt.plot(training_sizes, ensemble_scores, 'g-s', label='Random Forest', linewidth=2)\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves Preview\\n(Single vs Ensemble)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add performance gap annotation\n",
    "final_gap = ensemble_scores[-1] - single_tree_scores[-1]\n",
    "plt.annotate(f'Performance Gap: {final_gap:.3f}', \n",
    "            xy=(training_sizes[-1], single_tree_scores[-1]), \n",
    "            xytext=(250, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'),\n",
    "            bbox=dict(boxstyle=\"round\", facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# 11. Feature interaction analysis\n",
    "plt.subplot(4, 4, 12)\n",
    "\n",
    "# Create interaction feature (Age * Salary normalized)\n",
    "age_norm = (X[:, 0] - X[:, 0].min()) / (X[:, 0].max() - X[:, 0].min())\n",
    "salary_norm = (X[:, 1] - X[:, 1].min()) / (X[:, 1].max() - X[:, 1].min())\n",
    "interaction = age_norm * salary_norm\n",
    "\n",
    "# Plot interaction vs target\n",
    "colors = ['red' if label == 0 else 'green' for label in y]\n",
    "plt.scatter(interaction, y + np.random.normal(0, 0.05, len(y)), \n",
    "           c=colors, alpha=0.6, s=30)\n",
    "plt.xlabel('Age Ã— Salary (Normalized)')\n",
    "plt.ylabel('Purchase (with jitter)')\n",
    "plt.title('Feature Interaction Analysis\\n(Age Ã— Salary)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(interaction, y, 1)\n",
    "p = np.poly1d(z)\n",
    "x_trend = np.linspace(interaction.min(), interaction.max(), 100)\n",
    "plt.plot(x_trend, p(x_trend), \"b--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# 12-16: Additional ensemble-specific analyses\n",
    "analyses = [\n",
    "    (\"Bootstrap\\nStability\", \"Variation in\\nbootstrap samples\"),\n",
    "    (\"Voting\\nMechanism\", \"How ensemble\\ncombines predictions\"),\n",
    "    (\"Feature\\nSubsampling\", \"Random feature\\nselection impact\"),\n",
    "    (\"Overfitting\\nReduction\", \"Ensemble vs\\nsingle tree\"),\n",
    "    (\"Ensemble\\nReadiness\", \"Dataset suitability\\nsummary\")\n",
    "]\n",
    "\n",
    "for i, (title, description) in enumerate(analyses):\n",
    "    plt.subplot(4, 4, 13 + i)\n",
    "    plt.text(0.5, 0.7, title, ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.5, 0.3, description, ha='center', va='center', fontsize=10,\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add a simple visualization for each\n",
    "    if i == 0:  # Bootstrap stability\n",
    "        x = np.linspace(0, 1, 100)\n",
    "        for j in range(3):\n",
    "            y_sim = np.sin(2*np.pi*x + j*0.5) + np.random.normal(0, 0.1, 100)\n",
    "            plt.plot(x, 0.1 + j*0.2 + 0.1*y_sim, alpha=0.7)\n",
    "    \n",
    "    elif i == 1:  # Voting mechanism\n",
    "        votes = [3, 7, 2, 8]  # Example votes\n",
    "        labels = ['Tree1', 'Tree2', 'Tree3', 'Tree4']\n",
    "        colors = ['red', 'red', 'green', 'green']\n",
    "        plt.bar(range(len(votes)), votes, color=colors, alpha=0.7)\n",
    "        plt.text(0.5, 0.1, \"Majority: Green\", ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary insights for Random Forest\n",
    "print(f\"\\nğŸŒ² RANDOM FOREST ENSEMBLE INSIGHTS:\")\n",
    "print(f\"   ğŸ“Š Class balance: {'Good' if class_ratio > 0.3 else 'Consider balancing'}\")\n",
    "print(f\"   ğŸ² Bootstrap diversity: High sample uniqueness supports diverse trees\")\n",
    "print(f\"   ğŸ”€ Feature correlation: {abs(correlation_matrix[0,1]):.3f} ({'Low - good for random selection' if abs(correlation_matrix[0,1]) < 0.5 else 'Moderate - still beneficial'})\")\n",
    "print(f\"   ğŸ“ˆ Expected OOB samples: ~{theoretical_oob:.1%} for validation\")\n",
    "print(f\"   ğŸŒ³ Tree diversity: Simulated agreement {mean_agreement:.2f} suggests good diversity potential\")\n",
    "print(f\"   ğŸ“Š Feature importance: Both features show discriminative power\")\n",
    "\n",
    "print(f\"\\nâœ… ENSEMBLE LEARNING ADVANTAGES EXPECTED:\")\n",
    "print(f\"   ğŸ¯ Reduced overfitting through bootstrap aggregation\")\n",
    "print(f\"   ğŸ“ˆ Improved generalization via ensemble voting\")\n",
    "print(f\"   ğŸ” Built-in feature importance analysis\")\n",
    "print(f\"   âœ… Out-of-bag validation without separate test set\")\n",
    "print(f\"   ğŸ›¡ï¸  Robustness to outliers and noise\")\n",
    "print(f\"   ğŸ“Š Smooth decision boundaries from ensemble averaging\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Ready for ensemble learning theory deep dive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§® Ensemble Learning Theory: The Mathematics of Collective Intelligence\n",
    "\n",
    "**Understanding Bootstrap Sampling, Bagging, Random Feature Selection, and the Wisdom of Crowds**\n",
    "\n",
    "Now we dive into the theoretical foundations that make Random Forest so powerful. We'll explore how combining many \"weak\" learners creates a \"strong\" ensemble through mathematical principles.\n",
    "\n",
    "### ğŸ¯ The Central Questions\n",
    "*\"Why do many imperfect trees outperform one perfect tree?\"*\n",
    "*\"How does randomness lead to better performance?\"*\n",
    "\n",
    "The answers lie in understanding **ensemble learning theory** and the mathematics of **variance reduction**.\n",
    "\n",
    "### ğŸ“Š Key Concepts We'll Master\n",
    "\n",
    "**1. ğŸ² Bootstrap Sampling** - Creating Diversity\n",
    "- Sample with replacement from original dataset\n",
    "- Each tree sees ~63.2% of original data\n",
    "- Remaining ~36.8% used for out-of-bag validation\n",
    "\n",
    "**2. ğŸ“ˆ Bagging (Bootstrap Aggregating)** - Reducing Variance\n",
    "- Train multiple models on bootstrap samples\n",
    "- Average predictions to reduce variance\n",
    "- Mathematical guarantee of improvement\n",
    "\n",
    "**3. ğŸ”€ Random Feature Selection** - Breaking Correlations\n",
    "- Each split considers random subset of features\n",
    "- Reduces correlation between trees\n",
    "- Prevents feature dominance\n",
    "\n",
    "**4. ğŸ—³ï¸ Ensemble Voting** - Democratic Decision Making\n",
    "- Classification: Majority vote\n",
    "- Confidence through vote distribution\n",
    "- Soft voting with probability averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning Theory Implementation & Visualization\n",
    "print(\"ğŸ§® ENSEMBLE LEARNING THEORY DEEP DIVE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Bootstrap Sampling Theory and Practice\n",
    "print(\"ğŸ² BOOTSTRAP SAMPLING MATHEMATICS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "def demonstrate_bootstrap_sampling(X, y, n_samples=5):\n",
    "    \"\"\"Demonstrate bootstrap sampling and its properties\"\"\"\n",
    "    bootstrap_stats = []\n",
    "    original_mean_age = X[:, 0].mean()\n",
    "    original_mean_salary = X[:, 1].mean()\n",
    "    \n",
    "    print(f\"ğŸ“Š Original Dataset Statistics:\")\n",
    "    print(f\"   Age mean: {original_mean_age:.2f}\")\n",
    "    print(f\"   Salary mean: ${original_mean_salary:.0f}\")\n",
    "    print(f\"   Sample size: {len(X)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ² Bootstrap Samples Analysis:\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create bootstrap sample\n",
    "        bootstrap_indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        bootstrap_X = X[bootstrap_indices]\n",
    "        bootstrap_y = y[bootstrap_indices]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        unique_samples = len(np.unique(bootstrap_indices))\n",
    "        oob_samples = len(X) - unique_samples\n",
    "        duplicate_samples = len(X) - unique_samples\n",
    "        \n",
    "        bootstrap_stats.append({\n",
    "            'sample_id': i + 1,\n",
    "            'unique_samples': unique_samples,\n",
    "            'oob_samples': oob_samples,\n",
    "            'age_mean': bootstrap_X[:, 0].mean(),\n",
    "            'salary_mean': bootstrap_X[:, 1].mean(),\n",
    "            'class_balance': np.bincount(bootstrap_y, minlength=2) / len(bootstrap_y)\n",
    "        })\n",
    "        \n",
    "        print(f\"   Sample {i+1}: {unique_samples} unique ({unique_samples/len(X)*100:.1f}%), \"\n",
    "              f\"{oob_samples} OOB ({oob_samples/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    return bootstrap_stats\n",
    "\n",
    "bootstrap_stats = demonstrate_bootstrap_sampling(X, y)\n",
    "\n",
    "# Theoretical bootstrap properties\n",
    "n = len(X)\n",
    "theoretical_oob_fraction = 1 - (1 - 1/n)**n\n",
    "theoretical_unique_fraction = 1 - theoretical_oob_fraction\n",
    "\n",
    "print(f\"\\nğŸ“ THEORETICAL BOOTSTRAP PROPERTIES:\")\n",
    "print(f\"   Expected unique samples: {theoretical_unique_fraction:.1%}\")\n",
    "print(f\"   Expected OOB samples: {theoretical_oob_fraction:.1%}\")\n",
    "print(f\"   As nâ†’âˆ: OOB fraction â†’ 36.8% (1/e)\")\n",
    "\n",
    "# 2. Bagging Theory - Variance Reduction\n",
    "print(f\"\\nğŸ“ˆ BAGGING THEORY - VARIANCE REDUCTION\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "def simulate_bagging_effect(n_trees_list=[1, 5, 10, 25, 50, 100]):\n",
    "    \"\"\"Simulate the variance reduction effect of bagging\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¬ Bias-Variance Decomposition Simulation:\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for n_trees in n_trees_list:\n",
    "        # Simulate individual tree predictions (with noise)\n",
    "        tree_predictions = []\n",
    "        \n",
    "        for _ in range(n_trees):\n",
    "            # Simulate tree prediction: true_prob + bias + variance_noise\n",
    "            true_prob = 0.6  # True probability\n",
    "            bias = 0.05      # Small bias\n",
    "            variance_noise = np.random.normal(0, 0.2)  # Individual tree variance\n",
    "            \n",
    "            tree_pred = true_prob + bias + variance_noise\n",
    "            tree_pred = np.clip(tree_pred, 0, 1)  # Keep in valid range\n",
    "            tree_predictions.append(tree_pred)\n",
    "        \n",
    "        # Ensemble prediction is average\n",
    "        ensemble_pred = np.mean(tree_predictions)\n",
    "        \n",
    "        # Calculate bias and variance\n",
    "        bias_squared = (ensemble_pred - true_prob)**2\n",
    "        variance = np.var(tree_predictions) / n_trees  # Variance reduces by 1/n_trees\n",
    "        \n",
    "        results.append({\n",
    "            'n_trees': n_trees,\n",
    "            'bias_squared': bias_squared,\n",
    "            'variance': variance,\n",
    "            'total_error': bias_squared + variance,\n",
    "            'individual_var': np.var(tree_predictions)\n",
    "        })\n",
    "        \n",
    "        print(f\"   {n_trees:3d} trees: BiasÂ²={bias_squared:.4f}, \"\n",
    "              f\"Variance={variance:.4f}, Total={bias_squared + variance:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "bagging_results = simulate_bagging_effect()\n",
    "\n",
    "# 3. Random Feature Selection Theory\n",
    "print(f\"\\nğŸ”€ RANDOM FEATURE SELECTION THEORY\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "def analyze_feature_randomness(n_features_total=2, max_features_options=['sqrt', 'log2', None]):\n",
    "    \"\"\"Analyze the impact of random feature selection\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“Š Feature Randomness Analysis (Total features: {n_features_total}):\")\n",
    "    \n",
    "    for max_features in max_features_options:\n",
    "        if max_features == 'sqrt':\n",
    "            n_features_selected = int(np.sqrt(n_features_total))\n",
    "        elif max_features == 'log2':\n",
    "            n_features_selected = int(np.log2(n_features_total))\n",
    "        elif max_features is None:\n",
    "            n_features_selected = n_features_total\n",
    "        else:\n",
    "            n_features_selected = max_features\n",
    "        \n",
    "        n_features_selected = max(1, n_features_selected)  # At least 1\n",
    "        \n",
    "        # Calculate probability that best feature is selected\n",
    "        prob_best_selected = n_features_selected / n_features_total\n",
    "        \n",
    "        # Expected correlation reduction (simplified)\n",
    "        correlation_reduction = 1 - (n_features_selected / n_features_total)\n",
    "        \n",
    "        print(f\"   max_features={max_features}: {n_features_selected} features selected\")\n",
    "        print(f\"      Prob(best feature selected): {prob_best_selected:.2f}\")\n",
    "        print(f\"      Expected correlation reduction: {correlation_reduction:.2f}\")\n",
    "\n",
    "analyze_feature_randomness()\n",
    "\n",
    "# 4. Voting Mechanism Demonstration\n",
    "print(f\"\\nğŸ—³ï¸ ENSEMBLE VOTING MECHANISMS\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "def demonstrate_voting(n_trees=7, true_accuracy=0.7):\n",
    "    \"\"\"Demonstrate ensemble voting and its benefits\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“Š Voting Analysis ({n_trees} trees, individual accuracy: {true_accuracy:.1%}):\")\n",
    "    \n",
    "    # Simulate individual tree votes for multiple samples\n",
    "    n_samples = 1000\n",
    "    individual_correct = 0\n",
    "    ensemble_correct = 0\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        # True label (random)\n",
    "        true_label = np.random.randint(0, 2)\n",
    "        \n",
    "        # Individual tree predictions\n",
    "        tree_votes = []\n",
    "        for tree in range(n_trees):\n",
    "            # Each tree has 'true_accuracy' chance of being correct\n",
    "            if np.random.random() < true_accuracy:\n",
    "                vote = true_label\n",
    "            else:\n",
    "                vote = 1 - true_label\n",
    "            tree_votes.append(vote)\n",
    "        \n",
    "        # First tree accuracy (for comparison)\n",
    "        if tree_votes[0] == true_label:\n",
    "            individual_correct += 1\n",
    "        \n",
    "        # Ensemble prediction (majority vote)\n",
    "        ensemble_vote = 1 if sum(tree_votes) > n_trees // 2 else 0\n",
    "        if ensemble_vote == true_label:\n",
    "            ensemble_correct += 1\n",
    "    \n",
    "    individual_accuracy = individual_correct / n_samples\n",
    "    ensemble_accuracy = ensemble_correct / n_samples\n",
    "    \n",
    "    print(f\"   Individual tree accuracy: {individual_accuracy:.3f}\")\n",
    "    print(f\"   Ensemble accuracy: {ensemble_accuracy:.3f}\")\n",
    "    print(f\"   Improvement: {ensemble_accuracy - individual_accuracy:.3f}\")\n",
    "    \n",
    "    # Theoretical ensemble accuracy (binomial distribution)\n",
    "    from scipy.special import comb\n",
    "    theoretical_ensemble_acc = 0\n",
    "    for k in range(n_trees // 2 + 1, n_trees + 1):\n",
    "        prob_k_correct = comb(n_trees, k) * (true_accuracy**k) * ((1-true_accuracy)**(n_trees-k))\n",
    "        theoretical_ensemble_acc += prob_k_correct\n",
    "    \n",
    "    print(f\"   Theoretical ensemble accuracy: {theoretical_ensemble_acc:.3f}\")\n",
    "    \n",
    "    return individual_accuracy, ensemble_accuracy, theoretical_ensemble_acc\n",
    "\n",
    "voting_results = demonstrate_voting()\n",
    "\n",
    "# Comprehensive visualization of ensemble theory\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Bootstrap sampling visualization\n",
    "plt.subplot(4, 4, 1)\n",
    "oob_fractions = [stats['oob_samples'] / len(X) for stats in bootstrap_stats]\n",
    "unique_fractions = [stats['unique_samples'] / len(X) for stats in bootstrap_stats]\n",
    "\n",
    "x_pos = range(len(bootstrap_stats))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([x - width/2 for x in x_pos], unique_fractions, width, \n",
    "        label='Unique Samples', color='lightblue', alpha=0.7)\n",
    "plt.bar([x + width/2 for x in x_pos], oob_fractions, width, \n",
    "        label='OOB Samples', color='orange', alpha=0.7)\n",
    "\n",
    "plt.axhline(theoretical_unique_fraction, color='blue', linestyle='--', \n",
    "           label=f'Theoretical Unique: {theoretical_unique_fraction:.1%}')\n",
    "plt.axhline(theoretical_oob_fraction, color='red', linestyle='--', \n",
    "           label=f'Theoretical OOB: {theoretical_oob_fraction:.1%}')\n",
    "\n",
    "plt.xlabel('Bootstrap Sample')\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Bootstrap Sampling Properties')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Bias-Variance tradeoff\n",
    "plt.subplot(4, 4, 2)\n",
    "n_trees = [result['n_trees'] for result in bagging_results]\n",
    "bias_squared = [result['bias_squared'] for result in bagging_results]\n",
    "variance = [result['variance'] for result in bagging_results]\n",
    "total_error = [result['total_error'] for result in bagging_results]\n",
    "\n",
    "plt.plot(n_trees, bias_squared, 'r-o', label='BiasÂ²', linewidth=2, markersize=6)\n",
    "plt.plot(n_trees, variance, 'b-s', label='Variance', linewidth=2, markersize=6)\n",
    "plt.plot(n_trees, total_error, 'g-^', label='Total Error', linewidth=2, markersize=6)\n",
    "\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Error Component')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "# 3. Bootstrap sample statistics variation\n",
    "plt.subplot(4, 4, 3)\n",
    "age_means = [stats['age_mean'] for stats in bootstrap_stats]\n",
    "salary_means = [stats['salary_mean'] for stats in bootstrap_stats]\n",
    "\n",
    "sample_ids = [stats['sample_id'] for stats in bootstrap_stats]\n",
    "plt.plot(sample_ids, age_means, 'bo-', label='Age Mean', linewidth=2, markersize=8)\n",
    "plt.axhline(X[:, 0].mean(), color='blue', linestyle='--', alpha=0.7, label='Original Age Mean')\n",
    "\n",
    "# Secondary y-axis for salary\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(sample_ids, salary_means, 'ro-', label='Salary Mean', linewidth=2, markersize=8)\n",
    "ax2.axhline(X[:, 1].mean(), color='red', linestyle='--', alpha=0.7, label='Original Salary Mean')\n",
    "\n",
    "plt.xlabel('Bootstrap Sample ID')\n",
    "plt.gca().set_ylabel('Age', color='blue')\n",
    "ax2.set_ylabel('Salary ($)', color='red')\n",
    "plt.title('Bootstrap Statistics Variation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Out-of-bag fraction distribution\n",
    "plt.subplot(4, 4, 4)\n",
    "# Simulate many bootstrap samples to show OOB distribution\n",
    "n_bootstrap_sims = 1000\n",
    "oob_fractions_sim = []\n",
    "\n",
    "for _ in range(n_bootstrap_sims):\n",
    "    bootstrap_indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    unique_count = len(np.unique(bootstrap_indices))\n",
    "    oob_fraction = (len(X) - unique_count) / len(X)\n",
    "    oob_fractions_sim.append(oob_fraction)\n",
    "\n",
    "plt.hist(oob_fractions_sim, bins=30, alpha=0.7, color='orange', density=True, \n",
    "         label='Simulated Distribution')\n",
    "plt.axvline(theoretical_oob_fraction, color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Theoretical: {theoretical_oob_fraction:.1%}')\n",
    "plt.axvline(np.mean(oob_fractions_sim), color='blue', linestyle='--', linewidth=2,\n",
    "           label=f'Observed: {np.mean(oob_fractions_sim):.1%}')\n",
    "\n",
    "plt.xlabel('Out-of-Bag Fraction')\n",
    "plt.ylabel('Density')\n",
    "plt.title('OOB Fraction Distribution\\n({} Bootstrap Samples)'.format(n_bootstrap_sims))\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Voting mechanism visualization\n",
    "plt.subplot(4, 4, 5)\n",
    "tree_accuracies = np.arange(0.5, 0.9, 0.05)\n",
    "ensemble_improvements = []\n",
    "\n",
    "for acc in tree_accuracies:\n",
    "    _, _, theoretical_acc = demonstrate_voting(n_trees=7, true_accuracy=acc)\n",
    "    improvement = theoretical_acc - acc\n",
    "    ensemble_improvements.append(improvement)\n",
    "    \n",
    "plt.plot(tree_accuracies, ensemble_improvements, 'g-o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Individual Tree Accuracy')\n",
    "plt.ylabel('Ensemble Improvement')\n",
    "plt.title('Voting Benefit vs Tree Quality\\n(7 trees)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation for sweet spot\n",
    "max_improvement_idx = np.argmax(ensemble_improvements)\n",
    "max_acc = tree_accuracies[max_improvement_idx]\n",
    "max_imp = ensemble_improvements[max_improvement_idx]\n",
    "plt.annotate(f'Max benefit at {max_acc:.2f}', \n",
    "            xy=(max_acc, max_imp), xytext=(max_acc + 0.1, max_imp - 0.01),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            bbox=dict(boxstyle=\"round\", facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# 6. Feature correlation impact\n",
    "plt.subplot(4, 4, 6)\n",
    "# Simulate ensemble performance with different feature correlations\n",
    "correlations = np.linspace(0, 0.9, 10)\n",
    "ensemble_performances = []\n",
    "\n",
    "for corr in correlations:\n",
    "    # Simulate performance degradation with high correlation\n",
    "    base_performance = 0.85\n",
    "    correlation_penalty = corr * 0.15  # Higher correlation reduces benefit\n",
    "    ensemble_perf = base_performance - correlation_penalty\n",
    "    ensemble_performances.append(ensemble_perf)\n",
    "\n",
    "plt.plot(correlations, ensemble_performances, 'purple', linewidth=2, marker='s', markersize=6)\n",
    "plt.xlabel('Feature Correlation')\n",
    "plt.ylabel('Ensemble Performance')\n",
    "plt.title('Feature Correlation Impact\\non Ensemble Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add interpretation zones\n",
    "plt.axvspan(0, 0.3, alpha=0.2, color='green', label='Good')\n",
    "plt.axvspan(0.3, 0.7, alpha=0.2, color='yellow', label='Moderate')\n",
    "plt.axvspan(0.7, 0.9, alpha=0.2, color='red', label='Poor')\n",
    "plt.legend()\n",
    "\n",
    "# 7. Ensemble size vs performance\n",
    "plt.subplot(4, 4, 7)\n",
    "ensemble_sizes = [1, 2, 5, 10, 20, 50, 100, 200]\n",
    "performances = []\n",
    "\n",
    "for size in ensemble_sizes:\n",
    "    # Simulate performance improvement with ensemble size\n",
    "    if size == 1:\n",
    "        perf = 0.75  # Single tree baseline\n",
    "    else:\n",
    "        # Logarithmic improvement with diminishing returns\n",
    "        improvement = 0.15 * (1 - np.exp(-size/50))\n",
    "        perf = 0.75 + improvement\n",
    "    performances.append(perf)\n",
    "\n",
    "plt.semilogx(ensemble_sizes, performances, 'b-o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Ensemble Size vs Performance\\n(Diminishing Returns)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Single Tree', xy=(1, 0.75), xytext=(2, 0.73),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'))\n",
    "plt.annotate('Sweet Spot', xy=(50, performances[ensemble_sizes.index(50)]), \n",
    "            xytext=(20, 0.88),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "# 8. Random Forest construction flowchart\n",
    "plt.subplot(4, 4, 8)\n",
    "plt.text(0.5, 0.95, 'Random Forest Construction', ha='center', va='top', \n",
    "         fontsize=12, fontweight='bold', transform=plt.gca().transAxes)\n",
    "\n",
    "flowchart_text = \"\"\"\n",
    "1. ğŸ² Bootstrap Sample\n",
    "   â””â”€ Sample with replacement\n",
    "   \n",
    "2. ğŸ”€ Random Feature Selection\n",
    "   â””â”€ âˆšp features per split\n",
    "   \n",
    "3. ğŸŒ³ Train Decision Tree\n",
    "   â””â”€ Grow to max depth\n",
    "   \n",
    "4. ğŸ”„ Repeat N times\n",
    "   â””â”€ Build tree ensemble\n",
    "   \n",
    "5. ğŸ—³ï¸ Ensemble Voting\n",
    "   â””â”€ Majority vote wins\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.05, 0.85, flowchart_text, fontsize=9, fontfamily='monospace',\n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "# 9-16: Mathematical formulas and additional theory\n",
    "formulas = [\n",
    "    (\"Bootstrap\\nSampling\", \"P(sample not selected)\\n= (1-1/n)â¿ â†’ 1/e â‰ˆ 0.368\"),\n",
    "    (\"Bagging\\nVariance\", \"Var(ensemble) = Var(tree)/n\\n+ correlation Ã— other_terms\"),\n",
    "    (\"Voting\\nProbability\", \"P(majority correct) = \\nÎ£ C(n,k) Ã— páµ Ã— (1-p)â¿â»áµ\"),\n",
    "    (\"Feature\\nRandomness\", \"Correlation reduction\\nâˆ 1 - (selected/total)\"),\n",
    "    (\"Ensemble\\nError\", \"E = BiasÂ² + Variance\\n+ Irreducible Error\"),\n",
    "    (\"OOB\\nError\", \"OOB_error â‰ˆ Test_error\\n(unbiased estimate)\"),\n",
    "    (\"Information\\nGain\", \"IG = H(parent) - \\nÎ£ (|child|/|parent|) Ã— H(child)\"),\n",
    "    (\"Random Forest\\nAdvantage\", \"Wisdom of Crowds\\n= Diversity + Accuracy\")\n",
    "]\n",
    "\n",
    "for i, (title, formula) in enumerate(formulas):\n",
    "    plt.subplot(4, 4, 9 + i)\n",
    "    plt.text(0.5, 0.8, title, ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.5, 0.3, formula, ha='center', va='center', fontsize=8, fontfamily='monospace',\n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightyellow', alpha=0.8))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of ensemble theory\n",
    "print(f\"\\nğŸ§  ENSEMBLE LEARNING THEORY SUMMARY:\")\n",
    "print(f\"   ğŸ² Bootstrap sampling provides diversity through data randomness\")\n",
    "print(f\"   ğŸ“ˆ Bagging reduces variance while maintaining bias\")\n",
    "print(f\"   ğŸ”€ Feature randomness reduces tree correlation\")\n",
    "print(f\"   ğŸ—³ï¸ Voting mechanisms leverage collective intelligence\")\n",
    "print(f\"   ğŸ“Š Out-of-bag samples provide free validation\")\n",
    "print(f\"   ğŸ¯ Ensemble performance > individual tree performance\")\n",
    "\n",
    "print(f\"\\nğŸ“ MATHEMATICAL GUARANTEES:\")\n",
    "print(f\"   â€¢ Bootstrap OOB fraction: ~36.8% (as nâ†’âˆ)\")\n",
    "print(f\"   â€¢ Variance reduction: Factor of 1/n_trees (uncorrelated case)\")\n",
    "print(f\"   â€¢ Voting improvement: Exponential with tree quality\")\n",
    "print(f\"   â€¢ Feature randomness: Reduces overfitting through decorrelation\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Ready for Random Forest algorithm construction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Random Forest Construction Theory\n",
    "\n",
    "**Understanding How the Algorithm Builds a Forest of Diverse Decision Trees**\n",
    "\n",
    "Now that we understand the theory, let's dive into the specific algorithm that Random Forest uses to construct its ensemble of trees and prevent overfitting through intelligent diversity creation.\n",
    "\n",
    "### ğŸŒ² The Random Forest Algorithm Step-by-Step\n",
    "\n",
    "**The CART-based Random Forest Process:**\n",
    "\n",
    "1. **ğŸ² Bootstrap Sampling**: Create diverse training sets\n",
    "2. **ğŸŒ³ Tree Construction**: Build trees with random feature selection  \n",
    "3. **ğŸ”€ Feature Randomness**: Introduce controlled randomness at each split\n",
    "4. **ğŸ—³ï¸ Ensemble Voting**: Combine predictions democratically\n",
    "5. **ğŸ“Š Out-of-Bag Validation**: Built-in performance estimation\n",
    "\n",
    "### ğŸ¯ Key Algorithmic Innovations\n",
    "\n",
    "**1. ğŸ² Controlled Randomness**\n",
    "- Bootstrap sampling for data diversity\n",
    "- Random feature subsets for split diversity\n",
    "- Prevents correlation between trees\n",
    "\n",
    "**2. ğŸŒ³ Unpruned Trees**\n",
    "- Individual trees grown deep (low bias)\n",
    "- Ensemble averaging reduces variance\n",
    "- No individual tree pruning needed\n",
    "\n",
    "**3. ğŸ“Š Built-in Validation**\n",
    "- Out-of-bag samples provide free test set\n",
    "- No need for separate validation split\n",
    "- Unbiased performance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1588269343329,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "P3nS3-6r1i2B",
    "outputId": "e4a38929-7ac1-4895-a070-4f241ad247c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    44  39000]\n",
      " [    32 120000]\n",
      " [    38  50000]\n",
      " [    32 135000]\n",
      " [    52  21000]\n",
      " [    53 104000]\n",
      " [    39  42000]\n",
      " [    38  61000]\n",
      " [    36  50000]\n",
      " [    36  63000]\n",
      " [    35  25000]\n",
      " [    35  50000]\n",
      " [    42  73000]\n",
      " [    47  49000]\n",
      " [    59  29000]\n",
      " [    49  65000]\n",
      " [    45 131000]\n",
      " [    31  89000]\n",
      " [    46  82000]\n",
      " [    47  51000]\n",
      " [    26  15000]\n",
      " [    60 102000]\n",
      " [    38 112000]\n",
      " [    40 107000]\n",
      " [    42  53000]\n",
      " [    35  59000]\n",
      " [    48  41000]\n",
      " [    48 134000]\n",
      " [    38 113000]\n",
      " [    29 148000]\n",
      " [    26  15000]\n",
      " [    60  42000]\n",
      " [    24  19000]\n",
      " [    42 149000]\n",
      " [    46  96000]\n",
      " [    28  59000]\n",
      " [    39  96000]\n",
      " [    28  89000]\n",
      " [    41  72000]\n",
      " [    45  26000]\n",
      " [    33  69000]\n",
      " [    20  82000]\n",
      " [    31  74000]\n",
      " [    42  80000]\n",
      " [    35  72000]\n",
      " [    33 149000]\n",
      " [    40  71000]\n",
      " [    51 146000]\n",
      " [    46  79000]\n",
      " [    35  75000]\n",
      " [    38  51000]\n",
      " [    36  75000]\n",
      " [    37  78000]\n",
      " [    38  61000]\n",
      " [    60 108000]\n",
      " [    20  82000]\n",
      " [    57  74000]\n",
      " [    42  65000]\n",
      " [    26  80000]\n",
      " [    46 117000]\n",
      " [    35  61000]\n",
      " [    21  68000]\n",
      " [    28  44000]\n",
      " [    41  87000]\n",
      " [    37  33000]\n",
      " [    27  90000]\n",
      " [    39  42000]\n",
      " [    28 123000]\n",
      " [    31 118000]\n",
      " [    25  87000]\n",
      " [    35  71000]\n",
      " [    37  70000]\n",
      " [    35  39000]\n",
      " [    47  23000]\n",
      " [    35 147000]\n",
      " [    48 138000]\n",
      " [    26  86000]\n",
      " [    25  79000]\n",
      " [    52 138000]\n",
      " [    51  23000]\n",
      " [    35  60000]\n",
      " [    33 113000]\n",
      " [    30 107000]\n",
      " [    48  33000]\n",
      " [    41  80000]\n",
      " [    48  96000]\n",
      " [    31  18000]\n",
      " [    31  71000]\n",
      " [    43 129000]\n",
      " [    59  76000]\n",
      " [    18  44000]\n",
      " [    36 118000]\n",
      " [    42  90000]\n",
      " [    47  30000]\n",
      " [    26  43000]\n",
      " [    40  78000]\n",
      " [    46  59000]\n",
      " [    59  42000]\n",
      " [    46  74000]\n",
      " [    35  91000]\n",
      " [    28  59000]\n",
      " [    40  57000]\n",
      " [    59 143000]\n",
      " [    57  26000]\n",
      " [    52  38000]\n",
      " [    47 113000]\n",
      " [    53 143000]\n",
      " [    35  27000]\n",
      " [    58 101000]\n",
      " [    45  45000]\n",
      " [    23  82000]\n",
      " [    46  23000]\n",
      " [    42  65000]\n",
      " [    28  84000]\n",
      " [    38  59000]\n",
      " [    26  84000]\n",
      " [    29  28000]\n",
      " [    37  71000]\n",
      " [    22  55000]\n",
      " [    48  35000]\n",
      " [    49  28000]\n",
      " [    38  65000]\n",
      " [    27  17000]\n",
      " [    46  28000]\n",
      " [    48 141000]\n",
      " [    26  17000]\n",
      " [    35  97000]\n",
      " [    39  59000]\n",
      " [    24  27000]\n",
      " [    32  18000]\n",
      " [    46  88000]\n",
      " [    35  58000]\n",
      " [    56  60000]\n",
      " [    47  34000]\n",
      " [    40  72000]\n",
      " [    32 100000]\n",
      " [    19  21000]\n",
      " [    25  90000]\n",
      " [    35  88000]\n",
      " [    28  32000]\n",
      " [    50  20000]\n",
      " [    40  59000]\n",
      " [    50  44000]\n",
      " [    35  72000]\n",
      " [    40 142000]\n",
      " [    46  32000]\n",
      " [    39  71000]\n",
      " [    20  74000]\n",
      " [    29  75000]\n",
      " [    31  76000]\n",
      " [    47  25000]\n",
      " [    40  61000]\n",
      " [    34 112000]\n",
      " [    38  80000]\n",
      " [    42  75000]\n",
      " [    47  47000]\n",
      " [    39  75000]\n",
      " [    19  25000]\n",
      " [    37  80000]\n",
      " [    36  60000]\n",
      " [    41  52000]\n",
      " [    36 125000]\n",
      " [    48  29000]\n",
      " [    36 126000]\n",
      " [    51 134000]\n",
      " [    27  57000]\n",
      " [    38  71000]\n",
      " [    39  61000]\n",
      " [    22  27000]\n",
      " [    33  60000]\n",
      " [    48  74000]\n",
      " [    58  23000]\n",
      " [    53  72000]\n",
      " [    32 117000]\n",
      " [    54  70000]\n",
      " [    30  80000]\n",
      " [    58  95000]\n",
      " [    26  52000]\n",
      " [    45  79000]\n",
      " [    24  55000]\n",
      " [    40  75000]\n",
      " [    33  28000]\n",
      " [    44 139000]\n",
      " [    22  18000]\n",
      " [    33  51000]\n",
      " [    43 133000]\n",
      " [    24  32000]\n",
      " [    46  22000]\n",
      " [    35  55000]\n",
      " [    54 104000]\n",
      " [    48 119000]\n",
      " [    35  53000]\n",
      " [    37 144000]\n",
      " [    23  66000]\n",
      " [    37 137000]\n",
      " [    31  58000]\n",
      " [    33  41000]\n",
      " [    45  22000]\n",
      " [    30  15000]\n",
      " [    19  19000]\n",
      " [    49  74000]\n",
      " [    39 122000]\n",
      " [    35  73000]\n",
      " [    39  71000]\n",
      " [    24  23000]\n",
      " [    41  72000]\n",
      " [    29  83000]\n",
      " [    54  26000]\n",
      " [    35  44000]\n",
      " [    37  75000]\n",
      " [    29  47000]\n",
      " [    31  68000]\n",
      " [    42  54000]\n",
      " [    30 135000]\n",
      " [    52 114000]\n",
      " [    50  36000]\n",
      " [    56 133000]\n",
      " [    29  61000]\n",
      " [    30  89000]\n",
      " [    26  16000]\n",
      " [    33  31000]\n",
      " [    41  72000]\n",
      " [    36  33000]\n",
      " [    55 125000]\n",
      " [    48 131000]\n",
      " [    41  71000]\n",
      " [    30  62000]\n",
      " [    37  72000]\n",
      " [    41  63000]\n",
      " [    58  47000]\n",
      " [    30 116000]\n",
      " [    20  49000]\n",
      " [    37  74000]\n",
      " [    41  59000]\n",
      " [    49  89000]\n",
      " [    28  79000]\n",
      " [    53  82000]\n",
      " [    40  57000]\n",
      " [    60  34000]\n",
      " [    35 108000]\n",
      " [    21  72000]\n",
      " [    38  71000]\n",
      " [    39 106000]\n",
      " [    37  57000]\n",
      " [    26  72000]\n",
      " [    35  23000]\n",
      " [    54 108000]\n",
      " [    30  17000]\n",
      " [    39 134000]\n",
      " [    29  43000]\n",
      " [    33  43000]\n",
      " [    35  38000]\n",
      " [    41  45000]\n",
      " [    41  72000]\n",
      " [    39 134000]\n",
      " [    27 137000]\n",
      " [    21  16000]\n",
      " [    26  32000]\n",
      " [    31  66000]\n",
      " [    39  73000]\n",
      " [    41  79000]\n",
      " [    47  50000]\n",
      " [    41  30000]\n",
      " [    37  93000]\n",
      " [    60  46000]\n",
      " [    25  22000]\n",
      " [    28  37000]\n",
      " [    38  55000]\n",
      " [    36  54000]\n",
      " [    20  36000]\n",
      " [    56 104000]\n",
      " [    40  57000]\n",
      " [    42 108000]\n",
      " [    20  23000]\n",
      " [    40  65000]\n",
      " [    47  20000]\n",
      " [    18  86000]\n",
      " [    35  79000]\n",
      " [    57  33000]\n",
      " [    34  72000]\n",
      " [    49  39000]\n",
      " [    27  31000]\n",
      " [    19  70000]\n",
      " [    39  79000]\n",
      " [    26  81000]\n",
      " [    25  80000]\n",
      " [    28  85000]\n",
      " [    55  39000]\n",
      " [    50  88000]\n",
      " [    49  88000]\n",
      " [    52 150000]\n",
      " [    35  65000]\n",
      " [    42  54000]\n",
      " [    34  43000]\n",
      " [    37  52000]\n",
      " [    48  30000]\n",
      " [    29  43000]\n",
      " [    36  52000]\n",
      " [    27  54000]\n",
      " [    26 118000]]\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing Specific to Random Forest\n",
    "print(\"ğŸ”§ ENSEMBLE-SPECIFIC DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸŒ² RANDOM FOREST PREPROCESSING ADVANTAGES:\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "# Random Forest preprocessing advantages\n",
    "rf_advantages = {\n",
    "    \"Feature Scaling\": \"NOT REQUIRED - Trees are scale-invariant\",\n",
    "    \"Missing Values\": \"Can handle natively (advanced implementations)\", \n",
    "    \"Outliers\": \"Robust due to bootstrap sampling and voting\",\n",
    "    \"Categorical Variables\": \"Can handle with proper encoding\",\n",
    "    \"Feature Selection\": \"Built-in feature importance - no manual selection needed\",\n",
    "    \"Multicollinearity\": \"Handles correlated features through random selection\"\n",
    "}\n",
    "\n",
    "for aspect, advantage in rf_advantages.items():\n",
    "    print(f\"   {aspect}: {advantage}\")\n",
    "\n",
    "# Demonstrate preprocessing decisions for Random Forest\n",
    "print(f\"\\nğŸ” PREPROCESSING DECISION ANALYSIS:\")\n",
    "\n",
    "# 1. Feature scaling analysis\n",
    "print(f\"   ğŸ“Š Feature Scaling Analysis:\")\n",
    "age_range = X[:, 0].max() - X[:, 0].min()\n",
    "salary_range = X[:, 1].max() - X[:, 1].min()\n",
    "scale_ratio = salary_range / age_range\n",
    "\n",
    "print(f\"      Age range: {age_range}\")\n",
    "print(f\"      Salary range: {salary_range}\")\n",
    "print(f\"      Scale ratio: {scale_ratio:.0f}:1\")\n",
    "print(f\"      ğŸŒ³ Decision: NO SCALING needed for Random Forest\")\n",
    "print(f\"         Trees split on thresholds, not distances\")\n",
    "\n",
    "# 2. Missing value handling (simulate some missing values)\n",
    "print(f\"\\n   â“ Missing Value Handling:\")\n",
    "# For demonstration, let's check if we have any missing values\n",
    "missing_count = pd.isnull(dataset).sum().sum()\n",
    "print(f\"      Current missing values: {missing_count}\")\n",
    "if missing_count == 0:\n",
    "    print(f\"      ğŸŒ³ Decision: No missing values to handle\")\n",
    "    print(f\"         Random Forest can handle missing values natively in advanced implementations\")\n",
    "    print(f\"         Scikit-learn implementation requires preprocessing\")\n",
    "\n",
    "# 3. Categorical encoding demonstration\n",
    "print(f\"\\n   ğŸ·ï¸ Categorical Variable Handling:\")\n",
    "categorical_cols = dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"      Categorical columns: {categorical_cols}\")\n",
    "\n",
    "if 'Gender' in categorical_cols:\n",
    "    print(f\"      Gender unique values: {dataset['Gender'].unique()}\")\n",
    "    print(f\"      ğŸŒ³ Decision: Use Label Encoding for binary categorical\")\n",
    "    print(f\"         Random Forest can split on encoded categorical values\")\n",
    "    \n",
    "    # Demonstrate encoding\n",
    "    le_gender = LabelEncoder()\n",
    "    dataset_processed = dataset.copy()\n",
    "    dataset_processed['Gender_Encoded'] = le_gender.fit_transform(dataset['Gender'])\n",
    "    print(f\"      Encoding: {dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_)))}\")\n",
    "\n",
    "# 4. Outlier impact analysis\n",
    "print(f\"\\n   ğŸ¯ Outlier Impact Analysis:\")\n",
    "def detect_outliers_iqr(data, feature_name):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((data < lower_bound) | (data > upper_bound))\n",
    "    return outliers, outliers.sum()\n",
    "\n",
    "age_outliers, age_outlier_count = detect_outliers_iqr(X[:, 0], 'Age')\n",
    "salary_outliers, salary_outlier_count = detect_outliers_iqr(X[:, 1], 'Salary')\n",
    "\n",
    "print(f\"      Age outliers: {age_outlier_count} ({age_outlier_count/len(X)*100:.1f}%)\")\n",
    "print(f\"      Salary outliers: {salary_outlier_count} ({salary_outlier_count/len(X)*100:.1f}%)\")\n",
    "print(f\"      ğŸŒ³ Decision: KEEP OUTLIERS for Random Forest\")\n",
    "print(f\"         Bootstrap sampling dilutes outlier impact\")\n",
    "print(f\"         Ensemble voting provides robustness\")\n",
    "\n",
    "# Prepare final features for Random Forest\n",
    "print(f\"\\nğŸ¯ FINAL FEATURE PREPARATION:\")\n",
    "\n",
    "# Include Gender if available, otherwise proceed with Age and Salary only\n",
    "if 'Gender' in categorical_cols:\n",
    "    X_processed = np.column_stack([\n",
    "        X[:, 0],  # Age\n",
    "        X[:, 1],  # Salary\n",
    "        dataset_processed['Gender_Encoded'].values  # Gender encoded\n",
    "    ])\n",
    "    feature_names = ['Age', 'EstimatedSalary', 'Gender']\n",
    "    print(f\"   Features included: {feature_names}\")\n",
    "    print(f\"   Feature matrix shape: {X_processed.shape}\")\n",
    "else:\n",
    "    X_processed = X  # Just Age and Salary\n",
    "    feature_names = ['Age', 'EstimatedSalary']\n",
    "    print(f\"   Features included: {feature_names}\")\n",
    "    print(f\"   Feature matrix shape: {X_processed.shape}\")\n",
    "\n",
    "print(f\"   Target variable shape: {y.shape}\")\n",
    "print(f\"   Target classes: {np.unique(y)}\")\n",
    "\n",
    "# Demonstrate bootstrap sampling readiness\n",
    "print(f\"\\nğŸ² BOOTSTRAP SAMPLING READINESS:\")\n",
    "print(f\"   Total samples: {len(X_processed)}\")\n",
    "print(f\"   Unique samples: {len(np.unique(X_processed, axis=0))}\")\n",
    "print(f\"   Sample diversity: {len(np.unique(X_processed, axis=0))/len(X_processed)*100:.1f}%\")\n",
    "\n",
    "if len(np.unique(X_processed, axis=0))/len(X_processed) > 0.7:\n",
    "    print(f\"   âœ… Excellent diversity for bootstrap sampling\")\n",
    "elif len(np.unique(X_processed, axis=0))/len(X_processed) > 0.5:\n",
    "    print(f\"   âœ… Good diversity for bootstrap sampling\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Limited diversity - bootstrap still beneficial\")\n",
    "\n",
    "print(f\"\\nâœ… PREPROCESSING COMPLETE:\")\n",
    "print(f\"   ğŸ”§ Minimal preprocessing required (Random Forest advantage)\")\n",
    "print(f\"   ğŸ“Š No feature scaling needed\")\n",
    "print(f\"   ğŸ·ï¸ Categorical variables encoded\")\n",
    "print(f\"   ğŸ¯ Outliers preserved for robustness\")\n",
    "print(f\"   ğŸ² Ready for ensemble training!\")\n",
    "\n",
    "# Store the processed data for next steps\n",
    "X_final = X_processed\n",
    "y_final = y\n",
    "\n",
    "print(f\"\\nğŸ¯ Data ready for Random Forest implementation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1588269343330,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "8dpDLojm1mVG",
    "outputId": "2a9b0425-9e6d-480f-b32a-ebae6f413dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1\n",
      " 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0\n",
      " 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
      " 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split for Ensemble Learning\n",
    "print(\"âœ‚ï¸ TRAIN-TEST SPLIT FOR ENSEMBLE LEARNING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, \n",
    "    test_size=0.2,      # 80% train, 20% test\n",
    "    random_state=42,    # Reproducible results\n",
    "    stratify=y_final    # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š ENSEMBLE LEARNING SPLIT ANALYSIS:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "\n",
    "# Analyze class distribution in splits\n",
    "train_class_dist = np.bincount(y_train) / len(y_train)\n",
    "test_class_dist = np.bincount(y_test) / len(y_test)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ CLASS DISTRIBUTION VERIFICATION:\")\n",
    "print(f\"   Training set: {train_class_dist[0]:.3f} (No), {train_class_dist[1]:.3f} (Yes)\")\n",
    "print(f\"   Test set: {test_class_dist[0]:.3f} (No), {test_class_dist[1]:.3f} (Yes)\")\n",
    "print(f\"   Stratification success: {'âœ…' if abs(train_class_dist[1] - test_class_dist[1]) < 0.05 else 'âš ï¸'}\")\n",
    "\n",
    "print(f\"\\nğŸŒ² ENSEMBLE BENEFITS OF THIS SPLIT:\")\n",
    "print(f\"   ğŸ² Bootstrap sampling will use training set ({X_train.shape[0]} samples)\")\n",
    "print(f\"   ğŸ“Š Out-of-bag validation available (~37% of bootstrap samples)\")\n",
    "print(f\"   ğŸ¯ Independent test set for final evaluation ({X_test.shape[0]} samples)\")\n",
    "print(f\"   âœ… No data leakage - test set completely isolated\")\n",
    "\n",
    "print(f\"\\nâœ… Data split complete and ready for Random Forest training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1588269343657,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "qbb7i0DH1qui",
    "outputId": "b10e7737-ae02-4c0c-b49f-8d961e2921b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    30  87000]\n",
      " [    38  50000]\n",
      " [    35  75000]\n",
      " [    30  79000]\n",
      " [    35  50000]\n",
      " [    27  20000]\n",
      " [    31  15000]\n",
      " [    36 144000]\n",
      " [    18  68000]\n",
      " [    47  43000]\n",
      " [    30  49000]\n",
      " [    28  55000]\n",
      " [    37  55000]\n",
      " [    39  77000]\n",
      " [    20  86000]\n",
      " [    32 117000]\n",
      " [    37  77000]\n",
      " [    19  85000]\n",
      " [    55 130000]\n",
      " [    35  22000]\n",
      " [    35  47000]\n",
      " [    47 144000]\n",
      " [    41  51000]\n",
      " [    47 105000]\n",
      " [    23  28000]\n",
      " [    49 141000]\n",
      " [    28  87000]\n",
      " [    29  80000]\n",
      " [    37  62000]\n",
      " [    32  86000]\n",
      " [    21  88000]\n",
      " [    37  79000]\n",
      " [    57  60000]\n",
      " [    37  53000]\n",
      " [    24  58000]\n",
      " [    18  52000]\n",
      " [    22  81000]\n",
      " [    34  43000]\n",
      " [    31  34000]\n",
      " [    49  36000]\n",
      " [    27  88000]\n",
      " [    41  52000]\n",
      " [    27  84000]\n",
      " [    35  20000]\n",
      " [    43 112000]\n",
      " [    27  58000]\n",
      " [    37  80000]\n",
      " [    52  90000]\n",
      " [    26  30000]\n",
      " [    49  86000]\n",
      " [    57 122000]\n",
      " [    34  25000]\n",
      " [    35  57000]\n",
      " [    34 115000]\n",
      " [    59  88000]\n",
      " [    45  32000]\n",
      " [    29  83000]\n",
      " [    26  80000]\n",
      " [    49  28000]\n",
      " [    23  20000]\n",
      " [    32  18000]\n",
      " [    60  42000]\n",
      " [    19  76000]\n",
      " [    36  99000]\n",
      " [    19  26000]\n",
      " [    60  83000]\n",
      " [    24  89000]\n",
      " [    27  58000]\n",
      " [    40  47000]\n",
      " [    42  70000]\n",
      " [    32 150000]\n",
      " [    35  77000]\n",
      " [    22  63000]\n",
      " [    45  22000]\n",
      " [    27  89000]\n",
      " [    18  82000]\n",
      " [    42  79000]\n",
      " [    40  60000]\n",
      " [    53  34000]\n",
      " [    47 107000]\n",
      " [    58 144000]\n",
      " [    59  83000]\n",
      " [    24  55000]\n",
      " [    26  35000]\n",
      " [    58  38000]\n",
      " [    42  80000]\n",
      " [    40  75000]\n",
      " [    59 130000]\n",
      " [    46  41000]\n",
      " [    41  60000]\n",
      " [    42  64000]\n",
      " [    37 146000]\n",
      " [    23  48000]\n",
      " [    25  33000]\n",
      " [    24  84000]\n",
      " [    27  96000]\n",
      " [    23  63000]\n",
      " [    48  33000]\n",
      " [    48  90000]\n",
      " [    42 104000]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling Analysis for Random Forest\n",
    "print(\"ğŸ“ FEATURE SCALING ANALYSIS FOR RANDOM FOREST\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "print(\"ğŸŒ³ WHY RANDOM FOREST DOESN'T NEED FEATURE SCALING:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Demonstrate scale-invariance of trees\n",
    "print(\"ğŸ” SCALE INVARIANCE DEMONSTRATION:\")\n",
    "\n",
    "# Current feature statistics\n",
    "print(f\"   Current feature scales:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    feature_data = X_train[:, i]\n",
    "    print(f\"      {name}: mean={feature_data.mean():.2f}, std={feature_data.std():.2f}, range=[{feature_data.min():.0f}, {feature_data.max():.0f}]\")\n",
    "\n",
    "# Show that tree splits are threshold-based, not distance-based\n",
    "print(f\"\\nğŸŒ³ TREE SPLITTING IS THRESHOLD-BASED:\")\n",
    "print(f\"   Example splits:\")\n",
    "print(f\"      'Age <= 35' (threshold split)\")\n",
    "print(f\"      'Salary <= 60000' (threshold split)\")\n",
    "print(f\"   These splits are INDEPENDENT of feature scales!\")\n",
    "\n",
    "print(f\"\\nâŒ WHEN NOT TO SCALE FOR RANDOM FOREST:\")\n",
    "print(f\"   â€¢ Tree-based algorithms use thresholds, not distances\")\n",
    "print(f\"   â€¢ Scaling can actually hurt interpretability\")\n",
    "print(f\"   â€¢ Feature importance becomes harder to interpret\")\n",
    "print(f\"   â€¢ Original scales make business sense\")\n",
    "\n",
    "print(f\"\\nâœ… DECISION: NO SCALING APPLIED\")\n",
    "print(f\"   Keeping original scales for:\")\n",
    "print(f\"   â€¢ Better interpretability\")\n",
    "print(f\"   â€¢ Meaningful feature importance\")\n",
    "print(f\"   â€¢ Business-friendly thresholds\")\n",
    "\n",
    "# Store unscaled data\n",
    "X_train_final = X_train\n",
    "X_test_final = X_test\n",
    "\n",
    "print(f\"\\nğŸ¯ Features ready for Random Forest (unscaled)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Random Forest Classifier Implementation\n",
    "\n",
    "### ğŸ”§ Hyperparameter Configuration Strategy\n",
    "\n",
    "Random Forest success depends on careful hyperparameter tuning. Each parameter controls different aspects of the ensemble behavior:\n",
    "\n",
    "**ğŸŒ³ Tree Construction Parameters:**\n",
    "- **`n_estimators`**: Number of trees in the forest\n",
    "  - More trees â†’ Better performance but slower training\n",
    "  - Typical range: 100-1000 trees\n",
    "  - Our choice: 100 trees (balanced performance/speed)\n",
    "\n",
    "**ğŸ² Randomness Control:**\n",
    "- **`random_state`**: Ensures reproducible results\n",
    "  - Critical for experiment consistency\n",
    "  - Allows result replication across runs\n",
    "\n",
    "**ğŸ”€ Bootstrap Sampling:**\n",
    "- **`bootstrap=True`**: Enable bootstrap sampling (default)\n",
    "  - Each tree trained on different data subset\n",
    "  - Creates diversity in the ensemble\n",
    "\n",
    "**ğŸ“Š Tree Complexity:**\n",
    "- **`max_depth`**: Maximum tree depth (default: None = unlimited)\n",
    "- **`min_samples_split`**: Minimum samples to split (default: 2)\n",
    "- **`min_samples_leaf`**: Minimum samples in leaf (default: 1)\n",
    "\n",
    "### ğŸ¯ Implementation Strategy\n",
    "\n",
    "1. **Start Simple**: Use default parameters with proven n_estimators\n",
    "2. **Ensure Reproducibility**: Set random_state for consistent results\n",
    "3. **Monitor Performance**: Evaluate on validation data\n",
    "4. **Tune if Needed**: Adjust based on performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1608,
     "status": "ok",
     "timestamp": 1588269343658,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "kj1hnFAR1s5w",
    "outputId": "1f3a92ea-9844-4d4c-ca5f-075fa4ba98e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier Implementation\n",
    "print(\"ğŸŒ³ IMPLEMENTING RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\"*42)\n",
    "\n",
    "# Import Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Configure Random Forest parameters\n",
    "print(\"ğŸ”§ HYPERPARAMETER CONFIGURATION:\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "# Core parameters\n",
    "N_ESTIMATORS = 100      # Number of trees in the forest\n",
    "RANDOM_STATE = 42       # For reproducible results\n",
    "MAX_DEPTH = None        # Unlimited tree depth (let trees grow fully)\n",
    "MIN_SAMPLES_SPLIT = 2   # Minimum samples required to split\n",
    "MIN_SAMPLES_LEAF = 1    # Minimum samples required in leaf\n",
    "BOOTSTRAP = True        # Enable bootstrap sampling\n",
    "\n",
    "# Display configuration\n",
    "print(f\"   ğŸ“Š Number of trees: {N_ESTIMATORS}\")\n",
    "print(f\"   ğŸ² Random state: {RANDOM_STATE}\")\n",
    "print(f\"   ğŸ“ Max depth: {MAX_DEPTH if MAX_DEPTH else 'Unlimited'}\")\n",
    "print(f\"   ğŸ”„ Bootstrap sampling: {BOOTSTRAP}\")\n",
    "\n",
    "# Create Random Forest classifier\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    min_samples_split=MIN_SAMPLES_SPLIT,\n",
    "    min_samples_leaf=MIN_SAMPLES_LEAF,\n",
    "    bootstrap=BOOTSTRAP\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ Random Forest classifier created with {N_ESTIMATORS} trees!\")\n",
    "print(f\"   Ready for ensemble training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kW3c7UYih0hT"
   },
   "source": [
    "## ğŸ“ Ensemble Training Process\n",
    "\n",
    "### ğŸŒ³ How Random Forest Training Works\n",
    "\n",
    "**Step 1: Bootstrap Sampling** ğŸ”„\n",
    "- Create multiple bootstrap samples from training data\n",
    "- Each sample: same size as original, with replacement\n",
    "- Result: Each tree sees different data perspective\n",
    "\n",
    "**Step 2: Feature Randomness** ğŸ²\n",
    "- At each split: consider random subset of features\n",
    "- Default: âˆš(total features) for classification\n",
    "- Reduces correlation between trees\n",
    "\n",
    "**Step 3: Tree Construction** ğŸŒ²\n",
    "- Build decision tree on each bootstrap sample\n",
    "- Use only selected features at each split\n",
    "- Grow trees deep (minimal pruning)\n",
    "\n",
    "**Step 4: Ensemble Assembly** ğŸ—ï¸\n",
    "- Combine all trained trees into forest\n",
    "- Each tree contributes to final prediction\n",
    "- Diversity through data and feature randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fQlDPKCh8sc"
   },
   "outputs": [],
   "source": [
    "# Training the Random Forest Classifier\n",
    "print(\"ğŸ“ TRAINING RANDOM FOREST ENSEMBLE\")\n",
    "print(\"=\"*38)\n",
    "\n",
    "import time\n",
    "\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"ğŸŒ± Initiating ensemble training...\")\n",
    "print(\"   ğŸ“Š Training 100 decision trees\")\n",
    "print(\"   ğŸ”„ Each tree uses bootstrap sampling\")\n",
    "print(\"   ğŸ² Each split uses random feature subset\")\n",
    "print(\"   ğŸŒ² Growing deep trees with minimal constraints\")\n",
    "\n",
    "# Train the Random Forest\n",
    "classifier.fit(X_train_final, y_train)\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… TRAINING COMPLETED!\")\n",
    "print(f\"   â±ï¸  Training time: {training_time:.3f} seconds\")\n",
    "print(f\"   ğŸŒ³ Trees in forest: {classifier.n_estimators}\")\n",
    "print(f\"   ğŸ“ˆ Training samples per tree: ~{len(X_train_final)} (with replacement)\")\n",
    "\n",
    "# Display model properties\n",
    "print(f\"\\nğŸ” MODEL PROPERTIES:\")\n",
    "print(f\"   ğŸ¯ Target classes: {classifier.classes_}\")\n",
    "print(f\"   ğŸ“‹ Feature count: {classifier.n_features_in_}\")\n",
    "print(f\"   ğŸŒ² Tree depth: Unlimited (default)\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Random Forest ensemble ready for predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1600,
     "status": "ok",
     "timestamp": 1588269343659,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "syrnD1Op2BSR",
    "outputId": "b1fa2925-b7de-4530-b015-01bb51e742b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58164944 -0.88670699]\n",
      " [-0.60673761  1.46173768]\n",
      " [-0.01254409 -0.5677824 ]\n",
      " [-0.60673761  1.89663484]\n",
      " [ 1.37390747 -1.40858358]\n",
      " [ 1.47293972  0.99784738]\n",
      " [ 0.08648817 -0.79972756]\n",
      " [-0.01254409 -0.24885782]\n",
      " [-0.21060859 -0.5677824 ]\n",
      " [-0.21060859 -0.19087153]\n",
      " [-0.30964085 -1.29261101]\n",
      " [-0.30964085 -0.5677824 ]\n",
      " [ 0.38358493  0.09905991]\n",
      " [ 0.8787462  -0.59677555]\n",
      " [ 2.06713324 -1.17663843]\n",
      " [ 1.07681071 -0.13288524]\n",
      " [ 0.68068169  1.78066227]\n",
      " [-0.70576986  0.56295021]\n",
      " [ 0.77971394  0.35999821]\n",
      " [ 0.8787462  -0.53878926]\n",
      " [-1.20093113 -1.58254245]\n",
      " [ 2.1661655   0.93986109]\n",
      " [-0.01254409  1.22979253]\n",
      " [ 0.18552042  1.08482681]\n",
      " [ 0.38358493 -0.48080297]\n",
      " [-0.30964085 -0.30684411]\n",
      " [ 0.97777845 -0.8287207 ]\n",
      " [ 0.97777845  1.8676417 ]\n",
      " [-0.01254409  1.25878567]\n",
      " [-0.90383437  2.27354572]\n",
      " [-1.20093113 -1.58254245]\n",
      " [ 2.1661655  -0.79972756]\n",
      " [-1.39899564 -1.46656987]\n",
      " [ 0.38358493  2.30253886]\n",
      " [ 0.77971394  0.76590222]\n",
      " [-1.00286662 -0.30684411]\n",
      " [ 0.08648817  0.76590222]\n",
      " [-1.00286662  0.56295021]\n",
      " [ 0.28455268  0.07006676]\n",
      " [ 0.68068169 -1.26361786]\n",
      " [-0.50770535 -0.01691267]\n",
      " [-1.79512465  0.35999821]\n",
      " [-0.70576986  0.12805305]\n",
      " [ 0.38358493  0.30201192]\n",
      " [-0.30964085  0.07006676]\n",
      " [-0.50770535  2.30253886]\n",
      " [ 0.18552042  0.04107362]\n",
      " [ 1.27487521  2.21555943]\n",
      " [ 0.77971394  0.27301877]\n",
      " [-0.30964085  0.1570462 ]\n",
      " [-0.01254409 -0.53878926]\n",
      " [-0.21060859  0.1570462 ]\n",
      " [-0.11157634  0.24402563]\n",
      " [-0.01254409 -0.24885782]\n",
      " [ 2.1661655   1.11381995]\n",
      " [-1.79512465  0.35999821]\n",
      " [ 1.86906873  0.12805305]\n",
      " [ 0.38358493 -0.13288524]\n",
      " [-1.20093113  0.30201192]\n",
      " [ 0.77971394  1.37475825]\n",
      " [-0.30964085 -0.24885782]\n",
      " [-1.6960924  -0.04590581]\n",
      " [-1.00286662 -0.74174127]\n",
      " [ 0.28455268  0.50496393]\n",
      " [-0.11157634 -1.06066585]\n",
      " [-1.10189888  0.59194336]\n",
      " [ 0.08648817 -0.79972756]\n",
      " [-1.00286662  1.54871711]\n",
      " [-0.70576986  1.40375139]\n",
      " [-1.29996338  0.50496393]\n",
      " [-0.30964085  0.04107362]\n",
      " [-0.11157634  0.01208048]\n",
      " [-0.30964085 -0.88670699]\n",
      " [ 0.8787462  -1.3505973 ]\n",
      " [-0.30964085  2.24455257]\n",
      " [ 0.97777845  1.98361427]\n",
      " [-1.20093113  0.47597078]\n",
      " [-1.29996338  0.27301877]\n",
      " [ 1.37390747  1.98361427]\n",
      " [ 1.27487521 -1.3505973 ]\n",
      " [-0.30964085 -0.27785096]\n",
      " [-0.50770535  1.25878567]\n",
      " [-0.80480212  1.08482681]\n",
      " [ 0.97777845 -1.06066585]\n",
      " [ 0.28455268  0.30201192]\n",
      " [ 0.97777845  0.76590222]\n",
      " [-0.70576986 -1.49556302]\n",
      " [-0.70576986  0.04107362]\n",
      " [ 0.48261718  1.72267598]\n",
      " [ 2.06713324  0.18603934]\n",
      " [-1.99318916 -0.74174127]\n",
      " [-0.21060859  1.40375139]\n",
      " [ 0.38358493  0.59194336]\n",
      " [ 0.8787462  -1.14764529]\n",
      " [-1.20093113 -0.77073441]\n",
      " [ 0.18552042  0.24402563]\n",
      " [ 0.77971394 -0.30684411]\n",
      " [ 2.06713324 -0.79972756]\n",
      " [ 0.77971394  0.12805305]\n",
      " [-0.30964085  0.6209365 ]\n",
      " [-1.00286662 -0.30684411]\n",
      " [ 0.18552042 -0.3648304 ]\n",
      " [ 2.06713324  2.12857999]\n",
      " [ 1.86906873 -1.26361786]\n",
      " [ 1.37390747 -0.91570013]\n",
      " [ 0.8787462   1.25878567]\n",
      " [ 1.47293972  2.12857999]\n",
      " [-0.30964085 -1.23462472]\n",
      " [ 1.96810099  0.91086794]\n",
      " [ 0.68068169 -0.71274813]\n",
      " [-1.49802789  0.35999821]\n",
      " [ 0.77971394 -1.3505973 ]\n",
      " [ 0.38358493 -0.13288524]\n",
      " [-1.00286662  0.41798449]\n",
      " [-0.01254409 -0.30684411]\n",
      " [-1.20093113  0.41798449]\n",
      " [-0.90383437 -1.20563157]\n",
      " [-0.11157634  0.04107362]\n",
      " [-1.59706014 -0.42281668]\n",
      " [ 0.97777845 -1.00267957]\n",
      " [ 1.07681071 -1.20563157]\n",
      " [-0.01254409 -0.13288524]\n",
      " [-1.10189888 -1.52455616]\n",
      " [ 0.77971394 -1.20563157]\n",
      " [ 0.97777845  2.07059371]\n",
      " [-1.20093113 -1.52455616]\n",
      " [-0.30964085  0.79489537]\n",
      " [ 0.08648817 -0.30684411]\n",
      " [-1.39899564 -1.23462472]\n",
      " [-0.60673761 -1.49556302]\n",
      " [ 0.77971394  0.53395707]\n",
      " [-0.30964085 -0.33583725]\n",
      " [ 1.77003648 -0.27785096]\n",
      " [ 0.8787462  -1.03167271]\n",
      " [ 0.18552042  0.07006676]\n",
      " [-0.60673761  0.8818748 ]\n",
      " [-1.89415691 -1.40858358]\n",
      " [-1.29996338  0.59194336]\n",
      " [-0.30964085  0.53395707]\n",
      " [-1.00286662 -1.089659  ]\n",
      " [ 1.17584296 -1.43757673]\n",
      " [ 0.18552042 -0.30684411]\n",
      " [ 1.17584296 -0.74174127]\n",
      " [-0.30964085  0.07006676]\n",
      " [ 0.18552042  2.09958685]\n",
      " [ 0.77971394 -1.089659  ]\n",
      " [ 0.08648817  0.04107362]\n",
      " [-1.79512465  0.12805305]\n",
      " [-0.90383437  0.1570462 ]\n",
      " [-0.70576986  0.18603934]\n",
      " [ 0.8787462  -1.29261101]\n",
      " [ 0.18552042 -0.24885782]\n",
      " [-0.4086731   1.22979253]\n",
      " [-0.01254409  0.30201192]\n",
      " [ 0.38358493  0.1570462 ]\n",
      " [ 0.8787462  -0.65476184]\n",
      " [ 0.08648817  0.1570462 ]\n",
      " [-1.89415691 -1.29261101]\n",
      " [-0.11157634  0.30201192]\n",
      " [-0.21060859 -0.27785096]\n",
      " [ 0.28455268 -0.50979612]\n",
      " [-0.21060859  1.6067034 ]\n",
      " [ 0.97777845 -1.17663843]\n",
      " [-0.21060859  1.63569655]\n",
      " [ 1.27487521  1.8676417 ]\n",
      " [-1.10189888 -0.3648304 ]\n",
      " [-0.01254409  0.04107362]\n",
      " [ 0.08648817 -0.24885782]\n",
      " [-1.59706014 -1.23462472]\n",
      " [-0.50770535 -0.27785096]\n",
      " [ 0.97777845  0.12805305]\n",
      " [ 1.96810099 -1.3505973 ]\n",
      " [ 1.47293972  0.07006676]\n",
      " [-0.60673761  1.37475825]\n",
      " [ 1.57197197  0.01208048]\n",
      " [-0.80480212  0.30201192]\n",
      " [ 1.96810099  0.73690908]\n",
      " [-1.20093113 -0.50979612]\n",
      " [ 0.68068169  0.27301877]\n",
      " [-1.39899564 -0.42281668]\n",
      " [ 0.18552042  0.1570462 ]\n",
      " [-0.50770535 -1.20563157]\n",
      " [ 0.58164944  2.01260742]\n",
      " [-1.59706014 -1.49556302]\n",
      " [-0.50770535 -0.53878926]\n",
      " [ 0.48261718  1.83864855]\n",
      " [-1.39899564 -1.089659  ]\n",
      " [ 0.77971394 -1.37959044]\n",
      " [-0.30964085 -0.42281668]\n",
      " [ 1.57197197  0.99784738]\n",
      " [ 0.97777845  1.43274454]\n",
      " [-0.30964085 -0.48080297]\n",
      " [-0.11157634  2.15757314]\n",
      " [-1.49802789 -0.1038921 ]\n",
      " [-0.11157634  1.95462113]\n",
      " [-0.70576986 -0.33583725]\n",
      " [-0.50770535 -0.8287207 ]\n",
      " [ 0.68068169 -1.37959044]\n",
      " [-0.80480212 -1.58254245]\n",
      " [-1.89415691 -1.46656987]\n",
      " [ 1.07681071  0.12805305]\n",
      " [ 0.08648817  1.51972397]\n",
      " [-0.30964085  0.09905991]\n",
      " [ 0.08648817  0.04107362]\n",
      " [-1.39899564 -1.3505973 ]\n",
      " [ 0.28455268  0.07006676]\n",
      " [-0.90383437  0.38899135]\n",
      " [ 1.57197197 -1.26361786]\n",
      " [-0.30964085 -0.74174127]\n",
      " [-0.11157634  0.1570462 ]\n",
      " [-0.90383437 -0.65476184]\n",
      " [-0.70576986 -0.04590581]\n",
      " [ 0.38358493 -0.45180983]\n",
      " [-0.80480212  1.89663484]\n",
      " [ 1.37390747  1.28777882]\n",
      " [ 1.17584296 -0.97368642]\n",
      " [ 1.77003648  1.83864855]\n",
      " [-0.90383437 -0.24885782]\n",
      " [-0.80480212  0.56295021]\n",
      " [-1.20093113 -1.5535493 ]\n",
      " [-0.50770535 -1.11865214]\n",
      " [ 0.28455268  0.07006676]\n",
      " [-0.21060859 -1.06066585]\n",
      " [ 1.67100423  1.6067034 ]\n",
      " [ 0.97777845  1.78066227]\n",
      " [ 0.28455268  0.04107362]\n",
      " [-0.80480212 -0.21986468]\n",
      " [-0.11157634  0.07006676]\n",
      " [ 0.28455268 -0.19087153]\n",
      " [ 1.96810099 -0.65476184]\n",
      " [-0.80480212  1.3457651 ]\n",
      " [-1.79512465 -0.59677555]\n",
      " [-0.11157634  0.12805305]\n",
      " [ 0.28455268 -0.30684411]\n",
      " [ 1.07681071  0.56295021]\n",
      " [-1.00286662  0.27301877]\n",
      " [ 1.47293972  0.35999821]\n",
      " [ 0.18552042 -0.3648304 ]\n",
      " [ 2.1661655  -1.03167271]\n",
      " [-0.30964085  1.11381995]\n",
      " [-1.6960924   0.07006676]\n",
      " [-0.01254409  0.04107362]\n",
      " [ 0.08648817  1.05583366]\n",
      " [-0.11157634 -0.3648304 ]\n",
      " [-1.20093113  0.07006676]\n",
      " [-0.30964085 -1.3505973 ]\n",
      " [ 1.57197197  1.11381995]\n",
      " [-0.80480212 -1.52455616]\n",
      " [ 0.08648817  1.8676417 ]\n",
      " [-0.90383437 -0.77073441]\n",
      " [-0.50770535 -0.77073441]\n",
      " [-0.30964085 -0.91570013]\n",
      " [ 0.28455268 -0.71274813]\n",
      " [ 0.28455268  0.07006676]\n",
      " [ 0.08648817  1.8676417 ]\n",
      " [-1.10189888  1.95462113]\n",
      " [-1.6960924  -1.5535493 ]\n",
      " [-1.20093113 -1.089659  ]\n",
      " [-0.70576986 -0.1038921 ]\n",
      " [ 0.08648817  0.09905991]\n",
      " [ 0.28455268  0.27301877]\n",
      " [ 0.8787462  -0.5677824 ]\n",
      " [ 0.28455268 -1.14764529]\n",
      " [-0.11157634  0.67892279]\n",
      " [ 2.1661655  -0.68375498]\n",
      " [-1.29996338 -1.37959044]\n",
      " [-1.00286662 -0.94469328]\n",
      " [-0.01254409 -0.42281668]\n",
      " [-0.21060859 -0.45180983]\n",
      " [-1.79512465 -0.97368642]\n",
      " [ 1.77003648  0.99784738]\n",
      " [ 0.18552042 -0.3648304 ]\n",
      " [ 0.38358493  1.11381995]\n",
      " [-1.79512465 -1.3505973 ]\n",
      " [ 0.18552042 -0.13288524]\n",
      " [ 0.8787462  -1.43757673]\n",
      " [-1.99318916  0.47597078]\n",
      " [-0.30964085  0.27301877]\n",
      " [ 1.86906873 -1.06066585]\n",
      " [-0.4086731   0.07006676]\n",
      " [ 1.07681071 -0.88670699]\n",
      " [-1.10189888 -1.11865214]\n",
      " [-1.89415691  0.01208048]\n",
      " [ 0.08648817  0.27301877]\n",
      " [-1.20093113  0.33100506]\n",
      " [-1.29996338  0.30201192]\n",
      " [-1.00286662  0.44697764]\n",
      " [ 1.67100423 -0.88670699]\n",
      " [ 1.17584296  0.53395707]\n",
      " [ 1.07681071  0.53395707]\n",
      " [ 1.37390747  2.331532  ]\n",
      " [-0.30964085 -0.13288524]\n",
      " [ 0.38358493 -0.45180983]\n",
      " [-0.4086731  -0.77073441]\n",
      " [-0.11157634 -0.50979612]\n",
      " [ 0.97777845 -1.14764529]\n",
      " [-0.90383437 -0.77073441]\n",
      " [-0.21060859 -0.50979612]\n",
      " [-1.10189888 -0.45180983]\n",
      " [-1.20093113  1.40375139]]\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions with Random Forest Ensemble\n",
    "print(\"ğŸ”® GENERATING ENSEMBLE PREDICTIONS\")\n",
    "print(\"=\"*37)\n",
    "\n",
    "# Generate predictions on test set\n",
    "print(\"ğŸ¯ Predicting test set classifications...\")\n",
    "print(\"   ğŸŒ³ Each tree votes on classification\")\n",
    "print(\"   ğŸ—³ï¸  Majority vote determines final prediction\")\n",
    "\n",
    "y_pred = classifier.predict(X_test_final)\n",
    "\n",
    "print(f\"âœ… Predictions generated for {len(y_pred)} test samples!\")\n",
    "\n",
    "# Show first few predictions with interpretation\n",
    "print(f\"\\nğŸ“‹ FIRST 10 PREDICTIONS:\")\n",
    "print(\"-\"*25)\n",
    "for i in range(min(10, len(y_pred))):\n",
    "    prediction = \"Will Purchase\" if y_pred[i] == 1 else \"Won't Purchase\"\n",
    "    actual = \"Will Purchase\" if y_test[i] == 1 else \"Won't Purchase\"\n",
    "    match = \"âœ…\" if y_pred[i] == y_test[i] else \"âŒ\"\n",
    "    print(f\"   Sample {i+1}: Predicted={prediction}, Actual={actual} {match}\")\n",
    "\n",
    "print(f\"\\nğŸ² Prediction Distribution:\")\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "for value, count in zip(unique, counts):\n",
    "    label = \"Will Purchase\" if value == 1 else \"Won't Purchase\"\n",
    "    percentage = (count / len(y_pred)) * 100\n",
    "    print(f\"   {label}: {count} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1595,
     "status": "ok",
     "timestamp": 1588269343659,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "JUd6iBRp2C3L",
    "outputId": "48320ca4-33e0-4bfe-92ba-91c06bcf714e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.80480212  0.50496393]\n",
      " [-0.01254409 -0.5677824 ]\n",
      " [-0.30964085  0.1570462 ]\n",
      " [-0.80480212  0.27301877]\n",
      " [-0.30964085 -0.5677824 ]\n",
      " [-1.10189888 -1.43757673]\n",
      " [-0.70576986 -1.58254245]\n",
      " [-0.21060859  2.15757314]\n",
      " [-1.99318916 -0.04590581]\n",
      " [ 0.8787462  -0.77073441]\n",
      " [-0.80480212 -0.59677555]\n",
      " [-1.00286662 -0.42281668]\n",
      " [-0.11157634 -0.42281668]\n",
      " [ 0.08648817  0.21503249]\n",
      " [-1.79512465  0.47597078]\n",
      " [-0.60673761  1.37475825]\n",
      " [-0.11157634  0.21503249]\n",
      " [-1.89415691  0.44697764]\n",
      " [ 1.67100423  1.75166912]\n",
      " [-0.30964085 -1.37959044]\n",
      " [-0.30964085 -0.65476184]\n",
      " [ 0.8787462   2.15757314]\n",
      " [ 0.28455268 -0.53878926]\n",
      " [ 0.8787462   1.02684052]\n",
      " [-1.49802789 -1.20563157]\n",
      " [ 1.07681071  2.07059371]\n",
      " [-1.00286662  0.50496393]\n",
      " [-0.90383437  0.30201192]\n",
      " [-0.11157634 -0.21986468]\n",
      " [-0.60673761  0.47597078]\n",
      " [-1.6960924   0.53395707]\n",
      " [-0.11157634  0.27301877]\n",
      " [ 1.86906873 -0.27785096]\n",
      " [-0.11157634 -0.48080297]\n",
      " [-1.39899564 -0.33583725]\n",
      " [-1.99318916 -0.50979612]\n",
      " [-1.59706014  0.33100506]\n",
      " [-0.4086731  -0.77073441]\n",
      " [-0.70576986 -1.03167271]\n",
      " [ 1.07681071 -0.97368642]\n",
      " [-1.10189888  0.53395707]\n",
      " [ 0.28455268 -0.50979612]\n",
      " [-1.10189888  0.41798449]\n",
      " [-0.30964085 -1.43757673]\n",
      " [ 0.48261718  1.22979253]\n",
      " [-1.10189888 -0.33583725]\n",
      " [-0.11157634  0.30201192]\n",
      " [ 1.37390747  0.59194336]\n",
      " [-1.20093113 -1.14764529]\n",
      " [ 1.07681071  0.47597078]\n",
      " [ 1.86906873  1.51972397]\n",
      " [-0.4086731  -1.29261101]\n",
      " [-0.30964085 -0.3648304 ]\n",
      " [-0.4086731   1.31677196]\n",
      " [ 2.06713324  0.53395707]\n",
      " [ 0.68068169 -1.089659  ]\n",
      " [-0.90383437  0.38899135]\n",
      " [-1.20093113  0.30201192]\n",
      " [ 1.07681071 -1.20563157]\n",
      " [-1.49802789 -1.43757673]\n",
      " [-0.60673761 -1.49556302]\n",
      " [ 2.1661655  -0.79972756]\n",
      " [-1.89415691  0.18603934]\n",
      " [-0.21060859  0.85288166]\n",
      " [-1.89415691 -1.26361786]\n",
      " [ 2.1661655   0.38899135]\n",
      " [-1.39899564  0.56295021]\n",
      " [-1.10189888 -0.33583725]\n",
      " [ 0.18552042 -0.65476184]\n",
      " [ 0.38358493  0.01208048]\n",
      " [-0.60673761  2.331532  ]\n",
      " [-0.30964085  0.21503249]\n",
      " [-1.59706014 -0.19087153]\n",
      " [ 0.68068169 -1.37959044]\n",
      " [-1.10189888  0.56295021]\n",
      " [-1.99318916  0.35999821]\n",
      " [ 0.38358493  0.27301877]\n",
      " [ 0.18552042 -0.27785096]\n",
      " [ 1.47293972 -1.03167271]\n",
      " [ 0.8787462   1.08482681]\n",
      " [ 1.96810099  2.15757314]\n",
      " [ 2.06713324  0.38899135]\n",
      " [-1.39899564 -0.42281668]\n",
      " [-1.20093113 -1.00267957]\n",
      " [ 1.96810099 -0.91570013]\n",
      " [ 0.38358493  0.30201192]\n",
      " [ 0.18552042  0.1570462 ]\n",
      " [ 2.06713324  1.75166912]\n",
      " [ 0.77971394 -0.8287207 ]\n",
      " [ 0.28455268 -0.27785096]\n",
      " [ 0.38358493 -0.16187839]\n",
      " [-0.11157634  2.21555943]\n",
      " [-1.49802789 -0.62576869]\n",
      " [-1.29996338 -1.06066585]\n",
      " [-1.39899564  0.41798449]\n",
      " [-1.10189888  0.76590222]\n",
      " [-1.49802789 -0.19087153]\n",
      " [ 0.97777845 -1.06066585]\n",
      " [ 0.97777845  0.59194336]\n",
      " [ 0.38358493  0.99784738]]\n"
     ]
    }
   ],
   "source": [
    "# Prediction Probabilities Analysis\n",
    "print(\"ğŸ“Š ANALYZING PREDICTION PROBABILITIES\")\n",
    "print(\"=\"*39)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_pred_proba = classifier.predict_proba(X_test_final)\n",
    "\n",
    "print(\"ğŸ” PROBABILITY INTERPRETATION:\")\n",
    "print(\"-\"*32)\n",
    "print(\"   Column 0: Probability of 'Won't Purchase' (class 0)\")\n",
    "print(\"   Column 1: Probability of 'Will Purchase' (class 1)\")\n",
    "\n",
    "# Analyze probability distribution\n",
    "print(f\"\\nğŸ“ˆ CONFIDENCE ANALYSIS:\")\n",
    "print(\"-\"*23)\n",
    "\n",
    "# Calculate confidence levels (max probability)\n",
    "confidence_levels = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "print(f\"   Average confidence: {confidence_levels.mean():.3f}\")\n",
    "print(f\"   Confidence range: [{confidence_levels.min():.3f}, {confidence_levels.max():.3f}]\")\n",
    "\n",
    "# Show confidence distribution\n",
    "high_confidence = np.sum(confidence_levels > 0.8)\n",
    "medium_confidence = np.sum((confidence_levels > 0.6) & (confidence_levels <= 0.8))\n",
    "low_confidence = np.sum(confidence_levels <= 0.6)\n",
    "\n",
    "print(f\"\\nğŸ¯ CONFIDENCE DISTRIBUTION:\")\n",
    "print(f\"   High confidence (>0.8): {high_confidence} samples ({high_confidence/len(y_pred)*100:.1f}%)\")\n",
    "print(f\"   Medium confidence (0.6-0.8): {medium_confidence} samples ({medium_confidence/len(y_pred)*100:.1f}%)\")\n",
    "print(f\"   Low confidence (â‰¤0.6): {low_confidence} samples ({low_confidence/len(y_pred)*100:.1f}%)\")\n",
    "\n",
    "# Show first few probabilities\n",
    "print(f\"\\nğŸ“‹ FIRST 5 PROBABILITY EXAMPLES:\")\n",
    "print(\"-\"*34)\n",
    "for i in range(5):\n",
    "    prob_no = y_pred_proba[i, 0]\n",
    "    prob_yes = y_pred_proba[i, 1]\n",
    "    prediction = \"Will Purchase\" if y_pred[i] == 1 else \"Won't Purchase\"\n",
    "    confidence = max(prob_no, prob_yes)\n",
    "    print(f\"   Sample {i+1}: No={prob_no:.3f}, Yes={prob_yes:.3f} â†’ {prediction} (conf: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bb6jCOCQiAmP"
   },
   "source": [
    "## ğŸ”® Ensemble Prediction Process\n",
    "\n",
    "### ğŸ—³ï¸ How Random Forest Makes Predictions\n",
    "\n",
    "**Individual Tree Predictions** ğŸŒ²\n",
    "- Each tree makes independent prediction\n",
    "- Based on its unique training data and feature selection\n",
    "- Results in diverse individual opinions\n",
    "\n",
    "**Majority Voting** ğŸ—³ï¸\n",
    "- Collect predictions from all 100 trees\n",
    "- Count votes for each class\n",
    "- Class with most votes wins\n",
    "\n",
    "**Confidence Estimation** ğŸ“Š\n",
    "- Vote proportion indicates confidence\n",
    "- 100/100 votes = 100% confidence\n",
    "- 60/100 votes = 60% confidence\n",
    "\n",
    "**Ensemble Advantage** ğŸ’ª\n",
    "- Individual trees may overfit\n",
    "- Ensemble averages out individual errors\n",
    "- More robust and generalizable predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1589,
     "status": "ok",
     "timestamp": 1588269343659,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "e0pFVAmciHQs",
    "outputId": "79719013-2ffa-49f6-b49c-886d9ba19525"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yyxW5b395mR2"
   },
   "source": [
    "## Predicting a new result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1584,
     "status": "ok",
     "timestamp": 1588269343660,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "f8YOXsQy58rP",
    "outputId": "81727e50-9f85-49ad-a41e-5891aa34e6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(sc.transform([[30,87000]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKYVQH-l5NpE"
   },
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1578,
     "status": "ok",
     "timestamp": 1588269343660,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "p6VMTb2O4hwM",
    "outputId": "f160d9d3-e4cd-4484-db9d-99028dfed42d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4Hwj34ziWQW"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1898,
     "status": "ok",
     "timestamp": 1588269343985,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "D6bpZwUiiXic",
    "outputId": "b4ab126b-4118-461e-f02a-cfe538ae6a71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[63  5]\n",
      " [ 4 28]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6OMC_P0diaoD"
   },
   "source": [
    "## Visualising the Training set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87793,
     "status": "ok",
     "timestamp": 1588269429885,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "_NOjKvZRid5l",
    "outputId": "7efb744e-3ecb-4303-8543-8fabf49f64bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Random Forest Classification (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZ-j28aPihZx"
   },
   "source": [
    "## Visualising the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 172477,
     "status": "ok",
     "timestamp": 1588269514574,
     "user": {
      "displayName": "Hadelin de Ponteves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64",
      "userId": "15047218817161520419"
     },
     "user_tz": -240
    },
    "id": "qeTjz2vDilAC",
    "outputId": "f1a33b1a-e8b6-4b3e-e98a-a0c7c66fed56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Random Forest Classification (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPA7K2PAkEFgaKFIvslUMEc",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "random_forest_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
