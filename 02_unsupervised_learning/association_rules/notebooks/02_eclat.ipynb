{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1fziHl7Ar94J"
      },
      "source": [
        "# Eclat Algorithm - Association Rule Learning\n",
        "\n",
        "## What is the Eclat Algorithm?\n",
        "\n",
        "Eclat (Equivalence Class Clustering and bottom-up Lattice Traversal) is another popular algorithm for association rule learning, similar to Apriori but with a different approach to finding frequent itemsets.\n",
        "\n",
        "## How Eclat Differs from Apriori:\n",
        "\n",
        "### Apriori Algorithm:\n",
        "- Uses a **breadth-first search** approach\n",
        "- Generates candidate itemsets level by level\n",
        "- Uses horizontal data layout (transactions as rows)\n",
        "- Requires multiple database scans\n",
        "\n",
        "### Eclat Algorithm:\n",
        "- Uses a **depth-first search** approach\n",
        "- Uses vertical data layout (items as columns with transaction IDs)\n",
        "- More memory efficient for sparse datasets\n",
        "- Faster intersection operations using transaction ID sets\n",
        "\n",
        "## Key Advantages of Eclat:\n",
        "\n",
        "1. **Memory Efficiency**: Better performance with sparse data\n",
        "2. **Faster Execution**: Fewer database scans required\n",
        "3. **Simple Implementation**: Straightforward intersection-based approach\n",
        "4. **Scalability**: Works well with large datasets\n",
        "\n",
        "## Business Application:\n",
        "\n",
        "Just like Apriori, Eclat helps us discover:\n",
        "- Which products are frequently bought together\n",
        "- Customer purchasing patterns\n",
        "- Cross-selling opportunities\n",
        "- Optimal product placement strategies\n",
        "\n",
        "**Note**: In this implementation, we'll actually use the apyori library which implements Apriori, but we'll focus on the support metric (which is Eclat's primary focus) rather than confidence and lift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eiNwni1xsEgT"
      },
      "source": [
        "## Step 1: Importing the Required Libraries\n",
        "\n",
        "Before we start implementing the Eclat algorithm, we need to import the necessary libraries:\n",
        "\n",
        "- **numpy**: For numerical operations and array handling\n",
        "- **matplotlib.pyplot**: For creating visualizations (though we won't use it extensively here)\n",
        "- **pandas**: For data manipulation and creating structured DataFrames\n",
        "- **apyori**: Library containing association rule learning algorithms\n",
        "\n",
        "### Important Note:\n",
        "While we're studying the Eclat algorithm conceptually, we'll use the apyori library which implements Apriori. However, we'll focus primarily on the **support metric** and frequent itemset discovery, which aligns with Eclat's main objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "DUF77Qr1vqyM",
        "outputId": "86e8ae3e-6a35-47bc-a832-5a02936a9c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apyori in /usr/local/lib/python3.6/dist-packages (1.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install apyori"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing the Apyori Library\n",
        "\n",
        "First, let's install the apyori library which contains association rule mining algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UJfitBClsJlT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing Standard Libraries\n",
        "\n",
        "Now let's import the essential data science libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vLt-7XUKsXBd"
      },
      "source": [
        "## Step 2: Data Preprocessing\n",
        "\n",
        "In this step, we'll load and prepare our market basket data for the Eclat-style analysis.\n",
        "\n",
        "### Understanding the Data Format\n",
        "\n",
        "Our dataset contains market basket transactions where:\n",
        "- Each row represents a customer's shopping basket\n",
        "- Each column represents a potential product in that basket\n",
        "- The data needs to be converted into a transaction format for analysis\n",
        "\n",
        "### Eclat's Data Structure Preference\n",
        "\n",
        "While the traditional Eclat algorithm prefers vertical data format (item → list of transaction IDs), we'll adapt our approach:\n",
        "- Convert CSV data into transaction lists\n",
        "- Focus on finding frequent itemsets based on support values\n",
        "- Emphasize the support metric rather than confidence and lift\n",
        "\n",
        "Let's load and examine our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "J_A-UFOAsaDf"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)\n",
        "transactions = []\n",
        "for i in range(0, 7501):\n",
        "  transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's first examine the dataset structure\n",
        "print(\"Loading market basket data...\")\n",
        "dataset = pd.read_csv('Market_Basket_Optimisation.csv', header=None)\n",
        "print(f\"Dataset shape: {dataset.shape}\")\n",
        "print(f\"Total transactions: {len(dataset)}\")\n",
        "print(f\"Maximum items per transaction: {dataset.shape[1]}\")\n",
        "\n",
        "print(\"\\nFirst few transactions:\")\n",
        "for i in range(3):\n",
        "    items = [item for item in dataset.iloc[i] if pd.notna(item)]\n",
        "    print(f\"Transaction {i+1}: {items}\")\n",
        "\n",
        "print(f\"\\nSample of all columns in first transaction:\")\n",
        "print(dataset.iloc[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Converting to Transaction Format\n",
        "\n",
        "Now we need to convert our data into a format suitable for association rule mining. We'll create a list where each element represents a transaction (shopping basket):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert data to transaction format (removing null values)\n",
        "transactions = []\n",
        "for i in range(0, len(dataset)):\n",
        "    # Extract non-null items from each row\n",
        "    transaction = [str(dataset.values[i,j]) for j in range(0, dataset.shape[1]) if str(dataset.values[i,j]) != 'nan']\n",
        "    if transaction:  # Only add non-empty transactions\n",
        "        transactions.append(transaction)\n",
        "\n",
        "print(f\"Successfully converted {len(transactions)} transactions\")\n",
        "print(f\"Average items per transaction: {sum(len(t) for t in transactions)/len(transactions):.2f}\")\n",
        "\n",
        "print(\"\\nSample converted transactions:\")\n",
        "for i in range(3):\n",
        "    print(f\"Transaction {i+1}: {transactions[i]}\")\n",
        "\n",
        "# Get some statistics about item frequency\n",
        "from collections import Counter\n",
        "all_items = [item for transaction in transactions for item in transaction]\n",
        "item_counts = Counter(all_items)\n",
        "print(f\"\\nTotal unique products: {len(item_counts)}\")\n",
        "print(f\"Most frequent products:\")\n",
        "for item, count in item_counts.most_common(5):\n",
        "    print(f\"  - {item}: {count} transactions ({count/len(transactions)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1wYZdBd5sea_"
      },
      "source": [
        "## Step 3: Applying Eclat-Style Analysis\n",
        "\n",
        "Now we'll apply association rule mining with a focus on the **support metric**, which is the primary concern of the Eclat algorithm.\n",
        "\n",
        "### Understanding Support in Eclat Context\n",
        "\n",
        "In Eclat algorithm, **support** is the key metric:\n",
        "- **Support** = (Number of transactions containing itemset) / (Total number of transactions)\n",
        "- Eclat focuses on finding **frequent itemsets** based on minimum support threshold\n",
        "- Unlike Apriori's breadth-first approach, Eclat uses depth-first search through itemset space\n",
        "\n",
        "### Parameter Selection for Eclat-Style Analysis\n",
        "\n",
        "We'll use conservative parameters to focus on the most significant itemset relationships:\n",
        "- **min_support = 0.003**: Itemsets must appear in at least 0.3% of transactions (~23 transactions)\n",
        "- We'll extract and prioritize results by support values rather than confidence/lift\n",
        "- Focus on itemset frequency rather than rule strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YzIk4vXZsj5i"
      },
      "outputs": [],
      "source": [
        "from apyori import apriori\n",
        "rules = apriori(transactions = transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2, max_length = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running the Association Rule Mining\n",
        "\n",
        "Let's apply the algorithm to find frequent itemsets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Applying association rule mining with Eclat-style focus on support...\")\n",
        "print(f\"Analyzing {len(transactions)} transactions...\")\n",
        "\n",
        "# Apply the algorithm with focus on support (Eclat's primary metric)\n",
        "rules = apriori(\n",
        "    transactions=transactions, \n",
        "    min_support=0.003,      # Minimum support threshold (key Eclat parameter)\n",
        "    min_confidence=0.2,     # Secondary parameter\n",
        "    min_lift=3,             # Secondary parameter\n",
        "    min_length=2,           # Focus on item pairs\n",
        "    max_length=2            # Keep it simple with pairs\n",
        ")\n",
        "\n",
        "print(\"Analysis completed!\")\n",
        "print(\"Extracting frequent itemsets and their support values...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b176YNwWspiO"
      },
      "source": [
        "## Step 4: Analyzing Frequent Itemsets (Eclat Focus)\n",
        "\n",
        "In the Eclat algorithm, the primary goal is to discover **frequent itemsets** based on their support values. Let's extract and analyze these itemsets with a focus on support rather than rule strength.\n",
        "\n",
        "### What We're Looking For:\n",
        "\n",
        "- **Frequent itemsets**: Product combinations that appear together frequently\n",
        "- **Support values**: How often these combinations occur in our dataset\n",
        "- **Itemset patterns**: Which products have strong co-occurrence relationships\n",
        "\n",
        "This aligns with Eclat's core objective of efficiently finding frequent patterns in transactional data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iO6bF_dImT-E"
      },
      "source": [
        "### Displaying the first results coming directly from the output of the apriori function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kvF-sLc6ifhd"
      },
      "outputs": [],
      "source": [
        "results = list(rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to list for analysis\n",
        "results = list(rules)\n",
        "print(f\"Found {len(results)} frequent itemsets/rules\")\n",
        "\n",
        "if len(results) > 0:\n",
        "    print(\"\\nSample result structure:\")\n",
        "    print(\"First itemset:\", results[0])\n",
        "    print(\"\\nItems in first result:\", results[0].items)\n",
        "    print(\"Support of first result:\", results[0].support)\n",
        "else:\n",
        "    print(\"No frequent itemsets found with current parameters.\")\n",
        "    print(\"Consider lowering the min_support threshold.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "colab_type": "code",
        "id": "eAD8Co4_l9IE",
        "outputId": "b87395d9-2a75-49ab-9206-61fb678e6aa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[RelationRecord(items=frozenset({'chicken', 'light cream'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n",
              " RelationRecord(items=frozenset({'mushroom cream sauce', 'escalope'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n",
              " RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n",
              " RelationRecord(items=frozenset({'fromage blanc', 'honey'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n",
              " RelationRecord(items=frozenset({'ground beef', 'herb & pepper'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n",
              " RelationRecord(items=frozenset({'ground beef', 'tomato sauce'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n",
              " RelationRecord(items=frozenset({'olive oil', 'light cream'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n",
              " RelationRecord(items=frozenset({'whole wheat pasta', 'olive oil'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n",
              " RelationRecord(items=frozenset({'shrimp', 'pasta'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed information about frequent itemsets\n",
        "if len(results) > 0:\n",
        "    print(\"FREQUENT ITEMSETS ANALYSIS (Eclat Focus)\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for i, result in enumerate(results[:10]):  # Show first 10 results\n",
        "        items_list = list(result.items)\n",
        "        print(f\"\\nItemset {i+1}:\")\n",
        "        print(f\"  Products: {items_list}\")\n",
        "        print(f\"  Support: {result.support:.4f} ({result.support*len(transactions):.0f} transactions)\")\n",
        "        print(f\"  Frequency: {result.support*100:.2f}% of all transactions\")\n",
        "        \n",
        "        if result.support >= 0.01:\n",
        "            frequency_desc = \"Very Frequent\"\n",
        "        elif result.support >= 0.005:\n",
        "            frequency_desc = \"Frequent\"\n",
        "        else:\n",
        "            frequency_desc = \"Moderately Frequent\"\n",
        "            \n",
        "        print(f\"  Classification: {frequency_desc}\")\n",
        "    \n",
        "    print(f\"\\nSummary of all {len(results)} frequent itemsets:\")\n",
        "    support_values = [r.support for r in results]\n",
        "    print(f\"Average support: {np.mean(support_values):.4f}\")\n",
        "    print(f\"Highest support: {max(support_values):.4f}\")\n",
        "    print(f\"Lowest support: {min(support_values):.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No frequent itemsets to display.\")\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MFkQP-fcjDBC"
      },
      "source": [
        "### Creating a Structured DataFrame for Eclat Results\n",
        "\n",
        "Now let's organize our frequent itemsets into a clean DataFrame format, focusing on the support values which are the core output of the Eclat algorithm.\n",
        "\n",
        "#### DataFrame Structure:\n",
        "- **Product 1**: First item in the frequent itemset\n",
        "- **Product 2**: Second item in the frequent itemset  \n",
        "- **Support**: The frequency with which these items appear together (Eclat's primary metric)\n",
        "\n",
        "This format makes it easy to identify the most frequent item combinations and their co-occurrence patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gyq7Poi0mMUe"
      },
      "outputs": [],
      "source": [
        "def inspect(results):\n",
        "    lhs         = [tuple(result[2][0][0])[0] for result in results]\n",
        "    rhs         = [tuple(result[2][0][1])[0] for result in results]\n",
        "    supports    = [result[1] for result in results]\n",
        "    return list(zip(lhs, rhs, supports))\n",
        "resultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Product 1', 'Product 2', 'Support'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced function to extract frequent itemsets with error handling\n",
        "def inspect(results):\n",
        "    \"\"\"\n",
        "    Extract itemset information focusing on support values (Eclat's main concern)\n",
        "    Returns lists of Product 1, Product 2, and Support values\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return [], [], []\n",
        "    \n",
        "    product1 = []\n",
        "    product2 = []\n",
        "    supports = []\n",
        "    \n",
        "    for result in results:\n",
        "        # Extract support value\n",
        "        supports.append(result.support)\n",
        "        \n",
        "        # Extract the itemset (should be pairs for this analysis)\n",
        "        items = list(result.items)\n",
        "        if len(items) >= 2:\n",
        "            product1.append(items[0])\n",
        "            product2.append(items[1])\n",
        "        else:\n",
        "            # Handle single items by showing them in both columns\n",
        "            product1.append(items[0] if items else \"Unknown\")\n",
        "            product2.append(\"(single item)\" if items else \"Unknown\")\n",
        "    \n",
        "    return list(zip(product1, product2, supports))\n",
        "\n",
        "# Create the Eclat-focused DataFrame\n",
        "if len(results) > 0:\n",
        "    resultsinDataFrame = pd.DataFrame(\n",
        "        inspect(results), \n",
        "        columns=['Product 1', 'Product 2', 'Support']\n",
        "    )\n",
        "    print(f\"Created DataFrame with {len(resultsinDataFrame)} frequent itemsets\")\n",
        "    print(\"\\nDataFrame preview:\")\n",
        "    print(resultsinDataFrame.head())\n",
        "    \n",
        "    # Add some statistics\n",
        "    print(f\"\\nSupport Statistics:\")\n",
        "    print(f\"Mean support: {resultsinDataFrame['Support'].mean():.4f}\")\n",
        "    print(f\"Median support: {resultsinDataFrame['Support'].median():.4f}\")\n",
        "    print(f\"Standard deviation: {resultsinDataFrame['Support'].std():.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No results to create DataFrame\")\n",
        "    resultsinDataFrame = pd.DataFrame(columns=['Product 1', 'Product 2', 'Support'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IjrrlYW4jpTR"
      },
      "source": [
        "### Top Frequent Itemsets by Support (Eclat's Primary Output)\n",
        "\n",
        "In the Eclat algorithm, itemsets are typically ranked by their **support values** since support represents the frequency of co-occurrence, which is the algorithm's main focus.\n",
        "\n",
        "#### Why Support Matters in Eclat:\n",
        "- **High Support = High Frequency**: Itemsets with higher support appear together more often\n",
        "- **Business Relevance**: Most frequent combinations represent the strongest purchasing patterns\n",
        "- **Eclat's Goal**: Find the most frequent itemsets efficiently using vertical data representation\n",
        "\n",
        "Let's examine the top itemsets ranked by support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "colab_type": "code",
        "id": "nI7DJXng-nxQ",
        "outputId": "63b4b5d1-9d4d-43a0-ded4-6ee4de2f4854"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product 1</th>\n",
              "      <th>Product 2</th>\n",
              "      <th>Support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>herb &amp; pepper</td>\n",
              "      <td>ground beef</td>\n",
              "      <td>0.015998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>whole wheat pasta</td>\n",
              "      <td>olive oil</td>\n",
              "      <td>0.007999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pasta</td>\n",
              "      <td>escalope</td>\n",
              "      <td>0.005866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mushroom cream sauce</td>\n",
              "      <td>escalope</td>\n",
              "      <td>0.005733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tomato sauce</td>\n",
              "      <td>ground beef</td>\n",
              "      <td>0.005333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>pasta</td>\n",
              "      <td>shrimp</td>\n",
              "      <td>0.005066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>light cream</td>\n",
              "      <td>chicken</td>\n",
              "      <td>0.004533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fromage blanc</td>\n",
              "      <td>honey</td>\n",
              "      <td>0.003333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>light cream</td>\n",
              "      <td>olive oil</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Product 1    Product 2   Support\n",
              "4         herb & pepper  ground beef  0.015998\n",
              "7     whole wheat pasta    olive oil  0.007999\n",
              "2                 pasta     escalope  0.005866\n",
              "1  mushroom cream sauce     escalope  0.005733\n",
              "5          tomato sauce  ground beef  0.005333\n",
              "8                 pasta       shrimp  0.005066\n",
              "0           light cream      chicken  0.004533\n",
              "3         fromage blanc        honey  0.003333\n",
              "6           light cream    olive oil  0.003200"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resultsinDataFrame.nlargest(n = 10, columns = 'Support')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display top frequent itemsets with detailed Eclat-focused analysis\n",
        "if not resultsinDataFrame.empty:\n",
        "    print(\"TOP 10 FREQUENT ITEMSETS BY SUPPORT (Eclat Focus)\")\n",
        "    print(\"=\" * 55)\n",
        "    \n",
        "    # Sort by support (Eclat's primary ranking criterion)\n",
        "    top_itemsets = resultsinDataFrame.nlargest(n=10, columns='Support')\n",
        "    \n",
        "    # Detailed analysis for each top itemset\n",
        "    for idx, (index, itemset) in enumerate(top_itemsets.iterrows(), 1):\n",
        "        print(f\"\\n{idx}. Itemset: ['{itemset['Product 1']}', '{itemset['Product 2']}']\")\n",
        "        print(f\"   Support: {itemset['Support']:.4f}\")\n",
        "        print(f\"   Frequency: {itemset['Support']*100:.2f}% of all transactions\")\n",
        "        print(f\"   Absolute Count: ~{itemset['Support']*len(transactions):.0f} transactions\")\n",
        "        \n",
        "        # Categorize support level\n",
        "        if itemset['Support'] >= 0.01:\n",
        "            category = \"Very High Frequency\"\n",
        "            business_action = \"Core product pairing - prioritize for bundling\"\n",
        "        elif itemset['Support'] >= 0.007:\n",
        "            category = \"High Frequency\"\n",
        "            business_action = \"Strong candidate for cross-selling\"\n",
        "        elif itemset['Support'] >= 0.005:\n",
        "            category = \"Moderate Frequency\"\n",
        "            business_action = \"Consider for promotional campaigns\"\n",
        "        else:\n",
        "            category = \"Low-Moderate Frequency\"\n",
        "            business_action = \"Monitor for seasonal trends\"\n",
        "            \n",
        "        print(f\"   Category: {category}\")\n",
        "        print(f\"   Business Action: {business_action}\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*55)\n",
        "    print(\"ECLAT ALGORITHM SUMMARY\")\n",
        "    print(f\"Total frequent itemsets found: {len(resultsinDataFrame)}\")\n",
        "    print(f\"Support range: {resultsinDataFrame['Support'].min():.4f} - {resultsinDataFrame['Support'].max():.4f}\")\n",
        "    print(f\"Average support: {resultsinDataFrame['Support'].mean():.4f}\")\n",
        "    \n",
        "    # Show the DataFrame\n",
        "    print(f\"\\nTop 10 Itemsets (sorted by Support):\")\n",
        "    display(top_itemsets)\n",
        "    \n",
        "else:\n",
        "    print(\"No frequent itemsets found to analyze.\")\n",
        "    print(\"Consider lowering the min_support threshold to find more itemsets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Eclat Algorithm Insights and Business Applications\n",
        "\n",
        "### Key Differences: Eclat vs. Apriori\n",
        "\n",
        "Now that we've completed our analysis, let's understand how the Eclat approach differs from Apriori:\n",
        "\n",
        "#### Eclat Algorithm Characteristics:\n",
        "1. **Primary Focus**: Finding frequent itemsets based on support values\n",
        "2. **Data Structure**: Prefers vertical data format (item → transaction list)\n",
        "3. **Search Strategy**: Depth-first search through itemset lattice\n",
        "4. **Efficiency**: Better performance on sparse datasets\n",
        "5. **Memory Usage**: More memory-efficient for certain dataset types\n",
        "\n",
        "#### Apriori Algorithm Characteristics:\n",
        "1. **Primary Focus**: Comprehensive rule generation with confidence and lift\n",
        "2. **Data Structure**: Uses horizontal data format (transaction → item list)\n",
        "3. **Search Strategy**: Breadth-first search level by level\n",
        "4. **Rule Emphasis**: Strong focus on rule quality metrics\n",
        "5. **Comprehensive Output**: Support, confidence, and lift for decision making\n",
        "\n",
        "### Business Applications of Eclat Results:\n",
        "\n",
        "#### High Support Itemsets (Top Results):\n",
        "- **Product Placement**: Place frequently bought together items near each other\n",
        "- **Inventory Management**: Ensure adequate stock levels for item pairs\n",
        "- **Store Layout**: Design store sections based on item co-occurrence patterns\n",
        "\n",
        "#### Medium Support Itemsets:\n",
        "- **Promotional Campaigns**: Create targeted promotions for moderate frequency pairs\n",
        "- **Seasonal Analysis**: Monitor how support values change over time\n",
        "- **Customer Segmentation**: Identify different purchasing patterns\n",
        "\n",
        "### When to Use Eclat vs. Apriori:\n",
        "\n",
        "#### Choose Eclat When:\n",
        "- You primarily need frequent itemset discovery\n",
        "- Working with sparse datasets\n",
        "- Memory efficiency is important\n",
        "- Support-based ranking is sufficient\n",
        "- Quick itemset identification is the goal\n",
        "\n",
        "#### Choose Apriori When:\n",
        "- You need comprehensive rule analysis\n",
        "- Confidence and lift metrics are important\n",
        "- Building recommendation systems\n",
        "- Need detailed rule interpretation\n",
        "- Business requires rule strength assessment\n",
        "\n",
        "### Technical Implementation Notes:\n",
        "\n",
        "In this notebook, we used the apyori library (which implements Apriori) but focused on the support metric to simulate Eclat's approach. In a pure Eclat implementation:\n",
        "\n",
        "1. **Data would be vertically formatted**: Each item maps to its transaction IDs\n",
        "2. **Intersection operations**: Find common transaction IDs between items\n",
        "3. **Support calculation**: Count intersections divided by total transactions\n",
        "4. **Depth-first traversal**: Recursively explore itemset combinations\n",
        "\n",
        "This approach demonstrates the conceptual differences while providing practical insights for business decision-making."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "eclat.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
